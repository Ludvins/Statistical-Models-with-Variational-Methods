

Until this moment we have being using a complete information data but in practice this data is often incomplete in two different ways. There may be unobserved or \emph{hidden} variables that affect the visible ones, and there may be \emph{missing} information, that is, states of visible variables that are missing.

To illustrate the later, think about the example with the disease and the two habits we used in the last section, missing data would be a row in the table where some entry is missing, for example \(x_{3} = \{D = 1, A = 1\}\), where we know that this person got the disease and had habit \(A\) but we have no information about habit \(B\).

One approach to handle this situation would be marginalizing over that variable
\[
  P(x_{n} \mid \theta) = \int_{b}P(d_{n}, a_{n}, b \mid \theta) = P(a_{n} \mid \theta_{A})\int_{b}P(b \mid \theta_{B})P(d_{n} \mid b, a_{n}, \theta_{D})
\]
But this leads to a form which cannot be factorized and makes the posterior more complex, therefore, the problem is not conceptual but computational. Missing data does not always lead to this situation, for example, marginalizing over a collider would lead to loosing that variable as the integral simply equals \(1\).

There are three main types of missing data:
\begin{itemize}
  \item \textbf{Missing completely at random (MCAR)}. If the events that lead to any particular data to be missing is independent from both the observed and the unobserved variables, and occur at random.
  \item \textbf{Missing at random (MAR)}. When the absence is not random but can be explained with observed variables.
  \item \textbf{Missing not at random (MNAR)}. The missing data is related with the reason why it is missing. For example, skipping a question in a survey for being ashamed of the answer.
\end{itemize}

To express this mathematically, split the variables \(\X\) into visible \(\X_{vis}\) and hidden \(\X_{hid}\), let \(M\) be a variable denoting that the state of the hidden variables is known \((0)\) or unknown \((1)\).
So the differences between the three types resides on how \(P(M = 1 \mid x_{vis}, x_{hid}, \theta)\) simplifies. When data is \emph{missing at random}, we assume that we can explain the missing information with the visible one, so the probability of being missing only depends on the visible data, that is
\[
  P(M = 1 \mid x_{vis}, x_{hid}, \theta) = P(M = 1 \mid x_{vis})
\]
so that,
\[
  P(x_{vis}, M = 1 \mid \theta) = P(M = 1 \mid x_{vis})P(x_{vis} \mid \theta)
\]

Assuming the data is \emph{missing completely at random} is stronger, as we are supposing that there is no reason behind the missing data, so that it being messing is independent from the visible and hidden data.
\[
  P(M = 1 \mid x_{vis}, x_{hid}, \theta) = P(M = 1)
\]
so now
\[
    P( x_{vis}, M = 1 \mid \theta) = P(M = 1)P( x_{vis} \mid \theta)
\]
In both cases we may simply use the marginal likelihood \(P(x_{vis} \mid \theta)\) to assess parameters as \(P( x_{vis}, M = 1 \mid \theta)\) does not depend on the hidden variables. In case data is \emph{missing not at random} then no independence assumption is made over the probability of the data being unknown, meaning it depends on both the visible and the hidden information.

\section{Expectation Maximization}

The \emph{expectation maximization} algorithm is an iterative method to find maximum likelihood or maximum a posteriori estimates of parameters in statistical models, where the model depends on hidden variables. The main idea is to set a lower bound to the marginal likelihood, then using an iterative method to increase this lower bound.

\subsection{General case}

Consider we have only two variables, one visible \(V\) and one hidden \(H\). Consider also the Kullback-Leibler divergence between a 'variational' distribution \(Q(h\mid v)\) (where variational means that this distribution is the object of an optimization problem) and a parametric one \(P(h \mid v, \theta)\).
\[
  \begin{aligned}
    KL(Q(h\mid v) \mid P(h \mid v, \theta)) &= \E_{Q}[\log{Q(h \mid v)} - \log{P(h \mid v, \theta)}] \\
    &= \E_{Q}[\log{Q(h \mid v)}] - \E_{Q}[\log{P(h,v \mid \theta)}] + \E_{Q}[\log{P(v \mid \theta)}]\\
    &= \E_{Q}[\log{Q(h \mid v)}] - \E_{Q}[\log{P(h,v \mid \theta)}] + \log{P(v \mid \theta)} \geq 0
  \end{aligned}
\]
We got then a lower bound to \(\log P(v \mid \theta)\)
\[
  \log{P(v \mid \theta)}\geq \underbrace{-\E_{Q}[\log{Q(h \mid v)}]}_{\text{Entropy}} + \underbrace{\E_{Q}[\log{P(h,v \mid \theta)}]}_{\text{Energy}}\footnote{This terms come from a statistical physics terminology}
\]

Assume a set of observations of the visible data, \(\V = \{v_{1}, \dots, v_{N}\}\) and the states of the hidden variables in that observations \(\{h_{1},\dots, h_{N}\}\), notice that the values on this last set are unknown but they exists. Then has the variables of the observations are i.i.d we got that
\[
  \begin{aligned}
    \log{P(\V \mid \theta)} = \sum_{n=1}^{N}\log{P(v_{n} \mid \theta)} \geq \sum_{n = 1}^{N} -\E_{Q}[\log{Q(h_{n} \mid v_{n})}] + \E_{Q}[\log{P(h_{n},v_{n} \mid \theta)}]
  \end{aligned}
\]
And we know equality holds if and only if \(Q(h_{n} \mid v_{n}) = P(h_{n} \mid v_{n} , \theta) \ \forall n=1, \dots, N\).

This suggest the following iterative procedure to optimize the parameter \(\theta\) consisted in two steps.
\begin{itemize}
  \item \textbf{E-step}. For a fixed \(\theta\), find the distributions that maximize the above bound, i.e, choose \(Q(h_{n} \mid v_{n}) = P(h_{n} \mid v_{n} , \theta)\).
  \item \textbf{M-step}. For a fixed distribution \(Q\), find the parameter \(\theta\) that maximizes the bound. Since \(Q\) does not depend on the parameter, this is equivalent to maximize the energy term.
\end{itemize}

\begin{exampleth}
  Consider a single variable \(V\) with \(Dom(V) = \mathbb{R}\) and a single hidden variable \(H\) with \(dom(H) = \{1,2\}\). Consider the model
  \[
    P(v \mid h, \theta) = \frac{1}{\sqrt{\pi}}e^{-(v - \theta h)^{2}}
  \]
  and \(P(H = 1) = P(H = 2) = 0.5\). Suppose an observation \(v = 2.75\) we want to optimize the parameter \(\theta\) in
  \[
    P(V = 2.75 \mid \theta) = \int_{h} P(V = 2.75 \mid h, \theta) P(h) = \frac{1}{2\sqrt{\pi}}\big( e^{-(2.75 - \theta)^{2}} + e^{-(2.75 - 2\theta)^{2}} \big)
  \]

  The lower bound given to the log likelihood is
  \[
    \log{P(v \mid \theta)} \geq -Q(1)\log{Q(1)} - Q(2)\log{Q(2)} - \E_{Q}[(v - \theta h)^{2}]\ + \text{const.}
  \]
  The M-step can be done analytically, noticing that due to the negative sign we want to minimize \(\E_{Q}[(v - \theta h)^{2}]\)
  \[
    \frac{d}{d\theta}\E[(v - \theta h)^{2}] = \E[ 2 v h + 2\theta h^{2} ] = 2v\E[h] + 2\theta \E[h^{2}] = 0 \iff \theta = \frac{v\E[h]}{\E[h^{2}]}
  \]
  \[
     \frac{d^{2}}{d^{2}\theta}\E[(v - \theta h)^{2}] = 2\E[h^{2}] \geq 0
  \]

  so the new parameter optimal parameter is
  \[
    \theta_{new} = v \frac{\E_{Q}[H]}{\E_{Q}[H^{2}]}
  \]

  The E-step would set \(Q_{new}(h) = P(h \mid v , \theta)\), in this case
  \[
    Q_{new}(h) = \frac{P(V = 2.75 \mid h, \theta)P(H = 2)}{P(V = 2.75)} = \frac{e^{-(2.75-h\theta)}}{ e^{-(2.75-\theta)} + e^{-(2.75-2\theta)}  }
  \]

 \begin{algorithm}[h]
  \SetAlgoLined
  \KwData{A distribution \(P(x \mid \theta)\) and a dataset \(\V\). Where \(X\) splits in visible variables \(V\) and hidden variables \(H\)}
  \KwResult{Parameter \(\theta\) that maximizes the likelihood}
  \While{Convergence stop criteria}{
    \For{\(n \in 1,\dots,N\)}{
      \(Q(h \mid v_{n}) = P(h \mid v_{n}, \theta)\)\;
    }
    \(\theta = \argmax_{\theta}\sum_{n=1}^{N}\E_{Q(h \mid v_{n})}[\log{P(h \mid v_{n}, \theta)}]\)\;
  }
  \KwRet{\(\theta\)}\;
  \caption{Expectation Maximization Algorithm}
  \label{alg:pc1}
\end{algorithm}


\end{exampleth}

It is clear that the EM algorithm does increase the lower bound in each iteration but we would like it to increase not only the bound but also the marginal likelihood. We will be using a single data point as it easy holds by summation when using the full dataset.

Let \(\theta^{new}\) be the value of the parameter after one iteration, we now that \(Q\) is set to
\[
  Q(h \mid v_{n}) = P(h \mid v_{n}, \theta)
\]
So the lower bound in terms of \(\theta\) and \(\theta^{new}\) is
\[
  LB(\theta^{new} \mid \theta) =  \underbrace{-\E_{P(h \mid v, \theta)}[\log{P(h \mid v, \theta)}]}_{\text{Entropy}} + \underbrace{\E_{P(h \mid v, \theta)}[\log{P(h,v \mid \theta^{new})}]}_{\text{Energy}}
\]

From the definition of the Kullback-Leibler we get that
\[
  \log{P(v \mid \theta^{new})} = LB(\theta^{new}\mid \theta) + KL\big( P(h \mid v, \theta) \mid P(h \mid v, \theta^{new})  \big)
\]
We could use \(\theta\) in the above formula getting
\[
  \log{P(v \mid \theta)} = LB(\theta \mid \theta) + KL\big( P(h \mid v, \theta) \mid P(h \mid v, \theta)  \big) = LB(\theta \mid \theta)
\]
So we can compute the difference between the log likelihood between two consecutive iterations as
\[
  \log{P(v \mid \theta^{new})} - \log{P(v \mid \theta)} = LB(\theta^{new}\mid \theta) - LB(\theta \mid \theta) +  KL\big( P(h \mid v, \theta) \mid P(h \mid v, \theta^{new})  \big)
\]

Where we know the last term is always positive, about the difference of bounds, the M-step ensures the new parameter makes the lower bound higher or equal to the current one, so that difference is also positive.

\subsection{Belief Networks case}

As we did before let \(\X = (\V, \mathcal{H}) = \{X_{1}, \dots , X_{M}\} = \{(V_{1}, H_{1}), \dots ,(V_{M}, H_{M})\}\) be the set of variables partitioned in visible and hidden. Let \(\mathcal{D} = \{v^{i},\dots,v^{N}\}\) be the set of observations and \(\{h^{1},\dots, h^{N}\}\) the corresponding values of the hidden variables.

The \emph{energy term} in a Bayesian networks has the form
\[
  \sum_{n=1}^{N} \E_{Q(h^{n}\mid v^{n})}[ \log{P(x^{n} \mid \theta)} ] = \sum_{n = 1}^{N}\sum_{i = 0}^{M} \E_{Q(h^{n} \mid v^{n})} [\log{P(x_{i}^{n} \mid pa(x_{i}^{n}, \theta))}]
\]

It is useful to use the following notation that defines a conditional distribution of the hidden variable when the visible one equals \(v^{n}\).
\[
  Q^{n}(x) = Q^{n}(v,h) = Q(h \mid v^{n}) \mathbb{I}(v = v^{n})
\]
We can define the mixture distribution
\[
  Q(x) = \frac{1}{N}\sum_{n = 1}^{N}Q^{n}(x)
\]
Then we have that
\[
  \begin{aligned}
    \E_{Q(x)}[\log{P(x \mid \theta)}] &= \int_{x}Q(x)\log{P(x \mid \theta)} =  \int_{x} \frac{1}{N}\sum_{n=1}^{N}Q^{n}(x)\log{P(x \mid \theta)} \\
    &= \frac{1}{N} \int_{x}\sum_{n=1}^{N}Q(h \mid v^{n})\mathbb{I}[v = v^{n}]\log{P(x \mid \theta)}\\
    &= \frac{1}{N}\sum_{n = 1}^{N}\E_{Q(h \mid v^{n})} [\log{P(x^{n} \mid \theta)}]
  \end{aligned}
\]

Using the Belief Network structure
\[
  \begin{aligned}
    \E_{Q(x)}[\log{P(x \mid \theta)}] &= \sum_{i = 1}^{M}\E_{Q(x)}[\log{P(x_{i} \mid pa(x_{i}, \theta))}]\\
    &= \sum_{i=1}^{M} \int_{x}Q(x) \log{P(x_{i} \mid pa(x_{i}, \theta))}\\
    &= \sum_{i = 1}^{M}\E_{Q(pa(x_{i}), theta)}\Big[ \E_{Q(x_{i}\mid pa(x_{i}), \theta)} [ \log{P(x_{i} \mid pa(x_{i}), \theta)}  ] \Big]
  \end{aligned}
\]

We add a constant to the last term so it comes with the structure of a Kullback-Leibler Divergence (notice it has its sign has changed)
\[
  \sum_{i = 1}^{M}\E_{Q(pa(x_{i}))}\Big[[ \E_{Q(x_{i}\mid pa(x_{i}), \theta)} [ \log{Q(x_{i} \mid pa(x_{i}))}  ] - \E_{Q(x_{i}\mid pa(x_{i}), \theta)} [ \log{P(x_{i} \mid pa(x_{i}), \theta)}  ] \Big] =
\]
\[
  = \sum_{i = 1}^{M} E_{Q(pa(x_{i}))} \Big[KL \Big( Q(x_{i}\mid pa(x_{i}), \theta) \mid P(x_{i} \mid pa(x_{i}), \theta) \Big) \Big]
\]

So maximizing the energy term is equivalent to minimize the above formula, that is, setting
\[
  P(x_{i} \mid pa(x_{i}), \theta) = Q(x_{i} \mid pa(x_{i}))
\]
So the first observation is that \(\theta\) is not needed in order to maximize the energy term due to the Belief Network structure. The second one is that storing the full \(Q(x)\) on each iteration is not needed as only the distribution on the family of each variable \(X_{i}\) is required by the M-step.

The M-step is then equivalent to set
\[
  P(x_{i}\mid pa(x_{i})) = Q(x_{i} \mid pa(x_{i})) = \frac{\sum_{n = 1}^{N} Q^{n}(x_{i}, pa(x_{i}))}{\sum_{n=1}^{N} Q^{n}(pa(x_{i}))}
\]

\section{EM Extensions}

\subsection{Partial steps}

Making a partial M-step consist on not using the optimal parameter for the energy term, but using one with just higher energy. Finding this values can be easier than finding the optimal one and convergence still follows as the only requirement to make the likelihood increase was to increase the lower bound.

On the other hand, when studying the increase on the likelihood, we supposed that the optimal E-step was being used. In general, we cannot guarantee that a partial step would increase the likelihood. In fact, it is guaranteed to increase the lower bound, but nothing can be said about the log likelihood.


Another important factor is, that the EM algorithm assumes that the energy term is possible to calculate, which may not be. As an approach to solve this situation, we can set a clsas of distributions \(\mathcal{Q}\), and minimize the KullBack-Leibler divergence between \(P(h \mid v, \theta_{1})\) and a distribution \(Q \in \mathcal{Q}\), so we pick a distribution such that
\[
  Q = \argmin_{Q \in \mathcal{Q}} KL \big( Q(h \mid v, \theta_{2}) \mid P(h\mid v, \theta_{1}) \big)
\]

An extreme case is to choose \(\mathcal{Q}\) as delta functions, where the energy term is now a constant. And the optimal chose setting is
\[
Q(h^{n} \mid v^{n}) = \delta (h^{n}, h^{n}_{opt}) \hspace{2cm} h^{n}_{opt} = \argmax_{h}P(h , v^{n} \mid \theta)
\]
This is called \emph{Viterbi training} and does not guarantee that the log likelihood is being increased in each iteration.

\section{Variational Bayes}

Another method to deal with hidden variables is using \emph{Variational Bayes (VB)}, in contrast with the EM algorithm, this one uses a distribution that better represents the posterior than the one using a Maximum Likelihood approach.

Consider a simple datapoint \(x = (v,h)\), in this situation we focus our interest on the posterior distribution.
\[
  P(\theta, v) = \frac{P(v \mid \theta)P(\theta)}{P(v)} = \frac{1}{P(v)}\int_{h}P(v,h \mid \theta)P(\theta)
\]

Variational Bayes tries to factorize the joint hidden a parameter posterior, i.e, computes two distributions, one over the hidden variable and one over the parameter such that
\[
  P(h ,\theta \mid v) \approx Q(h)Q(\theta)\footnote{We use the same letter for both distributions, as they can be differenced from the context.}
\]

To achieve that, we minimize the Kullback-Leibler divergence between them.

\[
  KL\big( Q(h)Q(\theta) \mid P(h, \theta \mid v)  \big) = \Ex{Q(h)}{\log{Q(h)}} + \Ex{Q(\theta)}{\log{Q(\theta)}} - \Ex{Q(h)Q(\theta)}{\log{P(h,\theta \mid v)}} \geq 0
\]
