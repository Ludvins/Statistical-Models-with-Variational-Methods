
All our theory will be made under the assumption that there is a
\emph{referential set} \(\Omega\), set of all possible outcomes of an experiment. Any subset of
\(\Omega\) will be called an \emph{event}.

\begin{definition}
Let \(\mathcal{P}(\Omega)\) be the power set of \(\Omega\). Then, \(\mathcal{F} \subset \mathcal{P}(\Omega)\) is called a
\emph{\(\sigma\)-algebra} if it satisfies:
\begin{itemize}
\item \(\Omega \in \mathcal{F}\).
\item \(\mathcal{F}\) is closed under complementation.
\item \(\mathcal{F}\) is closed under countable unions.
\end{itemize}
From these properties it follows that \(\emptyset \in \mathcal{F}\) and that \(\mathcal{F}\)
is closed under countable intersections.

The tuple \((\Omega, \mathcal{F})\) is called a \emph{measurable space}.
\end{definition}

\begin{definition}
A \emph{probability} \(P\) over \((\Omega, \mathcal{F})\) is a mapping
\(P: \mathcal{F} \to [0,1]\) which satisfies
\begin{itemize}
\item \(P(\alpha) \geq 0 \ \ \forall \alpha \in \mathcal{F}\).
\item \(P(\Omega) = 1\).
\item \(P\) is countably additive, that is, if \(\{\alpha_i\}_{i \in \mathbb{N}}
  \subset \mathcal{F}\), is a countable collection of pairwise disjoint sets,
  then
  \[
  P\big(\bigcup_{i\in \mathbb{N}}\alpha_i\big) = \sum_{i\in \mathbb{N}}P(\alpha_i).
  \]
\end{itemize}
\end{definition}

The first condition guarantees non negativity. The second one states that the
\emph{trivial event} has the maximal possible probability of 1.
The third condition implies that given a set of pairwise disjoint events,
the probability of either one of them occurring is equal to the sum of the
probabilities of each one.

From these conditions it follows that
\begin{itemize}
\item \(P(\emptyset) = 0\)
\item \(P(\alpha \cup \beta) = P(\alpha) + P(\beta) - P(\alpha \cap \beta)\)
\end{itemize}

The triple \((\Omega, \mathcal{F}, P)\) is called a \emph{probability space}.

\begin{definition}
A function \(f:\Omega_1 \to \Omega_2\) between two
measurable spaces \((\Omega_1, \mathcal{F}_1)\) and \((\Omega_2, \mathcal{F}_2)\) is said to be \emph{measurable} if \(f^{-1}(\alpha) \in \mathcal{F}_1\) for every \(\alpha \in \mathcal{F}_2\).
\end{definition}

\begin{definition}
A \emph{random variable} is a measurable function \(X:\Omega \to E\) from a probability
space \((\Omega, \mathcal{F}, P)\) to a measurable space \(E\).

The probability of \(X\) taking a value on a measurable set \(S \subset E\) is
written as
\[
P_X(S) = P(X \in S) = P(\{a \in \Omega \ | \ X(a) \in S \}).
\]
\end{definition}

We could make a question like ``How likely is that the value of \(X\) equals
\(a\)?''. This is the same as asking for the probability of the set \(\{\omega
\in \Omega \ | \ X(w) = a\}\).

We define the \emph{probability distribution} of
an experiment as a function that provides the probability
of occurrence of the different events in \(\Omega\). With this, if  \(X\) is used to denote the outcome of the experiment, the probability
distribution of \(X\) would be a function that gives the probability of every
state of \(X\). With this approach, we use the random variable \(X\) to
``push-forward'' the probability \(P\) on \(\Omega\) to a probability \(P_X\) in
the measurable space \(E\). Typically, this
measurable space is the set of real numbers \(\mathbb{R}\) along with Borel's
\(\sigma\)-algebra \(\mathcal{B}\).

The probability distribution of a random variable is typically described by the \emph{probability
  mass function} or the \emph{probability density function}, depending on
whether \(X\) is discrete or not. We will define this concepts after setting the
notation that is going to be used, that is: random variables will be
denoted with an upper case letter like \(X\) and a set of variables with a
bold symbol like \(\bm{X}\). The meaning of \(P(state)\) will be clear without a reference to the variable.
Otherwise \(P(X = state)\) will be used.
Using a lower case letter like \(P(x)\) will denote the probability of the
corresponding upper case variable \(X\) taking a specific value.

\begin{definition}
The \emph{cumulative distribution function} of a random variable \(X\) is the
function given by:
\[
F_X (x) = P(X \leq x)
\]
where the right-hand side represents the probability of the random variable
taking value below or equal to \(x\).
\end{definition}

\begin{definition}
When the image of a random variable \(X\) is countable, the random variable it
is called a
\emph{discrete random variable}, its \emph{probability mass function} \(p\) gives the
probability of it being equal to some value.
\[
p(x) = P(X = x)
\]
If the image is uncountable then \(X\) is called a \emph{continuous random
  variable} and its \emph{probability density function} \(f\) is a non-negative
Lebesgue-integrable such that
\[
F_X(x) = P(X \leq x) = \int_{-\infty}^x f(u) du
\]
\end{definition}


\begin{definition}
  A \emph{multivariate random variable} or \emph{random vector} is a column vector \(\bm{X} =
  (X_1,\dots,X_n)^T\) whose components are real-valued random variables on the
  same probability space.

  Note that we use the same symbol \(\bm{X}\) for random vectors and sets of
  variables, but the meaning will be clear within the context.
\end{definition}

Now we are going to define some concepts over events \(\alpha, \beta \in \mathcal{F}\), then we will set those
over random variables and their distributions.

\begin{itemize}
\item The \emph{joint probability} \(P(\alpha, \beta)\) the probability of both
  events occurring.
\item The \emph{marginal probability} \(P(\alpha)\) is the probability of occurring
\(\alpha\) irrespective of the other event.
\item The \emph{conditional
  probability} \(P(\alpha | \beta)\) is the probability of occurring \(\alpha\)
given \(\beta\).
\end{itemize}

Let's illustrate this with a simple example.

\begin{exampleth}
  Suppose \(\Omega\) is the set of outcomes of rolling a dice, that is \(\Omega
  = \{1, 2, 3, 4, 5, 6\}\). We define \(\alpha = \{2, 4, 6\}\) and
  \(\beta = \{4, 5\}\). Then the joint probability is \(P(\{4\}) =
  1/6\), the marginal probability \(P(\alpha) = 3/6\) is easy to calculate as we
  can do it straight-forward, but if we couldn't, we should use the joint
  probability.
  We said that the marginal \(P(\alpha)\) is the probability of \(\alpha\)
  irrespective of \(\beta\). Then, we can calculate it as the probability of
  occurring both \(P(\alpha, \beta)\) plus the probability of occurring
  \(\alpha\) and not \(\beta\) \(P(\alpha, \{1,2,3,6\})\).

  The conditional probability \(P(\alpha | \beta)\) forces the outcome to be
  either \(4\) or \(5\), and the only option for it to be in
  \(\alpha\) is that it is \(4\), then \(P(\alpha | \beta) = 0.5\).
\end{exampleth}

\begin{definition}
  Let \(\bm{X} = \{X_1, X_2,\dots,X_n\}\) be a set of random variables, the
  \emph{joint probability distribution} for \(\bm{X}\) is function that gives the probability of each random variable \(X_i\)
  falling in a particular range or discrete set of values for that variable. It is
  called a \emph{multivariate distribution}.

  When using only two random variables, then is called a \emph{bivariate
    distribution}.

  This distribution can be expressed in terms of a joint cumulative distribution
  function
  \[
F_{\bm{X}}(\bm{x}) = F_{X_1,\dots,X_n}(x_1,\dots,x_n) = P(X_1 \leq x_1, \dots,
X_n \leq x_n) \footnote{Where \(\bm{x} = (x_1,\dots,x_n)\)}
\]
or using a probability density function (all variables must be continuous) or a
probability mass function (all variables must be discrete).
\end{definition}

\begin{definition}
  The \emph{marginal distribution} of a subset of random variables is the
  probability distribution of the variables contained in that subset.
\end{definition}

Let \(X, Y\) be two random variables, it follows that\footnote{Using both
  continuous and discrete notation}
\[
P(x) = \sum_y P(x,y) \hspace{2cm} P(x) = \int_y P(x,y)
\]

\begin{definition}
The \emph{conditional probability} of a subset of random variables is the
probability distribution of them when the rest of the variables are known to be
a particular value.
\end{definition}


\begin{theorem}
  \textbf{(Bayes' theorem)}. Let \(\alpha, \beta\) be two events of an
  experiment, given that \(P(\beta) \neq 0\). Then
  \[
  P(\alpha|\beta)= \frac{P(\beta|\alpha)P(\alpha)}{P(\beta)}
\]

We can extend this theorem to a pair of random variables \(X, Y\) as
  \[
  P(x|y)= \frac{P(y|x)P(x)}{P(y)}
\]
\end{theorem}



\begin{exampleth}
Consider a study where the relation of a disease \(d\) and an habit \(h\)
is being investigated. Suppose that \(P(d)=10^{-5}\), \(P(h)=0.5\) and \(P(h|d) = 0.9\). What is the
probability that a person with habit \(h\) will have the disease \(d\)?

\[
P(d|h) = \frac{P(d,h)}{P(h)} = \frac{P(h|d)P(d)}{P(h)} =
\frac{ 0.9 \times 10^{-5}}{ 0.5 } = 1.8 \times 10^{-5}
\]

If we set the probability of having habit \(h\) to a much lower value as \(P(h) =
0.001\), then the above calculation gives approximately \(1/100\). Intuitively, a smaller number of people have the habit and most of them have the
desease. This means that the relation between having the desease and the habit
is stronger now compared with the case where more people had the habit.
\end{exampleth}

\begin{definition}
We say that two random variables \(X\) and \(Y\) are \emph{independent} if knowing one of them doesn't give any extra information about the other. Mathematically,
\[
P(x,y) = P(x)P(y)
\]
From this it follows that if \(X\) and \(Y\) are independent, then \(P(x|y) = P(x)\).
\end{definition}


\begin{definition}
Let \(X,Y\) and \(Z\) be three random variables, then \(X\) and \(Y\) are
\emph{conditionally independent} given \(Z\) if and only if
\[
P(x,y | z) = P(x|z)P(y|z)
\]
in that case we will denote \(X \bigCI Y \mid Z\). If \(X\) and \(Y\) are not
conditionally independent, they are \emph{conditionally dependent} \(X \bigCD Y \mid Z\)

\end{definition}

Both independence definitions can be made over sets of variables \(\bm{X},
\bm{Y}\) and \(\bm{Z}\) in a straight forward way.


\begin{definition}
  We say that a set of \(n\) random variables \(\{X_1,\dots,X_n\}\) defined to
  assume values in \(I \subset \R\) are
  \emph{independent and identically distributed (i.i.d)}
  if and only if they are independent
  \[
    F_{X_1,\dots,X_n}(x_1,\dots,x_n) = F_{X_1}(x_1)\dots F_{X_n}(x_n) \ \forall
    x_1,\dots,x_n \in I
  \]
  and are identically distributed
  \[
    F_{X_1}(x_1) = F_{X_k}(x_k) \ \forall k \in \{2,\dots,n\} \text{ and } \forall x
    \in I
  \]


\end{definition}
