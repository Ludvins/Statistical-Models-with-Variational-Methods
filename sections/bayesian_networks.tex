Consider we have \(N\) variables with the corresponding distribution
\(P(x_1,\dots,x_N)\). Let \(\mathcal{E}\) be a set of indexes such as \texttt{evidence}
\(=\{X_e = x_e \ | \ e \in \mathcal{E}\}\). Inference could be made by brute
force:

\[
P(X_i = x_i \ | \ \texttt{evidence}) = \frac{ \int_{ j \not \in
\mathcal{E}, j \neq i } P(\texttt{evidence}, x_j, X_i = x_i)}{ \int_{ j
\not \in \mathcal{E} } P(\texttt{evidence}, x_j)}
\]

The notation when using discrete variables is analogous replacing integration
with summations.

Lets suppose all these variables are binary, this calculation will require
\(O(2^{N-\#\mathcal{E}})\) operations. Also, all entries of a table \(P(x_1,\dots,
x_N)\) take \(O(2^N)\) space.

This is unpractical when taking into account millions of variables. The
underlying idea of belief networks is to specify which variables are independent
of others, factoring the joint probability distribution.

\begin{definition}
Let \(G=(V,E)\) be a graph where \(V = \{X_1,\dots,X_n\}\) is a set of random
variables. We say that the joint
probability \(P(x_1, \dots, x_n)\) \emph{factorizes} according to \(G\) if and
only if
\[
P(x_1,\dots,x_N) = \prod_{i=1}^{N}P(x_i | pa(x_i))
\]
\end{definition}

\begin{definition}
A \emph{belief network or Bayesian network} is a pair \((G, P)\)
where \(P\) factorizes over \(G\). It is a probabilistic graphical model
that represents conditional dependencies of a set of variables \(X_1,\dots, X_n\).
\end{definition}

\begin{figure}
  \centering
  \begin{tikzpicture}[
    node distance=1.5cm and 1.5cm,
    mynode/.style={draw,circle,text width=0.5cm,align=center}
    ]

    \node[mynode] (1) {\(X_1\)};
    \node[mynode,right=of 1] (2) {\(X_2\)};
    \node[mynode,right=of 2] (3) {\(X_3\)};
    \node[mynode,right=of 3] (4) {\(X_4\)};

    \path (4) edge[-latex][bend right] (1)
    (3) edge[-latex] (2)
    (4) edge[-latex][bend right] (2)
    ;

    \end{tikzpicture}
    \captionof{figure}{Bayesian Network factorizing \(P(x_1, x_2, x_3, x_4) = P(x_1 | x_4)P(x_2| x_3, x_4)P(x_3)P(x_4)\)}
    \label{fig:bn_example}
\end{figure}


Any probability distribution can be written as a Bayesian Network, even though
it may end up been a fully-connected DAG.
To set the specification of the Belief Network, we need to define all elements of the probability
tables \(P(x_i|pa(x_i))\). When the number of variables is large, this is still
intractable so the tables are generally parameterized is a low dimensional
manner.

Bayesian Networks are good for encoding conditional independence over the
variables, but aren't for encoding dependence. For example, with the following
network \(P(x,y) = P(y|x)P(x)\) represented as \(x \to y\) in a DAG.
It may appear to encode dependence between both variables but the
conditional \(P(y|x)\) could happen to equal \(P(y)\), giving \(P(x,y) = P(x)P(y)\).

How could we check if two variables are conditionally independent given a
Bayesian Network? For example in figure \ref{fig:relations}, \(X_1 \bigCI
X_2 \mid X_4\) as\footnote{Continuous variable notation is used}:
\[
\begin{aligned}
P(x_2 | x_4) &= \frac{1}{P(x_4)}\int_{x_1,x_3}P(x_1, x_2, x_3, x_4)
= \frac{1}{P(x_4)}\int_{x_1,x_3}P(x_1|x_4)P(x_2|x_3,x_4)P(x_3)P(x_4)\\
                 &= \int_{x_3}P(x_2|x_3, x_4)P(x_3)
\end{aligned}
\]
\[
\begin{aligned}
P(x_1, x_2 | x_4) &= \frac{1}{P(x_4)}\int_{x_3}P(x_1, x_2, x_3, x_4)
= \frac{1}{P(x_4)}\int_{x_3}P(x_1|x_4)P(x_2|x_3,x_4)P(x_3)P(x_4)\\
                 &= P(x_1|x_4)\int_{x_3}P(x_2|x_3, x_4)P(x_3) = P(x_1|x_4)P(x_2|x_4)
\end{aligned}
\]

Now we are going to define two central concepts to determine conditional
independence in any Bayesian Network, these are \emph{d-connection} and \emph{d-separation}.

\begin{definition}
Let \(G\) be a DAG where \(\bm{X}, \bm{Y} \text{ and } \bm{Z}\)
are disjoint sets of vertices. We say that \(\bm{X} \text{ and
} \bm{Y}\) are \emph{d-connected} by \(\bm{Z}\) if and only if there
exists an undirected path \(U\) from any vertex in \(\bm{X}\) to any
vertex in \(\bm{Y}\) such that:
\begin{itemize}
\item For any collider \(C\), itself or any it's descendants is in \(\bm{Z}\)
\item No non-collider on \(U\) is on \(\bm{Z}\)
\end{itemize}
\end{definition}

\begin{definition}
Let \(G\) be a DAG where \(\bm{X}, \bm{Y} \text{ and } \bm{Z}\)
are disjoint sets of vertices. \(\bm{X}\) and \(\bm{Y}\)
are \emph{d-separated} by \(\bm{Z}\) if and only if they are not
d-connected by \(\bm{Z}\) in \(G\)
\end{definition}

\begin{figure}[h]
\centering
\begin{tikzpicture}[
  node distance=1cm and 0.5cm,
  mynode/.style={draw,circle,text width=0.5cm,align=center}
]

\node[mynode] (a) {a};
\node[mynode, below right=of a] (d) {d};
\node[mynode,above right=of d] (b) {b};
\node[mynode, below right=of b] (e) {e};
\node[mynode,above right=of e] (c) {c};

\path (a) edge[-latex] (d)
(b) edge[-latex] (d)
(c) edge[-latex] (e)
(b) edge[-latex] (e)
;

\end{tikzpicture}
\caption{D-separation example}
\label{fig:d-sep}
\end{figure}

For example, in figure \ref{fig:d-sep} \(d\) d-separates \(a\) and \(c\) (\(b\)
is a collider in the path that isn't in \(\{d\}\)),
and \(\{d,e\}\) d-connect them.

\begin{theorem}[\cite{pearl_and_detcher}]\label{th:d-separation}
Let \(G\) be a DAG where \(\bm{X}, \bm{Y} \text{ and } \bm{Z}\)
are disjoint sets of vertices. If  \(\bm{X}\) and \(\bm{Y}\)
are d-separated by \(\bm{Z}\), then they are independent conditional
on \(\bm{Z}\) in all probability distributions that G can represent.
\end{theorem}

The Bayes Ball algorithm [\cite{bayes_ball}] provides a linear time complexity
algorithm that computes conditional independent using this theorem.


\begin{exampleth}
In this example we are modeling three discrete random variables: Sprinkler (\(S\)),
Rain (\(R\)) and Grass wet (\(G\)).

The joint probability function is:
\[
P(s,r,g) = P(s|r)P(g|s,r)P(r)
\]

The following DAG illustrates the Bayesian Network among with the probability
tables we are using.

\begin{tikzpicture}[
  node distance=0.6cm and 0cm,
  mynode/.style={draw,ellipse,text width=2cm,align=center}
]
\node[mynode] (sp) {Sprinkler};
\node[mynode,below right=of sp] (gw) {Grass wet};
\node[mynode,above right=of gw] (ra) {Rain};
\path (ra) edge[-latex] (sp)
(sp) edge[-latex] (gw)
(gw) edge[latex-] (ra);
\node[left=0.5cm of sp]
{
\begin{tabular}{cm{1cm}m{1cm}}
\toprule
& \multicolumn{2}{c}{Sprinkler} \\
Rain & \multicolumn{1}{c}{T} & \multicolumn{1}{c}{F} \\
\cmidrule(r){1-1}\cmidrule(l){2-3}
F & 0.4 & 0.6 \\
T & 0.01 & 0.99 \\
\bottomrule
\end{tabular}
};
\node[right=0.5cm of ra]
{
\begin{tabular}{m{1cm}m{1cm}}
\toprule
\multicolumn{2}{c}{Rain} \\
\multicolumn{1}{c}{T} & \multicolumn{1}{c}{F} \\
\cmidrule{1-2}
0.2 & 0.8 \\
\bottomrule
\end{tabular}
};
\node[below=0.5cm of gw]
{
\begin{tabular}{ccm{1cm}m{1cm}}
\toprule
& & \multicolumn{2}{c}{Grass wet} \\
\multicolumn{2}{l}{Sprinkler Rain} & \multicolumn{1}{c}{T} & \multicolumn{1}{c}{F} \\
\cmidrule(r){1-2}\cmidrule(l){3-4}
F & F & 0.0 & 1.0 \\
F & T & 0.8 & 0.2 \\
T & F & 0.9 & 0.1 \\
T & T & 0.99 & 0.01 \\
\bottomrule
\end{tabular}
};
\end{tikzpicture}

This model can answer questions about the presence of a cause given the presence
of an effect. For example, What is the probability that it has being raining
given the grass is wet?

\[
P(R = T | G = T) = \frac{P(G = T, R = T)}{P(G=T)} = \frac{\sum_{s}P(G=T, R=T,
s)}{\sum_{r,s} P(G=T, r, s)}
\]

Using the expression of the joint probability among with the tables we can
compute every term. For example:
\[
\begin{aligned}
P(G=T, R=T, S=T) &= P(S=T|R=T)P(G=T|R=T,S=T)P(R=T) \\
&= 0.01 * 0.99 * 0.2 = 0.00198
\end{aligned}
\]
\end{exampleth}


In some situations our Belief Networks will contain a number of nodes that are
essentially the same but repeated a number of times, for this, we are going to
introduce the \emph{plate notation}. Suppose we have the situation that figure
\ref{fig:plate_notation} shows on the left. The we can collapse all \(B_i\)
variables in a box, indicating there number of variables inside it.

\begin{figure}[h]
\centering
\begin{tikzpicture}[
  node distance=1cm and 0.5cm,
  mynode/.style={draw,circle,text width=0.5cm,align=center}
]

\node[mynode] (a) {A};
\node[mynode,below=of a] (d) {\(B_1\)};
\node[mynode,left=of d] (c) {\(B_2\)};
\node[mynode,left=of c] (b) {\(B_3\)};
\node[mynode,right=of d] (e) {\(\dots\)};
\node[mynode,right=of e] (f) {\(B_n\)};

\node[mynode,right=2cm of f] (g) {\(B_i\)};
\node[mynode, above=of g] (h) {A};
\plate{} {(g)} {\(n\)}; %


\path (a) edge[-latex] (b)
(a) edge[-latex] (c)
(a) edge[-latex] (d)
(a) edge[-latex] (e)
(a) edge[-latex] (f)
(h) edge[-latex] (g)
;

\end{tikzpicture}
\caption{Plate notation example. Standard notation on the left and plate on the right}
\label{fig:plate_notation}
\end{figure}
