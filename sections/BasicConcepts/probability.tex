
All our theory will be made under the assumption that there is a
\emph{referential set} \(\Omega\), set of all possible outcomes of an experiment. Any subset of
\(\Omega\) will be called an \emph{event}.

\begin{definition}
Let \(\mathcal{P}(\Omega)\) be the power set of \(\Omega\). Then, \(\mathcal{F} \subset \mathcal{P}(\Omega)\) is called a
\emph{\(\sigma\)-algebra} if it satisfies:
\begin{itemize}
\item \(\Omega \in \mathcal{F}\).
\item \(\mathcal{F}\) is closed under complementation.
\item \(\mathcal{F}\) is closed under countable unions.
\end{itemize}
From these properties it follows that \(\emptyset \in \mathcal{F}\) and that \(\mathcal{F}\)
is closed under countable intersections.

The tuple \((\Omega, \mathcal{F})\) is called a \emph{measurable space}.
\end{definition}

\begin{definition}
A \emph{probability} \(P\) over \((\Omega, \mathcal{F})\) is a mapping
\(P: \mathcal{F} \to [0,1]\) which satisfies
\begin{itemize}
\item \(P(\alpha) \geq 0 \ \ \forall \alpha \in \mathcal{F}\).
\item \(P(\Omega) = 1\).
\item \(P\) is countably additive, that is, if \(\{\alpha_i\}_{i \in \mathbb{N}}
  \subset \mathcal{F}\), is a countable collection of pairwise disjoint sets,
  then
  \[
  P\big(\bigcup_{i\in \mathbb{N}}\alpha_i\big) = \sum_{i\in \mathbb{N}}P(\alpha_i).
  \]
\end{itemize}
\end{definition}

The first condition guarantees non negativity. The second one states that the
\emph{trivial event} has the maximal possible probability of 1.
The third condition implies that given a set of pairwise disjoint events,
the probability of either one of them occurring is equal to the sum of the
probabilities of each one.

From these conditions it follows that
\begin{itemize}
\item \(P(\emptyset) = 0\)
\item \(P(\alpha \cup \beta) = P(\alpha) + P(\beta) - P(\alpha \cap \beta)\)
\end{itemize}

The triple \((\Omega, \mathcal{F}, P)\) is called a \emph{probability space}.

\begin{definition}
  Given two events \(\alpha, \beta \in \mathcal{F}\), with \(P(\beta) \neq 0\),
  the conditional probability of \(\alpha\) given \(\beta\) is defined as the
  quotient of the probability of the joint events and the probability of
  \(\beta\):
  \[
    P(\alpha \mid \beta) = \frac{P(\alpha \cap \beta)}{P(\beta)}
  \]
\end{definition}



\begin{theorem}
  \textbf{(Bayes' theorem)}. Let \(\alpha, \beta\) be two events of an
  experiment, given that \(P(\beta) \neq 0\). Then
  \[
  P(\alpha \mid \beta)= \frac{P(\beta \mid \alpha)P(\alpha)}{P(\beta)}
\]
\end{theorem}



\begin{exampleth}
Consider a study where the relation of a disease \(d\) and an habit \(h\)
is being investigated. Suppose that \(P(d)=10^{-5}\), \(P(h)=0.5\) and \(P(h\mid d) = 0.9\). What is the
probability that a person with habit \(h\) will have the disease \(d\)?

\[
P(d \mid h) = \frac{P(d \cap h)}{P(h)} = \frac{P(h \mid d)P(d)}{P(h)} =
\frac{ 0.9 \times 10^{-5}}{ 0.5 } = 1.8 \times 10^{-5}
\]

If we set the probability of having habit \(h\) to a much lower value as \(P(h) =
0.001\), then the above calculation gives approximately \(1/100\). Intuitively, a smaller number of people have the habit and most of them have the
desease. This means that the relation between having the desease and the habit
is stronger now compared with the case where more people had the habit.
\end{exampleth}

\begin{definition}
  We say that two events \(\alpha, \beta \in \mathcal{F}\) are
  \emph{independent} if knowing one of them does not give any extra information
  about the other. Mathematically,

  \[
    P(\alpha \cap \beta) = P(\alpha)P(\beta) \hspace{2cm} P(\alpha \mid \beta) = P(\alpha)
  \]

  Let \(\gamma \in \mathcal{F}\), we say that \(\alpha\) and \(\beta\) are
  \emph{conditionally independent} on \(\gamma\), \(\alpha \bigCI \beta \mid \gamma\)
  if and only if
  \[
    P(\alpha \cup \beta \mid \gamma) = P(\alpha \mid \gamma)P(\beta \mid \gamma)
  \]
  Otherwise, they are said to be \emph{conditionally dependent} on \(\gamma\),  \(\alpha \bigCD \beta \mid \gamma\).

\end{definition}

Now we are going to introduce the concept of \emph{random variable} and some
properties as we have done with events.

\begin{definition}
A function \(f:\Omega_1 \to \Omega_2\) between two
measurable spaces \((\Omega_1, \mathcal{F}_1)\) and \((\Omega_2, \mathcal{F}_2)\) is said to be \emph{measurable} if \(f^{-1}(\alpha) \in \mathcal{F}_1\) for every \(\alpha \in \mathcal{F}_2\).
\end{definition}

\begin{definition}
  A \emph{random variable} is a measurable function \(X:\Omega \to E\) from a probability
  space \((\Omega, \mathcal{F}, P)\) to a measurable space \((E,
  \mathcal{F}')\) verifying \(X(\omega)\in \mathcal{F}' \ \forall \omega \in \Omega\).

The probability of \(X\) taking a value on a measurable set \(S \in E\) is
written as
\[
P_X(S) = P(X \in S) = P(\{a \in \Omega \ \mid  \ X(a) \in S \}).
\]
\end{definition}

We could make a question like ``How likely is that the value of \(X\) equals
\(a\)?''. This is the same as asking for the probability of the set \(\{\omega
\in \Omega \ \mid  \ X(w) = a\}\).

We will set the following notation that is going to be used, that is: random variables will be
denoted with an upper case letter like \(X\) and a set of variables with a
bold symbol like \(\bm{X}\). The meaning of \(P(state)\) will be clear without a reference to the variable.
Otherwise \(P(X = state)\) will be used.
Using a lower case letter like \(P(x)\) will denote the probability of the
corresponding upper case variable \(X\) taking a specific value.

\begin{definition}
The \emph{cumulative distribution function} of a real-valued random variable \(X\) is the
function given by:
\[
F_X (x) = P(X \leq x)
\]
where the right-hand side represents the probability of the random variable
taking value below or equal to \(x\).
\end{definition}

\begin{definition}
When the image of a random variable \(X\) is countable, the random variable it
is called a
\emph{discrete random variable}, its \emph{probability mass function} \(p\) gives the
probability of it being equal to some value.
\[
p(x) = P(X = x)
\]
If the image is uncountable and real, then \(X\) is called a \emph{continuous random
  variable} if there exists is a non-negative
Lebesgue-integrable \(f\), called its \emph{probability density function} such that
\[
F_X(x) = P(X \leq x) = \int_{-\infty}^x f(u) du
\]

A \emph{mixed random variable} is a random variable who is neither discrete nor
continuous, it can be realized as the sum of a discrete and continuous random
variables. An example of a random variable of mixed type would be based on an
experiment where a coin is flipped and a random positive number is chose only if
the result of the coin toss is heads, $-1$ otherwise.
\end{definition}

From now on, \(P(x)\) will denote \(f_X(x)\) when \(X\) is a continuous random
variable. We will define the \emph{probability distribution} \(P_X\) of a random
variable \(X\) over the probability space \((\Omega, \mathcal{F}, P)\)
as the pushforward measure of it, that is, \(P_X = PX^{-1}\).

As summation is integration with respect to the \emph{counting measure} defined as

\[
 \#(dx) = \sum_{n \in \I}\delta(x - n)dx
\]
Where \(\I\) is the set of values \(X\) can take, and \(\delta\) is the Dirac distribution.
Then

\[
  \int_{x} P(x) \#(dx) = \sum_{n \in \I}\int_{x} P(x) \delta(x-n) dx = \sum_{n \in I}P(n)
\]

Where we used that \(\int f(x)\delta(x - x_{0}) = f(x_{0})\). Given this, from now on, we will use the integration notation for both discrete and continuous variables given that the integrals will be respect to the counting measure when needed.

\begin{definition}
  As we did for events, we can define the \emph{conditional probability} over
  random variables, let \(X, Y\) be random variables,
  \[
    P(x \mid y) = \frac{P(x,y)}{P(y)}
  \]
  It is  required that \(P(y) \neq 0\) for the conditional probability to be defined.
\end{definition}

We can also enunciate the \emph{Bayes' theorem}.

\[
  P(x,y) = \frac{P(y\mid x)P(x)}{P(y)}
\]


\begin{definition}
  The \emph{marginal distribution} of a subset of random variables is the
  probability distribution of the variables contained in that subset.
\end{definition}

Let \(X, Y\) be two random variables, it follows that
\[
  P(x) = \int_y P(x,y)
\]


\begin{definition}
  Let \(\bm{X} = \{X_1, X_2,\dots,X_n\}\) be a set of random variables, the
  \emph{joint probability distribution} for \(\bm{X}\) is function that gives the probability of each random variable \(X_i\)
  falling in a particular range or discrete set of values for that variable. It is
  called a \emph{multi-variate distribution}.

  When using only two random variables, then is called a \emph{bivariate
    distribution}.

  This distribution can be expressed in terms of a joint cumulative distribution
  function
  \[
F_{\bm{X}}(\bm{x}) = F_{X_1,\dots,X_n}(x_1,\dots,x_n) = P(X_1 \leq x_1, \dots,
X_n \leq x_n) \footnote{Where \(\bm{x} = (x_1,\dots,x_n)\)}
\]
or using a probability density function (all variables must be continuous) or a
probability mass function (all variables must be discrete).
\end{definition}

\begin{definition}
We say that two random variables \(X\) and \(Y\) are \emph{independent} if knowing one of them doesn't give any extra information about the other. Mathematically,
\[
P(x,y) = P(x)P(y)
\]
From this it follows that if \(X\) and \(Y\) are independent, then \(P(x\mid y) = P(x)\).
\end{definition}


\begin{definition}
Let \(X,Y\) and \(Z\) be three random variables, then \(X\) and \(Y\) are
\emph{conditionally independent} given \(Z\) if and only if
\[
P(x,y \mid  z) = P(x\mid z)P(y\mid z)
\]
in that case we will denote \(X \bigCI Y \mid Z\). If \(X\) and \(Y\) are not
conditionally independent, they are \emph{conditionally dependent} \(X \bigCD Y \mid Z\)

\end{definition}

Both independence definitions can be made over sets of variables \(\bm{X},
\bm{Y}\) and \(\bm{Z}\) in a straight forward way.


\begin{definition}
  We say that a set of \(n\) random variables \(\{X_1,\dots,X_n\}\) defined to
  assume values in \(I \subset \R\) are
  \emph{independent and identically distributed (i.i.d)}
  if and only if they are independent
  \[
    F_{X_1,\dots,X_n}(x_1,\dots,x_n) = F_{X_1}(x_1)\dots F_{X_n}(x_n) \ \forall
    x_1,\dots,x_n \in I
  \]
  and are identically distributed
  \[
    F_{X_1}(x_1) = F_{X_k}(x_k) \ \forall k \in \{2,\dots,n\} \text{ and } \forall x
    \in I
  \]


\end{definition}


\begin{definition}
  A \emph{multi-variate random variable} or \emph{random vector} is a column vector \(\bm{X} =
  (X_1,\dots,X_n)^T\) whose components are random variables that can be defined
  over different probability spaces.

  Note that we use the same symbol \(\bm{X}\) for random vectors and sets of
  variables, but the meaning will be clear within the context.
\end{definition}
