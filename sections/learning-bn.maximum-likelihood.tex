
In this section we will introduce two concepts, Maximum Likelihood and Maximum a
Posteriori, showing that \emph{training} a model's parameter to maximize the
Maximum Likelihood equals to take the empirical distribution as it.

\begin{definition}
  Maximum Likelihood is calculated as
  \[
    \theta^{ML} = \argmax_\theta p(\mathcal{V} \mid \theta)
  \]
   it refers to the value of the parameter
\(\theta\) for which the observed data better fits the model.
\end{definition}

\begin{definition}
  Maximum A Posteriori refers to
  \[
    \theta^{MAP} = \argmax_\theta p(\mathcal{V} \mid \theta)P(\theta)
  \]
\end{definition}

The decision of taking the Maximum A Posteriori can be motivated using an
utility that equals zero for all but the correct parameter
\[
  U(\theta, \theta_{true}) = \mathbb{I}[\theta = \theta_{true}]
\]

using this, the expected utility of a parameter \(\theta = \theta_0\) is

\[
  U(\theta = \theta_0) = \sum_{\theta_{true}}\mathbb{I}[\theta_{true} = \theta_0]P(\theta = \theta_{true}  \mid  \mathcal{V}) = P(\theta_0  \mid  \mathcal{V})
\]

This means that the maximum utility decision is to take the value \(\theta_0\)
with the highest posterior value.

\begin{remark}
When using a flat prior \(\theta^{ML}= \theta ^{MAP}\).
\end{remark}

\section{Relation with Kullback-Leibler divergergence}

Now, we are going to show the relation between the Maximum Likelihood and the
Kullback-Leibler divergence of the empirical distribution and our model.
Firstly, we define the empirical distribution of a set

Let \(\{X_1, \dots, X_m\}\) be a set of real i.i.d random variables and
\(\{x_{1}, \dots, x_{m}\}\) a set of observations of those variables, we can define the
empirical distribution as a distribution whose probability mass function
\(Q\) is
\[
  Q(x) = \frac{1}{m}\sum_{i = 1}^m \mathbb{I}[x = x_i]
\]

 We may calculate this Kullback-Leibler divergence and study their functional independence.
\[
  KL(Q \mid P) = \E{Q}{\log(Q(x))} - \E{Q}{\log(P(x))}
\]

Notice the term \(\E{Q}{\log(Q(x))}\) is a constant as the variables are
i.i.d it follows that
\[
   \E{Q}{\log(P(x))} = \frac{1}{m}\sum_{i = 1}^mlogP(x_i)
 \]
 where the right side is the log likelihood under \(Q\). As the logarithm is
 a strictly increasing function, maximizing the log likelihood equals to
 maximize the likelihood itself, and we can see here how it is equivalent to
 minimize the Kullback-Leibler divergence between the empirical distribution and
 our distribution.

 In case \(P(x)\) is unconstrained, the optimal choice is \(P(x) = Q(x)\), that
 is, the maximum likelihood distribution corresponds to the empirical distribution.

 For a Belief Network we know there is the following constraint
 \[
   P(x_{1}, \dots, x_{m}) = \prod_{i = 1}^K P(x_i  \mid  pa(x_i))
 \]
 We now want to minimize the Kullback-Leibler divergence between the empirical
 distribution \(Q(x)\) and \(P(x)\) in order to get the Maximum Likelihood.
 \[
   \begin{aligned}
   KL(Q \mid P) &= - \E{Q}{\sum_{i = 1}^K\log{P(x_i \mid pa(X_i))}} +
   \E{P}{\sum_{i = 1}^K\log{P(x_i \mid pa(X_i))}}
   \\ &= - \sum_{i =
     1}^K \E{Q}{\log{P(x_i \mid pa(X_i))}} + \sum_{i =
     1}^K \E{Q}{\log{P(x_i \mid pa(X_i))}}
   \end{aligned}
 \]

 We can not use Proposition \ref{prop:expectation_over_marginal} on \(\log{P(x_i \mid pa(x_i))}\).

 \[
   \begin{aligned}
     KL(Q \mid P) &= \sum_{i = 1}^K \E{Q(x_i,pa(x_i))}{\log{Q(x_i \mid pa(x_i))}} - \E{Q(x_i,pa(x_i))}{\log{P(x_i \mid pa(x_i))}}\\
     &= \sum_{i = 1}^K \E{Q(x_i,pa(x_i))}{KL\Big(Q(x_i \mid pa(x_i)) \mid P(x_i \mid pa(x_i))\Big)}
   \end{aligned}
 \]

 The minimal setting is then
 \[
   P(x_i \mid pa(x_i)) = Q(x_i \mid pa(x_i))
 \]
 in terms of the initial data it is to set \(P(x_i \mid pa(x_i))\) to the number of
 times the state appears in it.
