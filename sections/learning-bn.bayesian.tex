A Bayesian approach where we set a distribution over the parameters is an
alternative to Maximum Likelihood training of a Bayesian Network, as we did in
the coin tossing example. We go deep into it using the following scenario, consider a disease
\(D\) and two habits \(A\) and \(B\).

\begin{figure}[!ht]
  \begin{tabular}{*{2}{>{\centering\arraybackslash}b{\dimexpr0.5\linewidth-2\tabcolsep\relax}}}
  \centering
  \begin{tikzpicture}[
    node distance=1cm and 0.5cm,
    mynode/.style={draw,circle,text width=0.5cm,align=center}
    ]

    \node[mynode] (d) {\(D_{n}\)};
    \node[mynode, above left=of d] (a) {\(A_{n}\)};
    \node[mynode, above right=of d] (b) {\(B_{n}\)};
    \node[mynode, above=of a] (ta) {\(\theta_{A}\)};
    \node[mynode, above=of b] (tb) {\(\theta_{B}\)};
    \node[mynode, below=of d] (td) {\(\theta_{D}\)};
    \plate{} {(d)(a)(b)} {\(1\dots N\)}; %
    \path (a) edge[-latex] (d)
    (b) edge[-latex] (d)
    (ta) edge[-latex] (a)
    (tb) edge[-latex] (b)
    (td) edge[-latex] (d)
    ;

  \end{tikzpicture}
    \caption{Bayesian parameter model for the relation between \(A,B,D\)}
    \label{fig:bayesian_example}
    &
      \renewcommand{\arraystretch}{1.3}
      \begin{tabular}{|l|l|l|}
    \hline
    A & B & D \\ \hline
    1 & 1 & 1 \\ \hline
    1 & 0 & 0 \\ \hline
    0 & 1 & 1 \\ \hline
    0 & 1 & 0 \\ \hline
    1 & 1 & 1 \\ \hline
    0 & 0 & 0 \\ \hline
    1 & 0 & 1 \\ \hline
  \end{tabular}\captionof{table}{Observations}
\end{tabular}
    \end{figure}



\[
P(a,b,d) = P(d|a,b)P(a)P(b)
\]

Consider the set of i.i.d random variables
\(\{A_{n}, B_{n}, D_{n}\}_{n = 1,\dots, N}\) and the
corresponding set of observations
\(\mathcal{V} = \{(a_{n}, b_{n}, d_{n}), n = 1,\dots , N\}\)

We need a notation for the parameters, as all the variables are binary we are
going to use
\[
  P(A = 1 \mid \theta_{A}) = \theta_{A}, \hspace{1cm} P(B = 1 \mid \theta_{B} = \theta_{B}), \hspace{1cm} P(D = 1 \mid A = 0, B = 1, \theta_{D}) = \theta_{1}
\]
\[\theta_{D} = (\theta_{0}, \theta_{1}, \theta_{2}, \theta_{3})\]
Using the states of \(A\) and \(B\) to create \((01)\) and its correspondent
decimal form in the sub-index of \(\theta\).

We need to specify a prior and since dealing with multi-dimensional continuous
distributions is computationally problematic it is normal to use uni-variate
distributions.

A convenient assumption is that the prior factorizes, this is usually called
\emph{global parameter independence}. We assume then
\[
  P(\theta_{A}, \theta_{B}, \theta_{D}) = P(\theta_{A})P(\theta_{B})P(\theta_{D})
\]
Assuming our data is i.i.d, we have
\[
  P(\theta_{A}, \theta_{B}, \theta_{D}, \mathcal{V}) = P(\theta_{A})P(\theta_{B})P(\theta_{D})\prod_{n}P(a_{n}\mid \theta_{A})P(b_{n} \mid \theta_{B})P(d_{n}\mid a_{n}, b_{n}, \theta_{D})
\]


Learning then corresponds to inference

\[
  \begin{aligned}
    P(\theta_{A}, \theta_{B}, \theta_{D}\mid \mathcal{V}) &= \frac{P(\mathcal{V} \mid \theta_{a}, \theta_{B}, \theta_{D})P(\theta_{A}, \theta_{B}, \theta_{D})}{P(\mathcal{V})} =\frac{P(\mathcal{V} \mid \theta_{A}, \theta_{B}, \theta_{D})P(\theta_{A}) P(\theta_{B})P( \theta_{D})}{P(\mathcal{V})}\\
    &= \frac{1}{P(\mathcal{V})}P(\theta_{A})\prod_{n}P(a_{n}\mid \theta_{A})P(\theta_{B})\prod_{n}P(b_{n}\mid \theta_{B})P(\theta_{D})\prod_{n}P(d_{n}\mid a_{n}, b_{n},\theta_{D})\\
    &= P(\theta_{A} \mid \V_{A} )P(\theta_{B}\mid \V_{B})P(\theta_{D} \mid \V)
  \end{aligned}
\]

Where \(V_{i}\) is the subset of the data restricted to the variable \(i\). If
we further assume that \(P(\theta_{D})\) factorizes as
\(P(\theta_{D}) = P(\theta_{0})P(\theta_{1})P(\theta_{2})P(\theta_{3})\),
this is called \emph{local parameter independence}, then it follows that
\[
  P(\theta_{D}\mid \V) = P(\theta_{0} \mid \V )P(\theta_{1} \mid \V )P(\theta_{2} \mid \V )P(\theta_{3} \mid \V )
\]

\section{Learning binary variables}

The simplest cases to continue are \(P(a\mid \theta_{A})\) and
\(P(b \mid \theta_{b})\) since they require only a uni-variate prior distribution
\(P(\theta_{A})\) or \(P(\theta_{b})\). We use \(P(\theta_{A})\) as the other
case is analogous.

The posterior is
\[
  P(\theta_{A} \mid \V_{A}) = \frac{1}{P(\V_{A})}P(\theta_{A})\theta_{A}^{\#(a=1)}(1-\theta_{A})^{\#(a=0)}
\]

The most convenient choice for the prior is a Beta distribution as conjugacy
will hold.

\[
  \theta_{A} \sim \text{Beta}(\alpha_{A}, \beta_{A}) \implies P(\theta_{A})  = \frac{1}{B(\alpha_{A}, \beta_{A})}\theta_{A}^{\alpha_{A}-1}(1-\theta_{A})^{\beta_{A} - 1}
\]
So it follows that
\[
  (\theta_{A} \mid \V_{A}) \sim \text{Beta}(\theta_{A} \mid \alpha_{A} + \#(A=1), \beta_{A} + \#(A = 0))
\]

The marginal is then
\[
  \begin{aligned}
    P(A = 1 \mid \V_{A})
    &= \frac{P(A = 1, \V_{A})}{P(\V_{A})} = \int_{\theta_{A}}  \frac{P(A = 1, \V_{A}, \theta_{A})}{P(\V_{A})} =  \int_{\theta_{A}}  \frac{P(A = 1 \mid \V_{A}, \theta_{A}) P(\V_{A}, \theta_{A})}{P(\V_{A})} \\
    &=  \int_{\theta_{A}}  \frac{P(A = 1 \mid \V_{A}, \theta_{A}) P(\theta_{A} \mid \V_{A})P(\V_{A})}{P(\V_{A})} = \int_{\theta_{A}}P(\theta_{A}\mid \V_{A})\theta_{A} = \E[\theta_{A} \mid \V_{A}] \\
    &= \frac{\alpha_{A} + \#(A= 1)}{\alpha_{A} + \#(A=1) + \beta_{A} + \#(A=0)}
  \end{aligned}
\]

For \(P(d \mid a ,b)\) the situation is more complex, the most convenient way is
to specify a Beta prior for each one of the four components of \(\theta_{D}\).
Lets focus on \(P(D = 1 \mid A = 1, B = 0)\), notice the parameters \(\alpha\)
and \(\beta\) we used before now depend on \(a\) and \(b\), for this reason we
are using \(\alpha_{D}(a,b)\) and \(\beta_{D}(a,b)\) as prior parameters, these
are called \emph{hyperparameters}.
\[
  P(\theta_{2}) = B(\theta_{2} \mid \alpha_{D}(1,0) + \#(D = 1, A = 1, B = 0), \beta_{D}(1,0) + \#(D = 0, A = 1, B = 0))
\]

As before we got that
\[
  P(D = 1 \mid A = 1, B = 0, \V) = \frac{\alpha_{D}(1,0) + \#(D = 1, A = 1, B = 0)}{\alpha_{D}(1,0) + \beta_{D}(1,0) + \#(A=1, B = 0)}
\]

In case we had no preference, we could set all hyperparameters to the same
value, and, a complete ignorance prior would correspond to set them to 1.

Let now consider two limit possibilities, the one where we have no data at all,
and the one where we have infinite data.

In case we have no data, the marginal probability corresponds to the prior which
in the last case is
\[
   P(D = 1 \mid A = 1, B = 0, \V) = \frac{\alpha_{D}(1,0)}{\alpha_{D}(1,0) + \beta_{D}(1,0)}
 \]
 Note that equal hyperparameters would give a result of \(0.5\).

 When infinite data is available, the marginal is generally dominated by it,
 this corresponds to the Maximum Likelihood solution.
 \[
   P(D = 1 \mid A = 1, B = 0, \V) = \frac{\#(D = 1, A = 1 , B = 0)}{\#(A = 1, B = 0)}
 \]
 This happens unless the prior has a pathologically strong effect.

 Consider the data given in the table in figure \ref{fig:bayesian_example}, and
 equal parameters and hyperparameters \(1\). Then we can compute the differences
 between this and using the Maximum Likelihood technique.
 \[
   P(A = 1 \mid \V) = \frac{1 + \#(A = 1)}{2 + N} = \frac{5}{9} \approx 0.556
 \]
 By comparison, the Maximum Likelihood result is \(4/7 = 0.571\), the Bayesian
 result is more prudent than this one, which fits in with our belief that any
 setting is equally probable i.e \(0.5\).

 \section{Learning discrete variables}

 The natural generalization to more than two states is using a Dirichlet
 distribution as prior, assuming i.i.d data and local and global prior
 independence. We are considering two different scenarios, firstly one where the
 variable has no parents, as the case for \(A\) and \(B\) in the previous
 example. Secondly, we will consider a variable with a non void set of parents,
 as in the case with the disease \(D\).

 Consider a variable \(X\) with
 \(Dom(X) = \{1, \dots, I\}, \ \theta = (\theta_{1},\dots, \theta_{I})\), then
 \[
   P(x \mid \theta) = \prod_{i = 1}^{I}\theta_{i}^{\mathbb{I}[x = i]} \text{
   with  } \sum_{i=1}^{I}\theta_{i} = 1
\]
So that the posterior (considering \(N\) observations of the variable
\((x_{1}, \dots, x_{N}) = \V\)) is
\[
  P(\theta \mid x_{1},\dots,x_{N}) = \frac{1}{P(\V)} P(\theta) \prod_{n = 1}^{N}\prod_{i =1 }^{I}\theta_{i}^{\mathbb{I}[x_{n} = i]} =  \frac{1}{P(\V)} P(\theta) \prod_{i = 1}^{I} \theta_{i}^{\sum_{n} \mathbb{I}[x_{n}=i]}
\]
Then assuming a Dirichlet prior with hyperparameters \(\bm{u} = (u_{1}, \dots, u_{I})\)
\[
  P(\theta) = \frac{1}{B(\bm{u})}\prod_{i =1}^{I}\theta_{i}^{u_{i}-1} \implies P(\theta \mid \V) = \frac{1}{B(\bm{u})P(\V)}\prod_{i=1}^{I}\theta_{i}^{u_{i}-1 + \sum_{n}\mathbb{I}[x_{n} = i]}
\]

Which means that, defining \(\bm{c} = ( \sum_{n=1}^{N}\mathbb{I}[x_{n} = i])_{i = 1,\dots,I}\)
\[
  P(\theta \mid \V) \sim \text{Dirichlet}(\bm{u} + \bm{c})
\]

The marginal is then given by
\[
  \begin{aligned}
    P(X=i \mid \V) &= \int_{\theta}P(X=i \mid \theta)P(\theta \mid \V) =  \int_{\theta}\theta_{i}P(\theta \mid \V)\\
    &=  \int_{\theta_{i}}\theta_{i}P(\theta_{i} \mid \V) = \E[\theta_{i} \mid \V]
\end{aligned}
\]
Where we used that
\[\int_{\theta_{j \neq i}}\theta_{i} P(\theta \mid \V) = \theta_{i}\prod_{k\neq j}P(\theta_{k} \mid V) \int_{\theta_{j}}P(\theta_{j}\mid \V) = \theta_{i} \prod_{k \neq j}P(\theta_{k} \mid \V)\]



As we already know from Proposition \ref{prop:dirichlet_marginal}, the univariate marginal of a Dirichlet distribution is a
Beta Distribution, then
\[
  (\theta_{i} \mid \V) \sim \text{Beta}(u_{i} + c_{i}, \sum_{j\neq i} u_{j} + c_{j})
\]
So the marginal is
\[
  P(X = i \mid \V) = \frac{u_{i} + c_{i}}{\sum_{j}u_{j} + c_{j}}
\]


Consider now that \(X\) has a set of parent variables \(pa(X)\), in this case,
we want to compute the marginal given a state of its parents and the data
\[
  P(X = i \mid pa(X) = \bm{j}, \V)
\]
Let set the following notation for the parameters
\[
  P(X = i \mid pa(X) = \bm{j}, \theta) = \theta_{i,\bm{j}} \hspace{2cm} \bm{\theta_{j}} = (\theta_{1,\bm{j}},\dots, \theta_{I,\bm{j}})
\]
Local independence means that
\[
  P(\bm{\theta}) = \prod_{j}P(\bm{\theta_{j}})
\]

As we did before, we consider a Dirichlet prior
\[
  \bm{\theta_{j}} \sim Dirichlet(\bm{u_{j}})
\]
the posterior is then
\[
  \begin{aligned}
    P(\bm{\theta} \mid \V) &= \frac{P(\bm{\theta})P(\V \mid \bm{\theta}) }{P(\V)} = \frac{1}{P(\V)}\Big(\prod_{\bm{j}}P(\bm{\theta_{j}}) \Big)P(\V \mid \bm{\theta}) \\
    &= \frac{1}{P(\V)}\Big(\prod_{\bm{j}}\frac{1}{B(\bm{u_{j}})}\prod_{i}\theta_{i,\bm{j}}^{u_{i,\bm{j}}-1}\Big) P(\V \mid \bm{\theta})\\
    &= \frac{1}{P(\V)}\Big(\prod_{\bm{j}}\frac{1}{B(\bm{u_{j}})}\prod_{i}\theta_{i,\bm{j}}^{u_{i,\bm{j}}-1}\Big) \Big(\prod_{n}\prod_{\bm{j}}\prod_{i} \theta_{i,\bm{j}}^{\mathbb{I}[x_{n} = i, pa(x_{n}) = \bm{j}]}\Big)\\
    &= \frac{1}{P(\V)}\prod_{\bm{j}}\frac{1}{B(\bm{u_{j}})}\prod_{i}\theta_{i,\bm{j}}^{u_{i,\bm{j}}-1 + \#(X = i,pa(X)=\bm{j})}
  \end{aligned}
\]
Naming \(\bm{v_{j}} = \bm{u_{j}} + \#(X = i, pa(X) = \bm{j})\), the posterior
is
\[
  (\bm{\theta} \mid \V) \sim \prod_{j}\text{Dirichlet}(\bm{v_{j}})
\]
Noting \(v_{i,j}\) the components of \(\bm{v_{j}}\), the marginal is then
\[
  P(X=i, pa(X) = \bm{j}, \V) = \frac{v_{i,j}}{\sum_{i}v_{i,j}}
\]
Notice all the above has been done using a fixed variable \(X\), so that all the parameters depend on that variable.

Using the above calculations, we can define the data likelihood under a model
\[
  \begin{aligned}
    P(\V \mid \mathcal{M}) &= \prod_{x}\prod_{n}P(x_{n} \mid pa(x_{n}), \mathcal{M}) = \prod_{x} \int_{\bm{\theta}} P(\bm{\theta}) \prod_{n} P(x_{n}\mid pa(x_{n}), \bm{\theta}, \mathcal{M})\\
    &= \prod_{x} \prod_{\bm{j}} \frac{1}{B(\bm{u_{j}})} \int_{\bm{\theta}} \prod_{i}\theta_{i,\bm{j}}^{u_{i,\bm{j}}-1 + \#(x = i,pa(x)=\bm{j})}\\
    &= \prod_{x}\prod_{j} \frac{B(\bm{v_{j}})}{B(\bm{u_{j}})}
  \end{aligned}
\]
