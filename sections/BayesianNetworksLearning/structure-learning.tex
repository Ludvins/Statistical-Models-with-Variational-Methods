
So far, we have assumed the Bayesian Network was an hypothesis of the problem, however, this structure is not always given and may be learned also form the data.

Even considering complete data (no missing observations), there are some problems that need to be taken into account.
\begin{itemize}
  \item The number of Belief networks is exponential over the number of
    variables so a brute force algorithm would not be viable.
  \item Testing dependencies requires a large amount of data. So a threshold
    must be set to measure when a dependence is significant.
  \item A Belief Network or a Markov Network may not be enough to represent
    the data due to the existence of unobserved variables.
\end{itemize}


\begin{algorithm}[t]
  \SetAlgoLined
  \KwData{Complete undirected graph \(G\), with vertices \(\V\)}
  \KwResult{\(G\) with removed links}
  \(i = 0\)\;
  \While{all nodes have \(\leq i\) neighbors}{
    \For{\(X \in \V\)}{
      \For{\(Y \in ne(x)\)}{
        \If{\(\exists S \subset ne(X)\backslash Y\) such that \(\#S = i\) and
          \(X \bigCI Y \mid S\)}{
          Remove \(X-Y\) from \(G\)\;
          \(S_{XY} = S\)\;
        }
      }
    }
    \(i = i+1\)\;
  }
  \caption{PC Algorithm}
  \label{alg:pc}
\end{algorithm}

\section{PC Algorithm}

An approach to learn the structure is the PC algorithm, it begins with a
complete graph \(G\) and tries to remove as many links as possible studying the
independence of the variables.

The algorithm \ref{alg:pc} iterates over neighborhoods, from smaller to bigger ones. It chooses a linked pair of variables \((X,Y) \in E\) and
a subset \(S_{XY} \subset ne(X)\), verifying
\(Y \notin S_{XY}\). If \(X \bigCI Y \mid S\), then the link is removed and
\(S_{XY}\) is stored.

The main idea behind this is that a set of independencies is
faithful to a graph if there is no link between two nodes \(X\) and \(Y\) if
and only if there exists a subset of \(ne(X)\) such that they are independent
given this subset.

This gives the skeleton of the Bayesian Network, no more edges will be removed or added. The directed graph may be constructed following two rules rule:
\begin{enumerate}
  \item For any undirected link \(X - Y - Z\), if \(Y \notin S_{XZ}\) then set
    \(X \to Y \leftarrow Z\) (we are creating a collider for that path).
  \item The rest of links may oriented arbitrarily not
creating cycles or colliders.
\end{enumerate}
The reasoning behind this is the
d-separation theorem \ref{th:d-separation},  if \(Y \notin S_{XZ}\) then \(X \bigCD Z \mid Y\) so \(Y\) must d-connect them, to get this we set it as a collider. On the other hand, if \(Y \in S_{XZ}\) and
\(X \bigCI Z \mid S_{XZ}\) then we want \(S_{XZ}\) to d-separate them, that is,
using any configuration that doesn't create a collider in \(S_{XZ}\).

\section{Independence Learning}

Our main concern now is given three variables \(X, Y, Z\) to measure \(X \bigCI Y \mid Z\). One approach is to measure the empirical \emph{conditional mutual information} of the variables.

\begin{definition}
  Given two random variables \(X, Y\), we define their \emph{mutual information} as the Kullback-Leibler divergence of their joint distribution and the product of their marginals and
  \[
    MI(X;Y) = \KL{P_{X,Y}}{P_{X}P_{Y}}
  \]
\end{definition}

\begin{definition}
  Given three random variables \(X, Y, Z\) we define the \emph{conditional mutual information} of \(X\) and \(Y\) over \(Z\) as
  \[
    MI(X;Y\mid Z) = \E{Z}{\KL{P_{X,Y \mid Z}}{P_{X\mid Z} P_{Y \mid Z}}}
  \]
\end{definition}
Where \(MI(X;Y \mid Z) \geq 0\) and \(MI(X;Y \mid Z) = 0 \iff P_{X,Y \mid Z} = P_{X\mid Z} P_{Y \mid Z} \iff X\bigCI Y \mid Z\). We can estimate this using the empirical distributions, however, this \emph{empirical} mutual information will be typically greater than \(0\) even when \(X\bigCI Y \mid Z\), therefore a threshold must be established.

A Bayesian approach would be comparing the model likelihood under independence and dependence hypothesis. That is computing the model likelihood for the below joint distributions assuming local and global parameter independence
\[
  P_{indep}(x,y,z) = P(x\mid z, \theta_{1})P(y \mid z, \theta_{2})P(z \mid \theta_{3})P(\theta_{1})P(\theta_{2})P(\theta_{3})
\]
\[
P_{dep}(x,y,z) = P(x,y,z \mid \theta)P(\theta)
\]
