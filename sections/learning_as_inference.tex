

In Machine Learning and related fields, the distributions are not fully specified
and need to be learned from the data.

From now on, \(\mathcal{V}\) will denote the known data and \(\theta\) the set
of parameters of the data distributions. The main task is to determine this set
of parameters using the information given by the data.

\begin{definition}
\emph{Priors} and \emph{posteriors} typically refer to the parameter
distribution before and after seeing the data, respectively. Using Bayes' rule
\[
  P(\theta \mid  \mathcal{V}) = \frac{P(\mathcal{V}  \mid  \theta)P(\theta)}{P(\mathcal{V})}
\]
The factor \(P(\mathcal{V} \mid \theta)\) is called the \emph{likelihood}.
\end{definition}

Let us see an example of our goal, in it we will try to learn the bias of a coin,
given a set of tossing results.

\begin{exampleth}
  Let \(\V = \{v_n\}_{n \in 0,\dots,N}\) be the results of tossing a coin \(N \in
  \mathbb{N}\) times, let \(1\) symbolize \emph{heads} and \(0\) \emph{tails}.

  Our objective is to estimate the probability \(\theta\) that the coin will be
  head \(P(v_n = 1  \mid  \theta)\), for this we have the i.i.d random variables \(v_1,\dots,v_n\)
  and \(\theta\), and we require a model \(P(v_1,\dots,v_n,\theta)\). We have a
  Belief Network shown in figure \ref{fig:learning_coin}
  \[
    P(\V,\theta) = P(\theta)\prod_{n=1}^N P(v_n \mid \theta)
  \]

\begin{figure}[H]
\centering
\begin{tikzpicture}[
  node distance=1cm and 0.5cm,
  mynode/.style={draw,circle,text width=0.5cm,align=center}
]

\node[mynode] (a) {\(\theta\)};
\node[mynode, below=of a] (b) {\(v_i\)};
\plate{} {(b)} {\(N\)}; %
\path (a) edge[-latex] (b)
;

\end{tikzpicture}
\caption{Belief network for coin tossing}
\label{fig:learning_coin}
\end{figure}

We want to calculate
\[
  P(\theta \mid \V) = \frac{P(\V \mid \theta)P(\theta)}{P(\V)}
\]
to do so, we need to specify the prior \(P(\theta)\), we are using a discrete
model where
\[
  P(\theta = 0.2) = 0.1 \hspace{2cm} P(\theta = 0.5) = 0.7 \hspace{2cm} P(\theta =
  0.8) = 0.2
\]
This means that we have a \(70\%\) belief that the coin is fair, a \(10\%\)
belied that is biased to tails and \(20\%\) that is biased to heads.
Notice that \(P(v_n = 1 \mid \theta) = \theta\) and \(P(v_n = 0 \mid \theta) = 1 - \theta\).

Let \(n_h\) be the number of heads in our observed data and \(n_t\)
the number of tails
\[
  n_{h} = \#\{v= 1\} \hspace{2cm} n_{t} = \#\{v = 0\}
\]

then the posterior has the form

\[
  P(\theta  \mid \V) = \frac{P(\theta)}{P(\mathcal{V})} \theta^{n_h}(1-\theta)^{n_t}
\]

Suppose now that \(n_h = 2\) and \(n_t = 8\), then
\begin{gather*}
  P(\theta = 0.2  \mid  \mathcal{V}) = \frac{1}{P(\mathcal{V})}\times 0.1 \times 0.2^{2}
  \times 0.8^{8} = \frac{1}{P(\mathcal{V})} \times 6.71\times10^{-4} \\
   P(\theta = 0.5  \mid  \mathcal{V}) = \frac{1}{P(\mathcal{V})}\times 0.7 \times 0.5^{2}
   \times 0.5^{8} = \frac{1}{P(\mathcal{V})} \times 6.83\times10^{-4}\\
    P(\theta = 0.8  \mid  \mathcal{V}) = \frac{1}{P(\mathcal{V})}\times 0.2 \times 0.2^{2}
  \times 0.8^{8} = \frac{1}{P(\mathcal{V})} \times 3.27\times10^{-7}
\end{gather*}

Now, we can compute
\[
   \frac{1}{P(\mathcal{V})} =  6.71\times10^{-4} +   6.83\times10^{-4} +
   3.27\times10^{-7} = 0.00135
 \]
 So,
\begin{gather*}
  P(\theta = 0.2  \mid  \mathcal{V}) = 0.4979\\
  P(\theta = 0.5  \mid  \mathcal{V}) = 0.5059\\
  P(\theta = 0.8  \mid  \mathcal{V}) = 0.00024
\end{gather*}

These are the posterior parameter beliefs of our experiment. Given this, it we
were to choose a single value for the posterior it would be \(\theta = 0.5\).
This result is intuitive, we had a strong belief of the coin being fair
and even though the number of tails was quite bigger than heads, it
was not enough to make the difference. Obviously the posterior of the coin being
biased to tails is now bigger than the prior.

Suppose an uniform prior distribution so that \(P(\theta) = k \implies \int_0^1 P(\theta) d\theta
= k = 1\) due to normalization.

Using the previous calculations we have
\[
  P(\theta \mid  \mathcal{V}) = \frac{1}{P(\mathcal{V})} \theta^{n_h}(1-\theta)^{n_t}
\]
where
\[
  P(\mathcal{V}) = \int_0^1 \theta^{n_h}(1-\theta)^{n_t} d\theta
\]
this implies that
\[
  P(\theta \mid \mathcal{V}) = \frac{\theta^{n_h}(1-\theta)^{n_t} }{ \int_0^1 u^{n_h}(1-u)^{n_t}
    du} \implies \theta \mid \V \sim Beta(n_h + 1, n_t + 1)
\]
\end{exampleth}

\begin{definition}
If the posterior distribution is in the same probability distribution family as
the prior distribution, they are then called \emph{conjugate distributions}, and
the prior is called a \emph{conjugate prior} of the likelihood distribution.
\end{definition}

Let's use a Beta distribution as the prior in the last example

\[
  \theta \sim \text{Beta}(\alpha, \beta) \implies P(\theta) = \frac{1}{B(\alpha, \beta)}\theta^{\alpha - 1}(1 - \theta)^{\beta -
    1}
\]
then, repeating the same as before we get that
\[
  P(\theta, \mathcal{V}) = \frac{1}{B(\alpha + n_h, \beta + n_t)}\theta^{\alpha
    + n_h - 1}(1 - \theta)^{\beta + n_t - 1} \implies (\theta, \V) \sim Beta(\alpha + n_h, \beta + n_t)
\]

So both the prior and posterior are Beta distributions, then the Beta
distribution is called ``conjugate'' of the Binomial distribution.


\section{Utility}

The Bayesian posterior says nothing about how to benefit from the beliefs it
represents, in order to do this we need to specify the utility of each decision.

With this idea we define an utility function over the parameters

\[
  U(\theta, \theta_{true}) = \alpha \mathbb{I}[\theta = \theta_{true}] - \beta
  \mathbb{I}[\theta \neq \theta_{true}]
\]
where \(\alpha, \beta \in \R\). This symbolizes the gains or looses of choosing
the parameter \(\theta\), when the true value of the parameter is supposed to be
\(\theta_{true}\). Then the expected utility of a parameter \(\theta_0\) is
calculated as
\[
  U(\theta = \theta_0) = \sum_{\theta_{true}}U(\theta = \theta_0,
  \theta_{true})P(\theta = \theta_{true}  \mid  \mathcal{V})
\]

Using the last example, we may define out utility function as
\[
  U(\theta, \theta_{true}) = 10\mathbb{I}[\theta = \theta_{true}] - 20
  \mathbb{I}[\theta \neq \theta_{true}]
\]
Where we interpret that the loss of choosing the wrong parameter is twice as
important as the gains from doing it right.

The expected utility of the decision that the parameter is \(\theta = 0.2\)
in our discrete example would be
\[
  \begin{aligned}
  U(\theta = 0.2) &= U(\theta = 0.2, \theta_{true} = 0.2)P(\theta_{true} = 0.2  \mid
  \mathcal{V})\\
  &+ U(\theta = 0.2, \theta_{true} = 0.5)P(\theta_{true} = 0.5  \mid
  \mathcal{V}) \\
  & +  U(\theta = 0.2, \theta_{true} = 0.8)P(\theta_{true} = 0.8  \mid  \mathcal{V})\\
  &= 10 \times 0.4979 - 20\times 0.5059 -20 \times 0.00024 \\
  &= -5.1438\\
  U(\theta = 0.5) &= -4.9038 \\
  U(\theta = 0.8) &= -20.0736
\end{aligned}
\]
 

This illustrate how an utility function can affect the results of the inference.
The most probable value for \(\theta\) was \(0.2\), but, using this utility
function, \(0.5\) is the one with which we expect minor losses.

\section{Directed Models}
\subsection{Maximum Likelihood training}

In this section we will introduce two concepts, Maximum Likelihood and Maximum a
Posteriori, showing that \emph{training} a model's parameter to maximize the
Maximum Likelihood equals to take the empirical distribution as it.

\begin{definition}
  Maximum Likelihood is calculated as
  \[
    \theta^{ML} = \argmax_\theta p(\mathcal{V} \mid \theta)
  \]
   it refers to the value of the parameter
\(\theta\) for which the observed data better fits the model.
\end{definition}

\begin{definition}
  Maximum A Posteriori refers to
  \[
    \theta^{MAP} = \argmax_\theta p(\mathcal{V} \mid \theta)P(\theta)
  \]
\end{definition}

The decision of taking the Maximum A Posteriori can be motivated using an
utility that equals zero for all but the correct parameter
\[
  U(\theta, \theta_{true}) = \mathbb{I}[\theta = \theta_{true}]
\]

using this, the expected utility of a parameter \(\theta = \theta_0\) is

\[
  U(\theta = \theta_0) = \sum_{\theta_{true}}\mathbb{I}[\theta_{true} = \theta_0]P(\theta = \theta_{true}  \mid  \mathcal{V}) = P(\theta_0  \mid  \mathcal{V})
\]

This means that the maximum utility decision is to take the value \(\theta_0\)
with the highest posterior value.

\begin{remark}
When using a flat prior \(\theta^{ML}= \theta ^{MAP}\).
\end{remark}



Now, we are going to show the relation between the Maximum Likelihood and the
Kullback-Leibler divergence of the empirical distribution and our model.
Firstly, we define the empirical distribution of a set

Let \(\{X_1, \dots, X_m\}\) be a set of real i.i.d random variables and
\(\{x_{1}, \dots, x_{m}\}\) a set of observations of those variables, we can define the
empirical distribution as a distribution whose probability mass function
\(Q\) is
\[
  Q(x) = \frac{1}{m}\sum_{i = 1}^m \mathbb{I}[x = x_i]
\]

 We may calculate this Kullback-Leibler divergence and study their functional independence.
\[
  KL(Q \mid P) = \E[\log(Q(x))]_{Q} - \E[\log(P(x))]_Q
\]

Notice the term \(\E[\log(Q(x))]_{Q} \) is a constant as the variables are
i.i.d it follows that
\[
   \E[\log(P(x))]_Q = \frac{1}{m}\sum_{i = 1}^mlogP(x_i)
 \]
 where the right side is the log likelihood under \(Q\). As the logarithm is
 a strictly increasing function, maximizing the log likelihood equals to
 maximize the likelihood itself, and we can see here how it is equivalent to
 minimize the Kullback-Leibler divergence between the empirical distribution and
 our distribution.

 In case \(P(x)\) is unconstrained, the optimal choice is \(P(x) = Q(x)\), that
 is, the maximum likelihood distribution corresponds to the empirical distribution.

 For a Belief Network we know there is the following constraint
 \[
   P(x_{1}, \dots, x_{m}) = \prod_{i = 1}^K P(x_i  \mid  pa(x_i))
 \]
 We now want to minimize the Kullback-Leibler divergence between the empirical
 distribution \(Q(x)\) and \(P(x)\) in order to get the Maximum Likelihood.

 \[
   \begin{aligned}
   KL(Q \mid P) &= - \E\big[\sum_{i = 1}^K\log{P(x_i \mid pa(X_i))}\big]_Q +
   \E\big[\sum_{i = 1}^K\log{P(x_i \mid pa(X_i))}\big]_P
   \\ &= - \sum_{i =
     1}^K \E\big[\log{P(x_i \mid pa(X_i))}\big]_Q + \sum_{i =
     1}^K \E\big[\log{P(x_i \mid pa(X_i))}\big]_P
   \end{aligned}
 \]

 We can not use Proposition \ref{prop:expectation_over_marginal} on \(\log{P(x_i \mid pa(x_i))}\).

 \[
   \begin{aligned}
     KL(Q \mid P) &= \sum_{i = 1}^K \E\Big[ \log{Q(x_i \mid pa(x_i))}\Big]_{Q(x_i,pa(x_i))} - \E\Big[
     \log{P(x_i \mid pa(x_i))}\Big]_{Q(x_i,pa(x_i))} \\
     &= \sum_{i = 1}^K \E \Big[ KL\Big(Q(x_i \mid pa(x_i)) \mid P(x_i \mid pa(x_i))\Big) \Big]_{Q(x_i,pa(x_i))}
   \end{aligned}
 \]

 The minimal setting is then
 \[
   P(x_i \mid pa(x_i)) = Q(x_i \mid pa(x_i))
 \]
 in terms of the initial data it is to set \(P(x_i \mid pa(x_i))\) to the number of
 times the state appears in it.

 \subsection{Bayesian Belief Network Training}

A Bayesian approach where we set a distribution over the parameters is an
alternative to Maximum Likelihood training of a Bayesian Network, as we did in
the coin tossing example. We go deep into it using the following scenario, consider a disease
\(D\) and two habits \(A\) and \(B\).

\begin{figure}[!ht]
  \begin{tabular}{*{2}{>{\centering\arraybackslash}b{\dimexpr0.5\linewidth-2\tabcolsep\relax}}}
  \centering
  \begin{tikzpicture}[
    node distance=1cm and 0.5cm,
    mynode/.style={draw,circle,text width=0.5cm,align=center}
    ]

    \node[mynode] (d) {\(D_{n}\)};
    \node[mynode, above left=of d] (a) {\(A_{n}\)};
    \node[mynode, above right=of d] (b) {\(B_{n}\)};
    \node[mynode, above=of a] (ta) {\(\theta_{A}\)};
    \node[mynode, above=of b] (tb) {\(\theta_{B}\)};
    \node[mynode, below=of d] (td) {\(\theta_{D}\)};
    \plate{} {(d)(a)(b)} {\(1\dots N\)}; %
    \path (a) edge[-latex] (d)
    (b) edge[-latex] (d)
    (ta) edge[-latex] (a)
    (tb) edge[-latex] (b)
    (td) edge[-latex] (d)
    ;

  \end{tikzpicture}
    \caption{Bayesian parameter model for the relation between \(A,B,D\)}
    \label{fig:bayesian_example}
    &
      \renewcommand{\arraystretch}{1.3}
      \begin{tabular}{|l|l|l|}
    \hline
    A & B & D \\ \hline
    1 & 1 & 1 \\ \hline
    1 & 0 & 0 \\ \hline
    0 & 1 & 1 \\ \hline
    0 & 1 & 0 \\ \hline
    1 & 1 & 1 \\ \hline
    0 & 0 & 0 \\ \hline
    1 & 0 & 1 \\ \hline
  \end{tabular}\captionof{table}{Observations}
\end{tabular}
    \end{figure}



\[
P(a,b,d) = P(d|a,b)P(a)P(b)
\]

Consider the set of i.i.d random variables
\(\{A_{n}, B_{n}, D_{n}\}_{n = 1,\dots, N}\) and the
corresponding set of observations
\(\mathcal{V} = \{(a_{n}, b_{n}, d_{n}), n = 1,\dots , N\}\)

We need a notation for the parameters, as all the variables are binary we are
going to use
\[
  P(A = 1 \mid \theta_{A}) = \theta_{A}, \hspace{1cm} P(B = 1 \mid \theta_{B} = \theta_{B}), \hspace{1cm} P(D = 1 \mid A = 0, B = 1, \theta_{D}) = \theta_{1}
\]
\[\theta_{D} = (\theta_{0}, \theta_{1}, \theta_{2}, \theta_{3})\]
Using the states of \(A\) and \(B\) to create \((01)\) and its correspondent
decimal form in the sub-index of \(\theta\).

We need to specify a prior and since dealing with multi-dimensional continuous
distributions is computationally problematic it is normal to use uni-variate
distributions.

A convenient assumption is that the prior factorizes, this is usually called
\emph{global parameter independence}. We assume then
\[
  P(\theta_{A}, \theta_{B}, \theta_{D}) = P(\theta_{A})P(\theta_{B})P(\theta_{D})
\]
Assuming our data is i.i.d, we have
\[
  P(\theta_{A}, \theta_{B}, \theta_{D}, \mathcal{V}) = P(\theta_{A})P(\theta_{B})P(\theta_{D})\prod_{n}P(a_{n}\mid \theta_{A})P(b_{n} \mid \theta_{B})P(d_{n}\mid a_{n}, b_{n}, \theta_{D})
\]

Learning then corresponds to inference

\[
  \begin{aligned}
    P(\theta_{A}, \theta_{B}, \theta_{D}\mid \mathcal{V}) &= \frac{P(\mathcal{V} \mid \theta_{a}, \theta_{B}, \theta_{D})P(\theta_{A}, \theta_{B}, \theta_{D})}{P(\mathcal{V})} =\frac{P(\mathcal{V} \mid \theta_{A}, \theta_{B}, \theta_{D})P(\theta_{A}) P(\theta_{B})P( \theta_{D})}{P(\mathcal{V})}\\
    &= \frac{1}{P(\mathcal{V})}P(\theta_{A})\prod_{n}P(a_{n}\mid \theta_{A})P(\theta_{B})\prod_{n}P(b_{n}\mid \theta_{B})P(\theta_{D})\prod_{n}P(d_{n}\mid a_{n}, b_{n},\theta_{D})\\
    &= P(\theta_{A} \mid \V_{A} )P(\theta_{B}\mid \V_{B})P(\theta_{D} \mid \V)
  \end{aligned}
\]

Where \(V_{i}\) is the subset of the data restricted to the variable \(i\). If
we further assume that \(P(\theta_{D})\) factorizes as
\(P(\theta_{D}) = P(\theta_{0})P(\theta_{1})P(\theta_{2})P(\theta_{3})\),
this is called \emph{local parameter independence}, then it follows that
\[
  P(\theta_{D}\mid \V) = P(\theta_{0} \mid \V )P(\theta_{1} \mid \V )P(\theta_{2} \mid \V )P(\theta_{3} \mid \V )
\]

The simplest cases to continue are \(P(a\mid \theta_{A})\) and
\(P(b \mid \theta_{b})\) since they require only a uni-variate prior distribution
\(P(\theta_{A})\) or \(P(\theta_{b})\). We use \(P(\theta_{A})\) as the other
case is analogous.

The posterior is
\[
  P(\theta_{A} \mid \V_{A}) = \frac{1}{P(\V_{A})}P(\theta_{A})\theta_{A}^{\#(a=1)}(1-\theta_{A})^{\#(a=0)}
\]

The most convenient choice for the prior is a Beta distribution as conjugacy
will hold.

\[
  \theta_{A} \sim \text{Beta}(\alpha_{A}, \beta_{A}) \implies P(\theta_{A})  = \frac{1}{B(\alpha_{A}, \beta_{A})}\theta_{A}^{\alpha_{A}-1}(1-\theta_{A})^{\beta_{A} - 1}
\]
So it follows that
\[
  (\theta_{A} \mid \V_{A}) \sim \text{Beta}(\theta_{A} \mid \alpha_{A} + \#(A=1), \beta_{A} + \#(A = 0))
\]

The marginal is then
\[
  \begin{aligned}
    P(A = 1 \mid \V_{A})
    &= \frac{P(A = 1, \V_{A})}{P(\V_{A})} = \int_{\theta_{A}}  \frac{P(A = 1, \V_{A}, \theta_{A})}{P(\V_{A})} =  \int_{\theta_{A}}  \frac{P(A = 1 \mid \V_{A}, \theta_{A}) P(\V_{A}, \theta_{A})}{P(\V_{A})} \\
    &=  \int_{\theta_{A}}  \frac{P(A = 1 \mid \V_{A}, \theta_{A}) P(\theta_{A} \mid \V_{A})P(\V_{A})}{P(\V_{A})} = \int_{\theta_{A}}P(\theta_{A}\mid \V_{A})\theta_{A} = \E[\theta_{A} \mid \V_{A}] \\
    &= \frac{\alpha_{A} + \#(A= 1)}{\alpha_{A} + \#(A=1) + \beta_{A} + \#(A=0)}
  \end{aligned}
\]

For \(P(d \mid a ,b)\) the situation is more complex, the most convenient way is
to specify a Beta prior for each one of the four components of \(\theta_{D}\).
Lets focus on \(P(D = 1 \mid A = 1, B = 0)\), notice the parameters \(\alpha\)
and \(\beta\) we used before now depend on \(a\) and \(b\), for this reason we
are using \(\alpha_{D}(a,b)\) and \(\beta_{D}(a,b)\) as prior parameters, these
are called \emph{hyperparameters}.
\[
  P(\theta_{2}) = B(\theta_{2} \mid \alpha_{D}(1,0) + \#(D = 1, A = 1, B = 0), \beta_{D}(1,0) + \#(D = 0, A = 1, B = 0))
\]

As before we got that

\[
  P(D = 1 \mid A = 1, B = 0, \V) = \frac{\alpha_{D}(1,0) + \#(D = 1, A = 1, B = 0)}{\alpha_{D}(1,0) + \beta_{D}(1,0) + \#(A=1, B = 0)}
\]

In case we had no preference, we could set all hyperparameters to the same
value, and, a complete ignorance prior would correspond to set them to 1.

Let now consider two limit possibilities, the one where we have no data at all,
and the one where we have infinite data.

In case we have no data, the marginal probability corresponds to the prior which
in the last case is
\[
   P(D = 1 \mid A = 1, B = 0, \V) = \frac{\alpha_{D}(1,0)}{\alpha_{D}(1,0) + \beta_{D}(1,0)}
 \]
 Note that equal hyperparameters would give a result of \(0.5\).

 When infinite data is available, the marginal is generally dominated by it,
 this corresponds to the Maximum Likelihood solution.
 \[
   P(D = 1 \mid A = 1, B = 0, \V) = \frac{\#(D = 1, A = 1 , B = 0)}{\#(A = 1, B = 0)}
 \]
 This happens unless the prior has a pathologically strong effect.

 Consider the data given in the table in figure \ref{fig:bayesian_example}, and
 equal parameters and hyperparameters \(1\). Then we can compute the differences
 between this and using the Maximum Likelihood technique.
 \[
   P(A = 1 \mid \V) = \frac{1 + \#(A = 1)}{2 + N} = \frac{5}{9} \approx 0.556
 \]
 By comparison, the Maximum Likelihood result is \(4/7 = 0.571\), the Bayesian
 result is more prudent than this one, which fits in with our belief that any
 setting is equally probable i.e \(0.5\).


 The natural generalization to more than two states is using a Dirichlet
 distribution as prior, assuming i.i.d data and local and global prior
 independence. We are considering two different scenarios, firstly one where the
 variable has no parents, as the case for \(A\) and \(B\) in the previous
 example. Secondly, we will consider a variable with a non void set of parents,
 as in the case with the disease \(D\).

 Consider a variable \(X\) with
 \(Dom(X) = \{1, \dots, I\}, \ \theta = (\theta_{1},\dots, \theta_{I})\), then
 \[
   P(x \mid \theta) = \prod_{i = 1}^{I}\theta_{i}^{\mathbb{I}[x = i]} \text{
   with  } \sum_{i=1}^{I}\theta_{i} = 1
\]
So that the posterior (considering \(N\) observations of the variable
\((x_{1}, \dots, x_{N}) = \V\)) is
\[
  P(\theta \mid x_{1},\dots,x_{N}) = \frac{1}{P(\V)} P(\theta) \prod_{n = 1}^{N}\prod_{i =1 }^{I}\theta_{i}^{\mathbb{I}[x_{n} = i]} =  \frac{1}{P(\V)} P(\theta) \prod_{i = 1}^{I} \theta_{i}^{\sum_{n} \mathbb{I}[x_{n}=i]}
\]
Then assuming a Dirichlet prior with hyperparameters \(\bm{u} = (u_{1}, \dots, u_{I})\)
\[
  P(\theta) = \frac{1}{B(\bm{u})}\prod_{i =1}^{I}\theta_{i}^{u_{i}-1} \implies P(\theta \mid \V) = \frac{1}{B(\bm{u})P(\V)}\prod_{i=1}^{I}\theta_{i}^{u_{i}-1 + \sum_{n}\mathbb{I}[x_{n} = i]}
\]

Which means that, defining \(\bm{c} = ( \sum_{n=1}^{N}\mathbb{I}[x_{n} = i])_{i = 1,\dots,I}\)
\[
  P(\theta \mid \V) \sim \text{Dirichlet}(\bm{u} + \bm{c})
\]

The marginal is then given by
\[
  \begin{aligned}
    P(X=i \mid \V) &= \int_{\theta}P(X=i \mid \theta)P(\theta \mid \V) =  \int_{\theta}\theta_{i}P(\theta \mid \V)\\
    &=  \int_{\theta_{i}}\theta_{i}P(\theta_{i} \mid \V) = \E[\theta_{i} \mid \V]
\end{aligned}
\]
Where we used that
\[\int_{\theta_{j \neq i}}\theta_{i} P(\theta \mid \V) = \theta_{i}\prod_{k\neq j}P(\theta_{k} \mid V) \int_{\theta_{j}}P(\theta_{j}\mid \V) = \theta_{i} \prod_{k \neq j}P(\theta_{k} \mid \V)\]


As we already know from Proposition \ref{prop:dirichlet_marginal}, the univariate marginal of a Dirichlet distribution is a
Beta Distribution, then
\[
  (\theta_{i} \mid \V) \sim \text{Beta}(u_{i} + c_{i}, \sum_{j\neq i} u_{j} + c_{j})
\]
So the marginal is
\[
  P(X = i \mid \V) = \frac{u_{i} + c_{i}}{\sum_{j}u_{j} + c_{j}}
\]


Consider now that \(X\) has a set of parent variables \(pa(X)\), in this case,
we want to compute the marginal given a state of its parents and the data
\[
  P(X = i \mid pa(X) = \bm{j}, \V)
\]
Let set the following notation for the parameters
\[
  P(X = i \mid pa(X) = \bm{j}, \theta) = \theta_{i,\bm{j}} \hspace{2cm} \bm{\theta_{j}} = (\theta_{1,\bm{j}},\dots, \theta_{I,\bm{j}})
\]
Local independence means that
\[
  P(\bm{\theta}) = \prod_{j}P(\bm{\theta_{j}})
\]

As we did before, we consider a Dirichlet prior
\[
  \bm{\theta_{j}} \sim Dirichlet(\bm{u_{j}})
\]
the posterior is then
\[
  \begin{aligned}
    P(\bm{\theta} \mid \V) &= \frac{P(\bm{\theta})P(\V \mid \bm{\theta}) }{P(\V)} = \frac{1}{P(\V)}\Big(\prod_{\bm{j}}P(\bm{\theta_{j}}) \Big)P(\V \mid \bm{\theta}) \\
    &= \frac{1}{P(\V)}\Big(\prod_{\bm{j}}\frac{1}{B(\bm{u_{j}})}\prod_{i}\theta_{i,\bm{j}}^{u_{i,\bm{j}}-1}\Big) P(\V \mid \bm{\theta})\\
    &= \frac{1}{P(\V)}\Big(\prod_{\bm{j}}\frac{1}{B(\bm{u_{j}})}\prod_{i}\theta_{i,\bm{j}}^{u_{i,\bm{j}}-1}\Big) \Big(\prod_{n}\prod_{\bm{j}}\prod_{i} \theta_{i,\bm{j}}^{\mathbb{I}[x_{n} = i, pa(x_{n}) = \bm{j}]}\Big)\\
    &= \frac{1}{P(\V)}\prod_{\bm{j}}\frac{1}{B(\bm{u_{j}})}\prod_{i}\theta_{i,\bm{j}}^{u_{i,\bm{j}}-1 + \#(X = i,pa(X)=\bm{j})}
  \end{aligned}
\]
Naming \(\bm{v_{j}} = \bm{u_{j}} + \#(X = i, pa(X) = \bm{j})\), the posterior
is
\[
  (\bm{\theta} \mid \V) \sim \prod_{j}\text{Dirichlet}(\bm{v_{j}})
\]

Noting \(v_{i,j}\) the components of \(\bm{v_{j}}\), the marginal is then
\[
  P(X=i, pa(X) = \bm{j}, \V) = \frac{v_{i,j}}{\sum_{i}v_{i,j}}
\]

\subsection{Structure Learning}

So far, both data and the BN have given to us. However, the BN structure is not
always given and may be learned also form the data. Even considering complete
data (no missing observations), there are some problems that need to be taken
into account.
\begin{itemize}
    \item The number of Belief networks is exponential over the number of
    variables so a brute force algorithm would not be viable.
  \item Testing dependencies requires a large amount of data. So a threshold
    must be set to measure when a dependence is significant.
    \item A Belief Network or a Markov Network may not be enough to represent
    the data due to the existence of unobserved variables.
\end{itemize}


\begin{algorithm}[H]
  \SetAlgoLined
  \KwData{Complete undirected graph \(G\), with vertices \(\V\)}
  \KwResult{\(G\) with removed links}
  \(i = 0\)\;
  \While{all nodes have \(\leq i\) neighbors}{
    \For{\(X \in \V\)}{
      \For{\(Y \in ne(x)\)}{
        \If{\(\exists S \subset ne(X)\backslash Y\) such that \(\#S = i\) and
          \(X \bigCI Y \mid S\)}{
          Remove \(X-Y\) from \(G\)\;
          \(S_{XY} = S\)\;
        }
      }
    }
    \(i = i+1\)\;
  }
  \caption{PC Algorithm}
  \label{alg:pc1}
\end{algorithm}

An approach to learn the structure is the PC algorithm, it begins with a
complete graph \(G\) and tries to remove as many links as possible studying the
independence of the variables.

The algorithm \ref{alg:pc1} iterates over a natural counter, ending when it gets bigger
than all existent neighborhoods. It chooses a linked pair of variables \(X - Y\) and
a subset \(S_{XY} \subset ne(X)\), following it has the desired size and
\(Y \notin S_{XY}\). If \(X \bigCI Y \mid S\), then the link is removed and
\(S_{XY}\) is stored. The main idea behind this is the set of independencies is
faithful to a graph then there is no link between two nodes \(X\) and \(Y\) if
and only if there exists a subset of \(ne(X)\) such that they are independent
given this subset.

When this process ends, the undirected graph may be constructed following one rule,
for any undirected link \(X - Y - Z\), if \(Y \notin S_{XZ}\) then set
\(X \to Y \leftarrow Z\). The rest of links may oriented arbitrarily not
creating cycles or colliders. The reasoning behind this is using the
d-separation theorem \ref{th:d-separation}, if \(Y \in S_{XZ}\) and
\(X \bigCI Z \mid S_{XZ}\) then we want \(S_{XZ}\) to d-separate them, that is,
using any configuration that doesn't create a collider in \(S_{XZ}\). On the
other hand if \(Y \notin S_{XZ}\) then \(X \bigCD Z \mid Y\) so \(Y\) d-connect
them, to get this we set it as a collider.

Our main concern now is given three variables \(X, Y, Z\) to measure \(X \bigCI Y \mid Z\).

\begin{definition}
  Given two random variables \(X, Y\), we define their \emph{mutual information} as the Kullback-Leibler divergence of their joint distribution and the product of their marginals and
  \[
    MI(X;Y) = KL(P_{X,Y}\mid P_{X}P_{Y})
  \]
\end{definition}

\begin{definition}
  Given three random variables \(X, Y, Z\) we define the \emph{conditional mutual information} of \(X\) and \(Y\) over \(Z\) as
  \[
    MI(X;Y\mid Z) = \E_{Z}[KL(P_{X,Y \mid Z} \mid P_{X\mid Z} P_{Y \mid Z})]
  \]
\end{definition}
Where \(MI(X;Y \mid Z) \geq 0\) and \(MI(X;Y \mid Z) = 0 \iff P_{X,Y \mid Z} = P_{X\mid Z} P_{Y \mid Z} \iff X\bigCI Y \mid Z\). We can estimate this using the empirical distributions, however, this \emph{empirical} mutual information will be typically greater than \(0\) even when \(X\bigCI Y \mid Z\), therefore a threshold must be established.


\section{Undirected Models}
