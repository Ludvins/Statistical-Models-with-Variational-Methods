#+TITLE:  Statistical Models with Variational Methods
#+SUBTITLE: End-of-degree project
#+LANGUAGE: en
#+AUTHOR: Luis Antonio Ortega Andrés @@latex: \\@@ 76425628D @@latex: \\@@ ludvins@correo.ugr.es
#+OPTIONS: toc:t num:3

#+latex_header: \usepackage[eng, exjob]{KTHEEtitlepage}
#+LATEX_HEADER: \renewcommand\maketitle{\makeititle}

#+latex_class: scrreprt
#+latex_class_options: [oneside,openright,titlepage,numbers=noenddot,openany,headinclude,footinclude=true,cleardoublepage=empty,abstractoff,BCOR=5mm,paper=a4,fontsize=12pt,ngerman,american]

#+latex_header_extra: \usepackage[T1]{fontenc}
#+latex_header_extra: \usepackage[beramono,eulerchapternumbers,linedheaders,parts,a5paper,dottedtoc,manychapters]{classicthesis}
#+latex_header_extra: \input{setup}
#+latex_header_extra: \input{classicthesis-config}
#+latex_header: \input{macros}


#+latex_header: \usepackage{tikz}
#+latex_header: \usetikzlibrary{positioning,shapes,arrows}
#+latex_header: \usepackage{dcolumn}
#+latex_header: \usepackage{booktabs}
#+latex_header: \usepackage{bm}

\clearpage

* Introduction

Some introduction about how important Variational methods are nowadays and what this project is about.

\clearpage

* Basic concepts
** Probability


All our theory will be made under the assumption that there is a /referential
set/ $\Omega$, set of all possible outcomes of an experiment. Any subset of
$\Omega$ will be called /event/.

#+begin_definition
Let $\mathcal{P}(\Omega)$ be the power set of $\Omega$. Then, $\mathcal{F} \subset \mathcal{P}(\Omega)$ is called a
/\sigma-algebra/ if it satisfies:
+ $\Omega \in \mathcal{F}$.
+ $\mathcal{F}$ is closed under complementation.
+ $\mathcal{F}$ is closed under countable unions.
From these properties it follows that $\emptyset \in \mathcal{F}$ and that $\mathcal{F}$
is closed under countable intersections.

The tuple $(\Omega, \mathcal{F})$ is called a /measurable space/.
#+end_definition

#+begin_definition
A /probability/ $P$ over $(\Omega, \mathcal{F})$ is a mapping
$P: \mathcal{F} \to \mathbb{R}$ which satisfies
+ $P(\alpha) \geq 0 \ \ \forall \alpha \in \mathcal{F}$.
+ $P(\Omega) = 1$.
+ If $\alpha$, $\beta \in \mathcal{F}$ and $\alpha \cap \beta = \emptyset$, then
  $P(\alpha \cup \beta) = P(\alpha) + P(\beta)$.
#+end_definition


The first condition guarantees non negativity. The second one states that the
/trivial event/ has the maximal possible probability of 1. The third condition implies that given two mutually disjoint events,
the probability of either one of them occurring is equal to the sum of the
probabilities of each one.

From these conditions it follows that $P(\emptyset) = 0$ and $P(\alpha \cup \beta)
= P(\alpha) + P(\beta) - P(\alpha \cap \beta)$.

#+begin_proposition
For any sequence $\{\alpha_n\}_{n \in \mathbb{N}}$ such that $\alpha_i \subset
  \alpha_{i+1} \ \forall i \in \mathbb{N}$ and $\alpha_n
  \xrightarrow{n \to \infty} \Omega$, then $P(\alpha_n)
  \xrightarrow{n \to \infty} P(\Omega) = 1$.
#+end_proposition

*************** Note. Proof this or smth.

The triple $(\Omega, \mathcal{F}, P)$ is called a /probability space/.

#+begin_definition
A function $f:\Omega_1 \to \Omega_2$ between two
measurable spaces is said to be /measurable/ if $f^{-1}(\alpha) \in \mathcal{F}_1$ for every $\alpha \in \mathcal{F}_2$.
#+end_definition

#+begin_definition
A /random variable/ is a measurable function $X:\Omega \to E$ from a probability
space $(\Omega, \mathcal{F}, P)$ to a measurable space $E$.

The probability of $X$ taking a value on a measurable set $S \subset E$ is
written as
$$
P(X \in S) = P(\{a \in \Omega \ | \ X(a) \in S \}).
$$
#+end_definition

We will adopt the following notation from now on: random variables will be
denoted with an upper case letter like $X$ and a set of variables with a
bold symbol like $\bm{X}$ The meaning of $P(state)$ will be clear without a reference to the variable.
Otherwise $P(X = state)$ will be used.
We will denote by $P(x)$ the probability of $X$ taking a specific value, which means
that
$$\int_x f(x) = \int_{dom(X)}f(X=s) \, ds$$

Also $P(x \text{ or } y) = P(x \cup y)$ and $P(x,y) = P(x \cap y)$.

We will define some concepts regarding a joint distribution $P(x,y)$, that is to say, the probability of both random variables $x$ and $y$.

**************** introduce el concepto de variable continua y discreta. usa p para la función de distribución de una discreta y f para la densidad de una continua


#+begin_definition
A /marginal distribution/ $p(x)$ of the joint distribution is the
distribution of a single variable given by
$$
p(x) = \sum_y p(x,y) \hspace{2cm} p(x) = \int_y p(x,y)
$$
#+end_definition

We can understand this as the probability of an event irrespective of the outcome
of the other variable.

#+begin_definition
The /conditional probability/ of $x$ given $y$ is defined as
$$
p(x|y) = \frac{p(x,y)}{p(y)}
$$

If $p(y) = 0$ then it is not defined.
#+end_definition

This formula is also known as /Bayes' rule/. With this definition the
conditional probability is the probability of one event occurring in the presence of a
second event. \\

#+begin_theorem
*(Bayes' rule)*.

#+end_theorem

Now suppose we have some observed data $\mathcal{D}$ and we want to learn about
a set of parameters \theta. Using Bayes' rule we got that

$$
p(\theta|\mathcal{D}) = \frac{p(\mathcal{D}|\theta)p(\theta)}{p(\mathcal{D})} =
\frac{p(\mathcal{D}|\theta)p(\theta)}{ \int_{\theta} p(\mathcal{D}|\theta)p(\theta)}
$$

This shows how from a /generative model/ $p(\mathcal{D}|\theta)$ of the dataset
and a /prior/ belief $p(\theta)$, we can infer the /posterior/ distribution
$p(\theta|\mathcal{D})$. \\

#+begin_exampleth
Consider a study where the relation of a disease $d$ and an habit $h$
is being investigated. Suppose that $p(d)=10^{-5}$, $p(h)=0.5$ and $p(h|d) = 0.9$. What is the
probability that a person with habit $h$ will have the disease $d$?

$$
p(d|h) = \frac{p(d,h)}{p(h)} = \frac{p(h|d)p(d)}{p(h)} =
\frac{ 0.9 \times 10^{-5}}{ 0.5 } = 1.8 \times 10^{-5}
$$

If we set the probability of having habit $h$ to a much lower value as $p(h) =
0.001$, then the above calculation gives approximately $1/100$. Intuitively, a smaller number of people have the habit and most of them have the
desease. This means that the relation between having the desease and the habit
is stronger now compared with the case where more people had the habit.
#+end_exampleth

#+begin_definition
We say that events $x$ and $y$ are /independent/ if knowing one of them doesn't give any extra information about the other. Mathematically,
$$p(x,y) = p(x) p(y)$$

From this it follows that if $x$ and $y$ are independent, then $p(x|y) = p(x)$.
#+end_definition

** Graphical models

#+begin_definition
A /graph/ $G = (V,E)$ is a set of vertices or nodes $V$ and edges $E\subset
V\times V$ between them.
These edges may be directed (have arrow in a single direction) or undirected. If all the edges of a graph are directed, it is called a /directed graph/, and if all of them are undirected, it is called an /undirected graph/.
#+end_definition

#+begin_latex
\begin{center}
\begin{tikzpicture}[
  node distance=1cm and 0.5cm,
  mynode/.style={draw,circle,text width=0.5cm,align=center}
]

\node[mynode] (a) {A};
\node[mynode,below right=of a] (b) {B};
\node[mynode,above right=of b] (c) {C};

\node[mynode, right=of c] (d) {A};
\node[mynode,below right=of d] (e) {B};
\node[mynode,above right=of e] (f) {C};

\path (c) edge[-latex] (a)
(a) edge[-latex] (b)
(b) edge[latex-] (c);

\draw (d) -- (e) -- (f) -- (d)

\end{tikzpicture}
\end{center}
\captionof{figure}{Example of directed and undirected graph, respectively.}
\label{fig:graphs}
#+end_latex

#+begin_definition
A /path/ $A \to B$ is a sequence of vertices ${A = A_0, A_1,\dots,A_{n-1}, A_n = B}$ where $(A_n, A_{n-1})$ an edge of the graph. In a directed graph, if the edges follow the sequence, the resulting path is called a /directed path/.
#+end_definition

#+begin_definition
Let $A,B$ be two vertices. If $A \to B$ and $B \not \to A$, then $A$ is called an /ancestor/ of $B$ and $B$ is called a /descendant/ of $A$.
#+end_definition

For example, in the figure \ref{fig:graphs}, $C$ is an ancestor of $B$.

#+begin_definition
A /directed acyclic graph (DAG)/ is a directed graph such that no directed path between any two nodes revisits a vertex.
#+end_definition

#+begin_latex
\begin{center}
\begin{tikzpicture}[
  node distance=1cm and 0.5cm,
  mynode/.style={draw,circle,text width=0.5cm,align=center}
]

\node[mynode] (a) {A};
\node[mynode,below right=of a] (b) {B};
\node[mynode,above right=of b] (c) {C};

\path (c) edge[-latex] (a)
(a) edge[-latex] (b)
(b) edge[-latex] (c);

\end{tikzpicture}
\end{center}
\captionof{figure}{Example of graph which isn't a DAG.}
\label{fig:not_dag}
#+end_latex

As we can see in the figure \ref{fig:not_dag}, $A \to B \to C \to A \to B$ is a
path from $A$ to $B$ that revisits $A$.

Now where are going to define some relations between nodes in a DAG.

#+begin_definition
The /parents/ of a node $A$ is the set of nodes $B$ such that there is a
directed edge from $B$ to $A$. The same applies for the /children/ of a node.

The /Markov blanket/ of a node is composed by the node itself, its children, its parents and the parents
of its children.
#+end_definition


#+begin_latex
\begin{center}
\begin{tikzpicture}[
  node distance=1cm and 0.5cm,
  mynode/.style={draw,circle,text width=0.5cm,align=center}
]

\node[mynode] (a) {A};
\node[mynode,below right=of a] (b) {B};
\node[mynode,above right=of b] (c) {C};
\node[mynode,below right=of b] (d) {D};
\node[mynode,below left=of b] (e) {E};
\node[mynode,above right=of d] (f) {F};
\node[mynode, above right=of f] (h) {H};

\path (c) edge[-latex] (a)
(a) edge[-latex] (b)
(b) edge[latex-] (c)
(b) edge[-latex] (e)
(c) edge[-latex] (f)
(b) edge[-latex] (d)
(f) edge[-latex] (d)
(h) edge[-latex] (f)
;

\end{tikzpicture}
\end{center}
\captionof{figure}{Directed acyclic graph}
\label{fig:relations}
#+end_latex



#+begin_definition
In a graph, the /neighbors/ of a node are those directly connected
to it.
#+end_definition

We can use figure \ref{fig:relations} to reflect on these definitions. The parents
of $B$ are $pa(B) = \{A,C\}$ and its children are $ch(B) = \{E,D\}$. Taking this into account, its neighbors
are $ne(B) = \{A,C,E,D\}$ and its Markov blanket is $\{A,B,C,D,E,F\}$.

#+begin_definition
A /graphical model/ is a probabilistic model for which a graph expresses the
conditional dependence structure between random variables.
#+end_definition

Commonly, they provide a graph-based representation for encoding a multi-dimensional
distribution representing a set of independences that hold in the specific
distribution. The most commonly used are /Bayesian networks/ and /Markov random
fields/, which differ in the set of independences they can encode and the
factorization of the distribution that they include.

* Belief networks

Consider we have $N$ variables with the corresponding distribution
$p(x_1,\dots,x_N)$. Let $\mathcal{E}$ be a set of indexes such as ~evidence~
$=\{x_e = \times _e \ | \ e \in \mathcal{E}\}$. Inference could be made by brute
force:

$$
p(x_i = \times _i \ | \ \texttt{evidence}) = \frac{ \int_{ j \not \in
\mathcal{E}, j \neq i } p(\texttt{evidence}, x_j, x_i = \times_i)}{ \int_{ j
\not \in \mathcal{E} } p(\texttt{evidence}, x_j)}
$$

The notation when using discrete variables is analogous replacing integration
with summations.

Lets suppose all these variables are binary, this calculation will require
$O(2^{N-\#\mathcal{E}})$ operations. Also, all entries of a table $p(x_1,\dots,
x_N)$ take $O(2^N)$ space.

This is unpractical when taking into account millions of variables. The
underlying idea of belief networks is to specify which variables are independent
of others, factoring the joint probability distribution.

#+begin_definition
A /belief network/ is a distribution of the form
$$
p(x_1,\dots,x_N) = \prod_{i=1}^{N}p(x_i | pa(x_i))
$$
#+end_definition

We can write it as a DAG where the $i^{th}$ node corresponds to the factor
$p(x_i|pa(x_i))$.

*************** TODO Example

* Graphical Model Test with Tikz

This is a test of making a graphical model in latex using Tikz package.

\begin{tikzpicture}[
  node distance=1cm and 0cm,
  mynode/.style={draw,ellipse,text width=1.5cm,align=center}
]

\node[mynode] (sp) {Sprinkler};
\node[mynode,below right=of sp] (gw) {Grass wet};
\node[mynode,above right=of gw] (ra) {Rain};
\path (ra) edge[-latex] (sp)
(sp) edge[-latex] (gw)
(gw) edge[latex-] (ra);

\node[left=0.5cm of sp] {
  \begin{tabular}{M{2}M{2}}
  \toprule
  & \multicolumn{2}{c}{Sprinkler} \\
  Rain & \multicolumn{1}{c}{T} & \multicolumn{1}{c}{F} \\
  \cmidrule(r){1-1}\cmidrule(l){2-3}
  F & 0.4 & 0.6 \\
  T & 0.01 & 0.99 \\
  \bottomrule
  \end{tabular}
};

\node[right=0.5cm of ra] {
  \begin{tabular}{M{1}M{1}}
  \toprule
  \multicolumn{2}{c}{Sprinkler} \\
  \multicolumn{1}{c}{T} & \multicolumn{1}{c}{F} \\
  \cmidrule{1-2}
  0.2 & 0.8 \\
  \bottomrule
  \end{tabular}
};

\node[below=0.5cm of gw] {
  \begin{tabular}{M{2}M{2}}
  \toprule
  & & \multicolumn{2}{c}{Grass wet} \\
  \multicolumn{2}{l}{Sprinkler rain} & \multicolumn{1}{c}{T} & \multicolumn{1}{c}{F} \\
  \cmidrule(r){1-2}\cmidrule(l){3-4}
  F & F & 0.4 & 0.6 \\
  F & T & 0.01 & 0.99 \\
  T & F & 0.01 & 0.99 \\
  T & T & 0.01 & 0.99 \\
  \bottomrule
  \end{tabular}
};

\end{tikzpicture}


\clearpage
Cites so the references appear (testing) \cite{koller_friedman}
\cite{barber}
\cite{wainwright}
#+BIBLIOGRAPHY: refs plain
