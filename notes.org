#+TITLE:  Statistical Models with Variational Methods
#+SUBTITLE: End-of-degree project
#+LANGUAGE: en
#+AUTHOR: Luis Antonio Ortega Andr√©s @@latex: \\@@76425628D @@latex: \\@@ ludvins@correo.ugr.es
#+OPTIONS: toc:t num:2

#+latex_class_options: [oneside,openright,titlepage,numbers=noenddot,openany,headinclude,footinclude=true,cleardoublepage=empty,abstractoff,BCOR=5mm,paper=a4,fontsize=12pt,ngerman,american]
#+latex_header_extra: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage[AUTO]{babel}
#+latex_header_extra: \usepackage{minted}
#+latex_header_extra: \usepackage[beramono,eulerchapternumbers,linedheaders,parts,a5paper,dottedtoc,manychapters]{classicthesis}

#+latex_header: \usepackage{tikz}
#+latex_header: \usetikzlibrary{positioning,shapes,arrows}
#+latex_header: \usepackage{dcolumn}
#+latex_header: \usepackage{booktabs}

#+latex_header_extra: \input{setup}
#+latex_header_extra: \input{classicthesis-config}
#+latex_header: \input{macros}
\clearpage


* Probability

** Definitions

#+begin_definition
An /event/ is a set of outcomes of an experiment to which a probability is assigned. 
This definition is made over the assumition that there is a /sample space/ $\Omega$, set of all possible otcomes of the experiment.

This sample space must:
+ Contain the /empty event/ $\empty$ and the /trivial event/ $\Omega$.
+ Be closed under union.
+ Be closed under complementation.
#+end_definition

Closeness under union and complementation implies closeness under intersection and set difference.

#+begin_definition
Let $S$ be a set of /meassurable events/ of the sample space $\Omega$, that is, $\forall \alpha \in S$ $\alpha$ is a subset of $\Omega$.
A /probability distribution/ $p$ over $(\Omega, S)$ is a mapping from $S$ to $\mathbb{R}$, following:
+ $p(\alpha) \geq 0 \ \forall \alpha \in S$.
+ $p(\Omega) = 1$
+ If $\alpha$, $\beta \in S$ and $\alpha \cap \beta = \empty$, then $p(\alpha \cup \beta) = p(\alpha) + p(\beta).
#+end_definition

The first condition implies non negativity. The second one states that the /trivial evet/ has the maximal possible probability of 1. The last condition states that given two mutually disjoint events, 
the probability of one of them is equal to the sum of the probabilities of each one.

From these conditions follows that $p(\empty) = 0$ and $p(\alpha \cup \beta) = p(\alpha) + p(\beta) - p(\alpha \cap \beta)$.

From now on, we will denote $p(\alpha \text{ or } \beta) = p(\alpha \cup \beta)$ and $p(\alpha, \beta) = p(\alpha \cap \beta)$. 
Variables will be denoted with lower case $x$ and a set of variables with a
calligraphic symbol like $\mathcal{V}$. The meaning of $p(state)$ will be clear without a reference to the variable.
Otherwise $p(x = state)$ will be used. \\

We will denote $p(x)$ the probability of $x$ taking a specific value, this means
that
$$\int_x f(x) = \int_{dom(x)}f(x=s) ds$$

We will define some concepts from a given joint distribution $p(x,y)$, this is,
the probability of two events.\\

#+begin_definition
A /marginal/ $p(x)$ of the joint distribution is the
distribution of a single variable given by
$$
p(x) = \sum_y p(x,y) \hspace{2cm} p(x) = \int_y p(x,y)
$$
#+end_definition

We can undestand this as the probability of an event irrespective of the outcome
of another variable.


#+begin_definition
The /conditional probability/ of $x$ given $y$ is defined as
$$
p(x|y) = \frac{p(x,y)}{p(y)}
$$

If $p(y) = 0$ then it is not defined.
#+end_definition

This formula is also known as /Bayes' rule/. With this definition the
conditional probability is the probability of one event occurring in the presence of a
second event. \\

Now suppose we have some observed data $\mathcal{D}$ and we want to learn about
a set of parameters \theta. Using Bayes' rule we got that

$$
p(\theta|\mathcal{D}) = \frac{p(\mathcal{D}|\theta)p(\theta)}{p(\mathcal{D})} =
\frac{p(\mathcal{D}|\theta)p(\theta)}{ \int_{\theta} p(\mathcal{D}|\theta)p(\theta)}
$$

This shows how from a /generative model/ $p(\mathcal{D}|\theta)$ of the dataset
and a /prior/ belief $p(\theta)$, we can infer the /posterior/ distribution
$p(\theta|\mathcal{D})$. \\

#+begin_exampleth
Consider a study where the relation of a disease $D$ and an habit $H$
is being investigated. Consider $p(D)=10^{-5}$, $p(H)=0.5$ and $p(H|D) = 0.9$. What is the
probability that a person with habit $H$ will have disease $D$?

$$
p(D|S) = \frac{p(D,H)}{p(D)} = \frac{p(H|D)p(D)}{p(H)} =
\frac{ 0.9 \times 10^{-5}}{ 0.5 } = 1.8 \times 10^{-5}
$$

If we set the probability of having habit $H$ to a much lower value as $p(H) =
0.001$, then the above calculation gives aproximately $1/100$.\\

Intuitively, a smaller number of people have the habit and most of them have the
desease. This means that the relation between having the desease and the habit
is stronger.
#+end_exampleth

#+begin_definition
We say that events $x$ and $y$ are /independent/ if knowing one of them doesn't give any extra information about the other. Mathematically, 
$$ p(x,y) = p(x) p(y)$

From this follows that, if $x$ and $y$ are independent, then $p(x|y) = p(x)$.
#+end_definition

* Graphical Models

#+begin_definition
A /graph/ G is a set of vertices (nodes) and edges (links) between the vertices.

This edges may be directed (have arrow in a single direction) or undirected. If all the edges of a graph are directed, it is called a /directed graph/, if all of them are undirected, is called a *undirected graph*.
#+end_definition

#+begin_definition
A /path/ $A \to B$ is a sequence of vertices ${A_0 = A, A_1,\dots,A_{n-1}, A_n = B}$ where $(A_n, A_{n=1})$ an edge of the graph. In a directed graph, if the edges follow the sequence, if is called a *directed path*$.

Let $A,B$ be two vertices, if $A \to B$ and $B \not \to A$, then $A$ is called an /ancestor/ of $B$ and $B$ is called a /descendant/ of $A$.
#+end_definition

#+begin_definition
A /directed acyclic graph (DAG)/ is a directed graph such that no directed path from any node to another revisits a vertex.
#+end_definition


** Graphical Model Test with Tikz

#+BEGIN_latex
\begin{tikzpicture}[
  node distance=1cm and 0cm,
  mynode/.style={draw,ellipse,text width=2cm,align=center}
]

\node[mynode] (sp) {Sprinkler};
\node[mynode,below right=of sp] (gw) {Grass wet};
\node[mynode,above right=of gw] (ra) {Rain};
\path (ra) edge[-latex] (sp)
(sp) edge[-latex] (gw)
(gw) edge[latex-] (ra);

\node[left=0.5cm of sp] {
  \begin{tabular}{cM{2}M{2}}
  \toprule
  & \multicolumn{2}{c}{Sprinkler} \\
  Rain & \multicolumn{1}{c}{T} & \multicolumn{1}{c}{F} \\
  \cmidrule(r){1-1}\cmidrule(l){2-3}
  F & 0.4 & 0.6 \\
  T & 0.01 & 0.99 \\
  \bottomrule
  \end{tabular}
};

\node[right=0.5cm of ra] {
  \begin{tabular}{M{1}M{1}}
  \toprule
  \multicolumn{2}{c}{Sprinkler} \\
  \multicolumn{1}{c}{T} & \multicolumn{1}{c}{F} \\
  \cmidrule{1-2}
  0.2 & 0.8 \\
  \bottomrule
  \end{tabular}
};

\node[below=0.5cm of gw] {
  \begin{tabular}{ccM{2}M{2}}
  \toprule
  & & \multicolumn{2}{c}{Grass wet} \\
  \multicolumn{2}{l}{Sprinkler rain} & \multicolumn{1}{c}{T} & \multicolumn{1}{c}{F} \\
  \cmidrule(r){1-2}\cmidrule(l){3-4}
  F & F & 0.4 & 0.6 \\
  F & T & 0.01 & 0.99 \\
  T & F & 0.01 & 0.99 \\
  T & T & 0.01 & 0.99 \\
  \bottomrule
  \end{tabular}
};

\end{tikzpicture}
#+END_latex
