#+TITLE:  Statistical Models with Variational Methods
#+SUBTITLE: End-of-degree project
#+LANGUAGE: en
#+AUTHOR: Luis Antonio Ortega Andr√©s @@latex: \\@@ 76425628D @@latex: \\@@ ludvins@correo.ugr.es
#+OPTIONS: toc:t num:2

#+latex_header: \usepackage[eng, exjob]{KTHEEtitlepage}
#+LATEX_HEADER: \renewcommand\maketitle{\makeititle}

#+latex_class_options: [oneside,titlepage,openany,headinclude,footinclude=true,BCOR=5mm,paper=a4,ngerman,american]
#+latex_header_extra: \usepackage[T1]{fontenc}
#+latex_header_extra: \usepackage{minted}
#+latex_header_extra: \usepackage[beramono,eulerchapternumbers,linedheaders,parts,a5paper,dottedtoc,manychapters]{classicthesis}

#+latex_header: \usepackage{tikz}
#+latex_header: \usetikzlibrary{positioning,shapes,arrows}
#+latex_header: \usepackage{dcolumn}
#+latex_header: \usepackage{booktabs}


#+latex_header_extra: \input{setup}
#+latex_header_extra: \input{classicthesis-config}
#+latex_header: \input{macros}

\clearpage

* Introduction

Some introduction about how important Variational methods are nowadays and what this project is about.

\clearpage

* Basic concepts
** Probability


#+begin_definition
An /event/ is a set of outcomes of an experiment to which a probability is assigned.
This definition is made under the assumption that there is a /sample space/ $A$, the set of all possible outcomes of the experiment.
#+end_definition


#+begin_definition
Let $\mathcal{P}(A)$ be the power set of $A$. Then, $\mathcal{A} \subset \mathcal{P}(A)$ is called a
/\sigma-algebra/ if it satisfies:
+ $A \in \mathcal{A}$.
+ $\mathcal{A}$ is closed under complementation.
+ $\mathcal{A}$ is closed under countable unions.
From these properties it follows that $\emptyset \in \mathcal{A}$ and that $\mathcal{A}$
is closed under countable intersections.

The tuple $(A, \mathcal{A})$ is called a /measurable space/.
#+end_definition

#+begin_definition
A /probability distribution/ $p$ over $(A, \mathcal{A})$ is a mapping
$p: \mathcal{A} \to \mathbb{R}$ which satisfies
+ $p(\alpha) \geq 0 \ \ \forall \alpha \in \mathcal{A}$.
+ $p(A) = 1$.
+ If $\alpha$, $\beta \in A$ and $\alpha \cap \beta = \emptyset$, then $p(\alpha \cup \beta) = p(\alpha) + p(\beta)$.
#+end_definition
The first condition guarantees non negativity. The second one states that the /trivial event/ has the maximal possible probability of 1. The last condition implies that given two mutually disjoint events,
the probability of either one of them occurring is equal to the sum of the probabilities of each one.

From these conditions it follows that $p(\emptyset) = 0$ and $p(\alpha \cup \beta)
= p(\alpha) + p(\beta) - p(\alpha \cap \beta)$.

#+begin_definition
A function $f:(A_1, \mathcal{A}_1) \to (A_2, \mathcal{A}_2)$ between two
measurable spaces is said to be /measurable/ if $f^{-1}(\alpha) \in \mathcal{A}_1$ for every $\alpha \in \mathcal{A}_2$.
#+end_definition

#+begin_definition
A /random variable/ is a measurable function $X:A \to E$ from a set of possible
outcomes $A$ to a measurable space $E$.

The probability of $X$ taking a value on a measurable set $S \subset E$ is
written as
$$
p(X \in S) = p(\{\alpha \in A \ | \ X(\alpha) \in S \}).
$$
#+end_definition

We will adopt the following notation from now on: random variables will be denoted with lower case letter like $x$ and a set of variables with a
calligraphic symbol like $\mathcal{V}$. The meaning of $p(state)$ will be clear without a reference to the variable.
Otherwise $p(x = state)$ will be used.
We will denote by $p(x)$ the probability of $x$ taking a specific value, which means
that
$$\int_x f(x) = \int_{dom(x)}f(x=s) \, ds$$

Also $p(x \text{ or } y) = p(x \cup y)$ and $p(x,y) = p(x \cap y)$.

We will define some concepts regarding a joint distribution $p(x,y)$, that is to say, the probability of both random variables $x$ and $y$.

#+begin_definition
A /marginal distribution/ $p(x)$ of the joint distribution is the
distribution of a single variable given by
$$
p(x) = \sum_y p(x,y) \hspace{2cm} p(x) = \int_y p(x,y)
$$
#+end_definition

We can understand this as the probability of an event irrespective of the outcome
of the other variable.


#+begin_definition
The /conditional probability/ of $x$ given $y$ is defined as
$$
p(x|y) = \frac{p(x,y)}{p(y)}
$$

If $p(y) = 0$ then it is not defined.
#+end_definition

This formula is also known as /Bayes' rule/. With this definition the
conditional probability is the probability of one event occurring in the presence of a
second event. \\

Now suppose we have some observed data $\mathcal{D}$ and we want to learn about
a set of parameters \theta. Using Bayes' rule we got that

$$
p(\theta|\mathcal{D}) = \frac{p(\mathcal{D}|\theta)p(\theta)}{p(\mathcal{D})} =
\frac{p(\mathcal{D}|\theta)p(\theta)}{ \int_{\theta} p(\mathcal{D}|\theta)p(\theta)}
$$

This shows how from a /generative model/ $p(\mathcal{D}|\theta)$ of the dataset
and a /prior/ belief $p(\theta)$, we can infer the /posterior/ distribution
$p(\theta|\mathcal{D})$. \\

#+begin_exampleth
Consider a study where the relation of a disease $d$ and an habit $h$
is being investigated. Suppose that $p(d)=10^{-5}$, $p(h)=0.5$ and $p(h|d) = 0.9$. What is the
probability that a person with habit $h$ will have the disease $d$?

$$
p(d|h) = \frac{p(d,h)}{p(h)} = \frac{p(h|d)p(d)}{p(h)} =
\frac{ 0.9 \times 10^{-5}}{ 0.5 } = 1.8 \times 10^{-5}
$$

If we set the probability of having habit $h$ to a much lower value as $p(h) =
0.001$, then the above calculation gives approximately $1/100$. Intuitively, a smaller number of people have the habit and most of them have the
desease. This means that the relation between having the desease and the habit
is stronger now compared with the case where more people had the habit.
#+end_exampleth

#+begin_definition
We say that events $x$ and $y$ are /independent/ if knowing one of them doesn't give any extra information about the other. Mathematically,
$$p(x,y) = p(x) p(y)$$

From this it follows that if $x$ and $y$ are independent, then $p(x|y) = p(x)$.
#+end_definition

** Graphical models

#+begin_definition
A /graph/ $G = (V,E)$ is a set of vertices or nodes $V$ and edges $E\subset
V\times V$ between them.
These edges may be directed (have arrow in a single direction) or undirected. If all the edges of a graph are directed, it is called a /directed graph/, and if all of them are undirected, it is called an /undirected graph/.
#+end_definition

#+begin_latex
\begin{center}
\begin{tikzpicture}[
  node distance=1cm and 0.5cm,
  mynode/.style={draw,circle,text width=0.5cm,align=center}
]

\node[mynode] (a) {A};
\node[mynode,below right=of a] (b) {B};
\node[mynode,above right=of b] (c) {C};

\node[mynode, right=of c] (d) {A};
\node[mynode,below right=of d] (e) {B};
\node[mynode,above right=of e] (f) {C};

\path (c) edge[-latex] (a)
(a) edge[-latex] (b)
(b) edge[latex-] (c);

\draw (d) -- (e) -- (f) -- (d)

\end{tikzpicture}
\end{center}
\captionof{figure}{Example of directed and undirected graph, respectively.}
\label{fig:graphs}
#+end_latex

#+begin_definition
A /path/ $A \to B$ is a sequence of vertices ${A = A_0, A_1,\dots,A_{n-1}, A_n = B}$ where $(A_n, A_{n-1})$ an edge of the graph. In a directed graph, if the edges follow the sequence, the resulting path is called a /directed path/.
#+end_definition

#+begin_definition
Let $A,B$ be two vertices. If $A \to B$ and $B \not \to A$, then $A$ is called an /ancestor/ of $B$ and $B$ is called a /descendant/ of $A$.
#+end_definition

For example, in the figure \ref{fig:graphs}, $C$ is an ancestor of $B$.

#+begin_definition
A /directed acyclic graph (DAG)/ is a directed graph such that no directed path between any two nodes revisits a vertex.
#+end_definition

#+begin_latex
\begin{center}
\begin{tikzpicture}[
  node distance=1cm and 0.5cm,
  mynode/.style={draw,circle,text width=0.5cm,align=center}
]

\node[mynode] (a) {A};
\node[mynode,below right=of a] (b) {B};
\node[mynode,above right=of b] (c) {C};

\path (c) edge[-latex] (a)
(a) edge[-latex] (b)
(b) edge[-latex] (c);

\end{tikzpicture}
\end{center}
\captionof{figure}{Example of graph which isn't a DAG.}
\label{fig:not_dag}
#+end_latex

As we can see in the figure \ref{fig:not_dag}, $A \to B \to C \to A \to B$ is a
path from $A$ to $B$ that revisits $A$.

Now where are going to define some relations between nodes in a DAG.

#+begin_definition
The /parents/ of a node $A$ is the set of nodes $B$ such that there is a
directed edge from $B$ to $A$. The same applies for the /children/ of a node.

The /Markov blanket/ of a node is composed by the node itself, its children, its parents and the parents
of its children.
#+end_definition


#+begin_latex
\begin{center}
\begin{tikzpicture}[
  node distance=1cm and 0.5cm,
  mynode/.style={draw,circle,text width=0.5cm,align=center}
]

\node[mynode] (a) {A};
\node[mynode,below right=of a] (b) {B};
\node[mynode,above right=of b] (c) {C};
\node[mynode,below right=of b] (d) {D};
\node[mynode,below left=of b] (e) {E};
\node[mynode,above right=of d] (f) {F};
\node[mynode, above right=of f] (h) {H};

\path (c) edge[-latex] (a)
(a) edge[-latex] (b)
(b) edge[latex-] (c)
(b) edge[-latex] (e)
(c) edge[-latex] (f)
(b) edge[-latex] (d)
(f) edge[-latex] (d)
(h) edge[-latex] (f)
;

\end{tikzpicture}
\end{center}
\captionof{figure}{Directed acyclic graph}
\label{fig:relations}
#+end_latex



#+begin_definition
In a graph, the /neighbors/ of a node are those directly connected
to it.
#+end_definition

We can use figure \ref{fig:relations} to reflect on these definitions. The parents
of $B$ are $\{A,C\}$ and its children are $\{E,D\}$. Taking this into account, its neighbors
are $ne(B) = \{A,C,E,D\}$ and its Markov blanket is $\{A,B,C,D,E,F\}$.

#+begin_definition
A /graphical model/ is a probabilistic model for which a graph expresses the
conditional dependence structure between random variables.
#+end_definition

Commonly, they provide a graph-based representation for encoding a multi-dimensional
distribution representing a set of independences that hold in the specific
distribution. The most commonly used are /Bayesian networks/ and /Markov random
fields/, which differ in the set of independences they can encode and the
factorization of the distribution that they include.

* Graphical Model Test with Tikz

This is a test of making a graphical model in latex using Tikz package.

\begin{tikzpicture}[
  node distance=1cm and 0cm,
  mynode/.style={draw,ellipse,text width=2cm,align=center}
]

\node[mynode] (sp) {Sprinkler};
\node[mynode,below right=of sp] (gw) {Grass wet};
\node[mynode,above right=of gw] (ra) {Rain};
\path (ra) edge[-latex] (sp)
(sp) edge[-latex] (gw)
(gw) edge[latex-] (ra);

\node[left=0.5cm of sp] {
  \begin{tabular}{M{2}M{2}}
  \toprule
  & \multicolumn{2}{c}{Sprinkler} \\
  Rain & \multicolumn{1}{c}{T} & \multicolumn{1}{c}{F} \\
  \cmidrule(r){1-1}\cmidrule(l){2-3}
  F & 0.4 & 0.6 \\
  T & 0.01 & 0.99 \\
  \bottomrule
  \end{tabular}
};

\node[right=0.5cm of ra] {
  \begin{tabular}{M{1}M{1}}
  \toprule
  \multicolumn{2}{c}{Sprinkler} \\
  \multicolumn{1}{c}{T} & \multicolumn{1}{c}{F} \\
  \cmidrule{1-2}
  0.2 & 0.8 \\
  \bottomrule
  \end{tabular}
};

\node[below=0.5cm of gw] {
  \begin{tabular}{M{2}M{2}}
  \toprule
  & & \multicolumn{2}{c}{Grass wet} \\
  \multicolumn{2}{l}{Sprinkler rain} & \multicolumn{1}{c}{T} & \multicolumn{1}{c}{F} \\
  \cmidrule(r){1-2}\cmidrule(l){3-4}
  F & F & 0.4 & 0.6 \\
  F & T & 0.01 & 0.99 \\
  T & F & 0.01 & 0.99 \\
  T & T & 0.01 & 0.99 \\
  \bottomrule
  \end{tabular}
};

\end{tikzpicture}


\clearpage
Cites so the references appear (testing) \cite{koller_friedman}
\cite{barber}
\cite{wainwright}
#+BIBLIOGRAPHY: refs plain
