
"However, Monte Carlo methods are computationally very intensive, and also suffer from difficulties in diagnosing convergence"

"Expectation propagation is limited to certain classes of model for which the required expectations can be evaluated, is also not guaranteed to converge in general, and is prone to finding poor solutions in the case of multi-modal distributions."

"VMP allows variational inference to be applied automatically to a large class of Bayesian networks, without the need to derive application-specific update equations"

In this section, we review the \emph{Variational Message Algorithm} or \emph{VMA} as a variational Bayes application to Bayesian networks where the exponential family is considered using a message passing procedure between the nodes of a graphical model. 

The full set of variables is \( \X = X_1\dots,X_N \), where we are considering both hidden variables \( \mathcal{H} = \{H_1,\dots,H_J\} \) and visible ones \( \V \). A variational distribution \( Q \) in the mean-field family
\[
   Q(\mathcal{H}) = \prod_{j=1}^J Q(h_j).
\] 
The optimized factor is given by 
\[
   \log Q^{new}(h_n) = \E{Q_{\backslash j}}{\log P(\V, \mathcal{H})} + \text{const.}
\]
Using the bayesian network structure, the update is given by 
\[
  \log Q^{new}(h_j) = \E{Q_{\backslash j}}{ \sum_{n=1}^N \log P(x_n \mid pa(x_n))} + \text{const.}
\]
Any term that does not depend on \( H_j \) will be constant under its expectation, these leaves the terms \( P(h_j \mid pa(h_j)) \) and the conditionals of all its children:
\[
   \log Q^{new}(h_j) = \E{Q_{\backslash j}}{\log P(h_j \mid pa(h_j))} + \sum_{X_k \in ch(H_j)} \E{Q_{\backslash j}}{\log P(x_k \mid pa(x_k))} + \text{const.}
\]

This shows how the update of a hidden variable only depends on its Markov blanket. The optimization of \( Q_j \) is therefore computed as the sum of a term involving \( H_j \) and its parent nodes, along with a term for each children. This terms can be interpreted as "messages" from the corresponding nodes.

The exact form of the messages will depend on the functional form of the conditional distributions in the model. Important simplifications to the variational update equations occur when the conditional distribution of a node given its parents is in the exponential family. 

The following notation \( pa_X = pa(X) \) will be used to give clearance to the notation.

Consider a variable \( X \) and \( Y \in pa_X \), such as \( Y \) is a hidden variable. Then,
\[
     P(y \mid pa_y) = h_Y(pa_Y)\exp \Big( \bm{\eta}_Y(pa_y)^T\bm{T}_Y(y) - \psi_Y(pa_Y) \Big).
\] 
\[
     P(x \mid pa(x)) = h_X(pa_X)\exp \Big( \bm{\eta}_X(pa_x)^T\bm{T}_X(x) - \psi_X(pa_X) \Big).
\] 

The distribution \( P(Y \mid pa_Y \) can be though as a prior over \( Y \), and \( P(X \mid pa_X) \) as a (contribution to) likelihood of \( Y \).  

Conjugacy requires that these two conditionals have the same functional form with respect to \( Y \), so the latter has to be rewritten in terms of \( \bm{T}_Y (y)\) by defining functions \( \bm{\eta}_{XY} \) and \( \lambda \) as
\[
     \log P(x \mid y , cp_Y) = \bm{\eta}_{XY}(x, cp_Y)^T \bm{T}_Y(y) + \lambda(x, cp_Y),
\]  
where \( cp_Y = pa_X - \{Y\} \).

\begin{exampleth}
    If \( X \) is Gaussian distributed with mean \( Y \) and variance \( \beta \), the log conditional is 
    \[
         \log P(x \mid y, \beta) = 
         \begin{pmatrix}
             \beta y\\
             -\beta/2
         \end{pmatrix}^T
         \begin{pmatrix}
             x\\
             x^2
         \end{pmatrix}
         + \frac{1}{2}\big( \log \beta - \beta Y^2 - \log 2\pi \big).
    \] 
    We may rewrite it the conditional as
    \[
         \log  P(x \mid y, \beta) = 
         \begin{pmatrix}
             \beta x\\
             -\beta/2
         \end{pmatrix}^T
         \begin{pmatrix}
             y\\
             y^2
         \end{pmatrix}
         + \frac{1}{2}\big( \log \beta - \beta x^2 - \log 2\pi \big),
    \]
    where
    \[
         \bm{\eta}_XY(x,\beta) =  \begin{pmatrix}
            \beta x\\
            -\beta/2
        \end{pmatrix}^T,\quad T_Y(y)=  \begin{pmatrix}
            y\\
            y^2
        \end{pmatrix}.
    \]
\end{exampleth}

From this results, it can be seen that \( \log P(x \mid y , cp_Y) \) is linear in \( \bm{T}_X(x) \) and \( \bm{T}_Y(y) \), and, by the same reasoning, linear in any sufficient statistic of any parent of \( X \). This is a general result for any variable \( X \) in this kind of models: \emph{For any variable \( X \) in a conjugate-exponential model, the log conditional under its parents must be multi-linear of the statistics of \( X \) and its parents}.

Returning to the variational update for a node \( Y \):
\[
     \log Q^{new}(y) = \E{}{log P(y \mid pa_Y)} = \sum_{X_k \in ch(Y)}\E{}{\log P(x_k \mid pa_{X_k}))} + \text{const.} \ ,
\]
where the expectations are over the variational distribution of all other hidden variables. All the expectations in the right hand side can be calculated in terms of \( \bm{T}_Y(y) \):
\[
    \begin{aligned}
     \log Q^{new}(y) &= \E{}{\log(h_Y (pa_y)) + \bm{\eta}_Y(pa_y)^T \bm{T}_Y(y) + \psi_Y(pa_y)}\\ 
     &+ \sum_{X_k \in ch(Y)} \E{}{ \bm{\eta}_{X_k, Y}(x, cp(x_k))^T \bm{T}_Y(y) + \lambda_{k}(x_k, cp(x_k)) } + \text{const.}\\
     &= \Bigg[ \E{}{\bm{\eta}_Y(pa_Y)^T} + \sum_{X_k \in ch(Y)} \E{}{ \bm{\eta}_{X_k, Y}(x, cp(x_k))^T \bm{T}_Y(y)}  \Bigg]^T \bm{T}_Y(y) \\
     & + \log h(pa_y)+ \text{const.}
    \end{aligned}   
\]

It follows that \( Q^{new}(Y) \) is in the exponential family of the same form as \( P(y \mid pa_y) \) but with parameter function
\[
     \eta^{new}_Y = \E{}{\bm{\eta}_Y(pa_y)} + \sum_{X_k \in Ch(Y)}\E{}{\bm{\eta}_{x_k, Y}(X_k, cp(x_k))}.
\] 

As the expectations of \( \bm{\eta}_Y \) and \( \bm{\eta}_{X_k, Y} \) are multilinear functions of the expectations of the statistic functions of their corresponding variables, it is possible to reparameterize these functions in terms of these expectations
\[
     \begin{aligned}
     \bar{\bm{\eta}}_Y (\{ \mathbb{E}[{\bm{T}_{X_k}}(x_k)] \}_{X_k \in pa_Y}) &= \E{}{\bm{\eta}_Y(pa_Y)}\\
     \bar{\bm{\eta}}_{X_k,Y} (  \mathbb{E}[\bm{T}_{X_k}(x_k)], \{ \mathbb{E}[{\bm{T}_{X_j}}(x_j)] \}_{X_j \in cp_{X_k}}) &= \E{}{\bm{\eta}_{X_k, cp_{X_k}}(x_k, cp_{x_k})}\\
     \end{aligned}
\] 

\section{Variational Message Passing Algorithm}

The message from a parent node \( Y \) to a child node \( X \) is the expectation under \( Q \) of its statistic vector
\[
      \bm{m}_{Y \to X} = \E{}{\bm{T}_Y(y)}.
\]
The message from a child node \( X \) to a parent node \( Y \) is:
\[
      \bm{m}_{X \to Y} = \bar{\bm{\eta}_{X,Y}}(\E{}{\bm{T}_X(x)}, \{\bm{m}_{X_k \to X}\}_{X_k\in cp_Y})
\]
which relied on having received all messages from all the co-parents. If a node \( X \)  is observed, the messages defined above are defined as \( \bm{T}_A(a) \) instead of \( \E{}{\bm{T}_A(a)} \).

\begin{exampleth}
     If \( X \) is Gaussian distributed and \( Y, \beta \) are its parents, the messages are:
     \[
           \bm{m}_{X \to Y} = \begin{pmatrix}
                \E{}{\beta}\E{}{X}\\
                - \E{}{\beta}/2
           \end{pmatrix}, \quad
           \bm{m}_{X \to \beta} = \begin{pmatrix}
               -\frac{1}{2}(\Bigg( \E{}{x^2} - 2\E{}{x}\E{}{y} + \E{}{y^2} \Bigg))\\
               \frac{1}{2}
          \end{pmatrix}.
     \]
     And the messages from \( X \) to any of its child nodes is 
     \[
          \begin{pmatrix}
               \E{}{\beta}\E{}{x}\\
               - \E{}{x^2}
          \end{pmatrix}.
     \]
\end{exampleth}

When a node \( Y \) has received all messages from its parents and children, we can compute the updated parameter \( \bar{\eta}^{new}_Y \) as
\[
     \bar{\eta}^{new}_Y = \bar{\bm{\eta}}_Y(\{ \bm{m}_{X_k \to Y} \}_{X_k \in pa_Y}) + \sum_{X_k \in ch_Y}\bm{m}_{X_k \to Y}.
\] 

\cite{winn2005variational}