
Another method to deal with hidden variables is using \emph{Variational Bayes (VB)}, in contrast with the EM algorithm, this one uses a distribution that better represents the posterior.

Consider a simple datapoint \(x = (v,h)\), in this situation we focus our interest on the posterior distribution:
\[
  P(\theta \mid v) = \frac{P(v \mid \theta)P(\theta)}{P(v)} = \frac{1}{P(v)}\int_{h}P(v,h \mid \theta)P(\theta).
\]

Variational Bayes assumes the joint hidden and parameter posterior can be approximated as the product of two distributions, one over the hidden variable and one over the parameter such that
\[
  P(h ,\theta \mid v) \approx Q(h)Q(\theta).\footnote{We use the same letter for both distributions, as they can be differenced from the context.}
\]

\begin{remark}
  By doing this assumption, we are also assuming that the posterior can be approximated with \(Q(\theta)\)
  \[
    P(\theta \mid v) = \int_{h} P(\theta, h \mid v) \approx \int_{h} Q(h)Q(\theta) = Q(\theta).
  \]
  For this reason, the main goal of the algorithm is to make this approximation as tightest as possible with an iterative method that calculates both \(Q(\theta)\) and \(Q(h)\) distributions.
\end{remark}

To achieve that, we minimize the Kullback-Leibler divergence between them.
\[
  \KL{Q(h)Q(\theta)}{P(h, \theta \mid v)} = \E{Q(h)}{\log{Q(h)}} + \E{Q(\theta)}{\log{Q(\theta)}} - \E{Q(h)Q(\theta)}{\log{P(h,\theta \mid v)}}.
\]

We use that \(\log{P(v)}\) is independent from \(Q(h)Q(\theta)\) to get the desired inequality.
\[
  \begin{aligned}
    0 &\leq \E{Q(h)}{\log{Q(h)}} + \E{Q(\theta)}{\log{Q(\theta)}} - \E{Q(h)Q(\theta)}{\log{P(h,\theta \mid v)}}\\
    &= \E{Q(h)}{\log{Q(h)}} + \E{Q(\theta)}{\log{Q(\theta)}} - \E{Q(h)Q(\theta)}{\log{\frac{P(h,\theta,v)}{P(v)}}} \\
    &=\E{Q(h)}{\log{Q(h)}} + \E{Q(\theta)}{\log{Q(\theta)}} - \E{Q(h)Q(\theta)}{\log{P(h,\theta, v)}} + \E{Q(h)Q(\theta)}{\log{P(v)}} \\
    &= \E{Q(h)}{\log{Q(h)}} + \E{Q(\theta)}{\log{Q(\theta)}} - \E{Q(h)Q(\theta)}{\log{P(h,\theta, v)}} + \log{P(v)}. \\
  \end{aligned}
\]
We got the following lower bound
\[
  \log{P(v)} \geq -\E{Q(h)}{\log{Q(h)}}- \E{Q(\theta)}{\log{Q(\theta)}} + \E{Q(h)Q(\theta)}{\log{P(h,\theta, v)}}.
\]
Therefore, minimizing the Kullback-Leibler divergence is equivalent to find the tightest lower bound.

The procedure is then split in two steps to keep the structure of the EM algorithm.

\begin{itemize}
  \item \textbf{E-step}. Given a fixed \(Q(\theta)\), minimize the Kullback-Leibler divergence.
    \[
    Q^{new}(h) = \argmin_{Q(h)}\KL{Q(h)Q(\theta)}{P(h,\theta \mid v)}.
    \]

  \item \textbf{M-step}. Given a fixed \(Q(h)\), minimize the Kullback-Leibler divergence.
    \[
    Q^{new}(\theta) = \argmin_{Q(\theta)}\KL{Q(h)Q(\theta)}{P(h,\theta \mid v)}.
    \]
\end{itemize}

For fixed \( Q(\theta) \), the contribution from \( Q(h) \) to the Kullback-Leibler divergence is, as we derived in the CAVI algorithm, 
\[
  \begin{aligned}
  \KL{Q(h)Q(\theta)}{P(h,\theta \mid v)} &= \E{Q(h)}{\log Q(h)} - \E{Q(h)Q(\theta)}{\log P(v,h,\theta)} + \text{const.} \\
  &= - \KL{Q(h)}{\exp \E{Q(\theta)}{P(h, v, \theta)}} + \text{const.}
  \end{aligned}
\]
Therefore, the update is proportional to 
\[
   Q^{new}(h) \propto \exp \E{Q(\theta)}{\log P(v,h,\theta)} \propto \exp \E{Q(\theta)}{\log P(v,h \mid \theta)}.
\]
Following a similar argument, 
\[
  Q^{new}(\theta) \propto \exp \E{Q(h)}{\log P(v,h,\theta)} \propto \exp \E{Q(h)}{\log P(v,h \mid \theta)}.
\]

As in the case of the EM algorithm, each iterations guarantees an increase in the lower bound of the marginal likelihood, but increasing the marginal likelihood itself is not guaranteed.

When using an i.i.d dataset \(\V, \mathcal{H}\), we may assume that \(Q(\mathcal{H})\) is in the mean-field family:
\[
  Q(h_{1}, \dots, h_{N}) = \prod_{n=1}^{N}Q(h_{n}).
\]

The lower bound to the marginal likelihood is then written as a summation of the bounds on each datapoint.
\[
  \log{P(\V)} \geq \mathlarger{\sum_{n}} -\E{Q(h_{n})}{\log{Q(h_{n})}}- \E{Q(\theta)}{\log{Q(\theta)}} + \E{Q(h_{n})Q(\theta)}{\log{P(v_{n}, h_{n},\theta)}}
\]

\subsection{VB is a generalization of the EM algorithm}

We start considering a distribution over the parameter that summarizes the information in the optimal point, let \(\theta_{opt}\) be the optimal value of \(\theta\).
\[
  Q(\theta) = \delta(\theta - \theta_{opt})
\]
The lower bound takes the form
\[
  \log{P(v)} \geq - \E{Q(h)}{\log{Q(h)}} + \E{Q(h)}{\log{P(h, v, \theta_{opt})}} + \text{ const. }
\]
The M-step is then picking the optimal parameter \(\theta_{opt}\) given a fixed \(Q(h)\):
\[
  \begin{aligned}
    \theta_{opt} &= \argmax_{\theta} \Big( \E{Q(h)}{\log{P(v,h,\theta)}} \Big)\\
    &=  \argmax_{\theta} \Big( \E{Q(h)}{\log{P(v, h \mid \theta)P(\theta)}} \Big) \\
    &= \argmax_{\theta} \Big( \E{Q(h)}{\log{P(v,h \mid\theta)}} + \log{P(\theta)} \Big) \\
  \end{aligned}
\]

If we take a flat prior (\(P(\theta) \)  constant), this term is equivalent to the energy one in the EM bound \(\E{Q(h\mid v)}{\log{P(h,v \mid \theta)}}\).

The VB E-step consists on minimizing \(\KL{Q(h)}{P(h,\theta_{opt} \mid v)}\) over \(Q(h)\), as
\[
  \begin{aligned}
    \KL{Q(h)}{P(h\mid \theta_{opt}, v)} &= \E{Q(h)}{Q(h)} - \E{Q(h)}{P(h \mid \theta_{opt}, v)} \\
    &= \E{Q(h)}{Q(h)} - \E{Q(h)}{\frac{P(h, \theta_{opt} \mid v)}{P(\theta_{opt})}}  \\
    &= \E{Q(h)}{Q(h)} - \frac{1}{P(\theta_{opt})}\E{Q(h)}{P(h, \theta_{opt} \mid v)}
\end{aligned}
\]
where \(P(\theta_{opt})\) is a constant and
\[
\KL{Q(h)}{P(h, \theta_{opt} \mid v)} = \E{Q(h)}{Q(h)} - \E{Q(h)}{P(h, \theta_{opt} \mid v)}
\]
Then, as  he E-step of the EM algorithm consisted on minimizing the Kullback-Leibler divergence \(\KL{Q(h \mid v)}{P(h \mid v, \theta)}\), both steps are equivalent.

\begin{remark}
  Expectation Maximization algorithm is a special case of Variational Bayes with a flat prior and a delta function as the parameter posterior approximation.
\end{remark}

\section{Comparison}
Variational Bayes is often compared with expectation maximization as their numerical procedure is quite similar. Their main differences are the following:

EM computes point estimates of posterior distribution of those random variables that can be categorized as "parameters", but only estimates the posterior distribution of the latent variables. On the other hand, VB computes a posterior distribution for all the variables.

For example, in a gaussian mixture, EM would directly estimate optimum values for the mean and variance of each gaussian and VB would fit a distribution to these parameters.
