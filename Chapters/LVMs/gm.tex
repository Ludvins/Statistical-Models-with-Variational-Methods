
A Gaussian mixture model (figure~\ref{fig:gaussian_mixture}) is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. We will discuss another mixture example with the EM algorithm in Section~\ref{sec:mixture_em}.

\pgfmathdeclarefunction{gauss}{2}{%
  \pgfmathparse{1/(#2*sqrt(2*pi))*exp(-((x-#1)^2)/(2*#2^2))}%
}
\begin{figure}[h!]
  \centering
  \begin{tikzpicture}
    \begin{axis}[every axis plot post/.append style={
        mark=none,domain=-2:5,samples=50,smooth}, % All plots: from -2:2, 50 samples, smooth, no marks
      axis x line*=bottom, % no box around the plot, only x and y axis
      axis y line*=left, % the * suppresses the arrow tips
      enlargelimits=upper] % extend the axes a bit to the right and top
      \addplot {gauss(0,0.5)};
      \addplot {gauss(2,0.75)};
      \addplot {gauss(3, 0.4)};
    \end{axis}
  \end{tikzpicture}
  \caption{One-dimensional Gaussian mixture with 3 clusters.}
\end{figure}


The following elements are being considered (using~\cite{bishop2006pattern} notation):
\begin{itemize}\setlength\itemsep{1em}
  \item \(K\) mixture components and \(N\) observations.
  \item A set of i.i.d real valued random variables \(\bX = \{X_{1},\dots, X_{N}\}\) and a corresponding set of observations \(\bx = \{x_{1},\dots, x_{n}\}\).
  \item The cluster assignment latent variables \(\bZ = \{Z_{1}, \dots, Z_{N}\}\), where each \(z_{n}\) indicates the cluster to which \( x_n \) belongs. 
  \item We choose a Dirichlet distribution over the mixing coefficients \(\bm{\pi}\)
    \[
    \bm{\pi} \sim \text{Symmetric-Dirichlet}(\alpha_{0}) \implies P(\bm{\pi}) \propto \prod_{k=1}^{K}\pi_{k}^{\alpha_{0}-1}
    \]
    The hyper-parameter \(\alpha_{0}\) is the effective prior of each mixture component. Then \(\bm{\pi} = \{\pi_{1},\dots,\pi_{K}\}\) are the mixture weights, i.e, prior probability of a particular component \(k\).
    \[
    P(\bz\mid \bm{\pi}) = \prod_{n=1}^{N}\prod_{k=1}^{K}\pi_{k}^{\mathbb{I}[z_{n}=k]} \implies (Z_{n} \mid \bm{\pi}) \sim \text{Categorical}(\bm{\pi})
    \]
  \item \(\bm{\mu} = \{\mu_{1},\dots,\mu_{K}\}\) and \(\bm{\Lambda} = \{\Lambda_{1},\dots,\Lambda_{K}\}\) are the distribution parameters of each observation full conditional
    \[
    (\bm{X} \mid \bm{Z}, \bm{\mu}, \bm{\Lambda}) \sim \prod_{n=1}^{N}\prod_{k=1}^{K}\mathcal{N}(\mu_{k} \mid \Lambda_{k})^{\mathbb{I}[z_{n}=k]}
    \]

  \item The prior governing \(\bm{\mu}\) and \(\bm{\Lambda}\) is an independent Gaussian, Inverse-Gamma distribution with hyper parameters \(m_{0}, \beta_{0}, w_{0}\) and \(v_{0}\):
    \[
    P(\bm{\mu}, \bm{\Lambda}) = P(\bm{\mu} \mid \bm{\Lambda})P(\bm{\Lambda}) \text{ where }
    \begin{cases}
      (\bm{\mu}\mid\bm{\Lambda}) &\sim \displaystyle\prod_{k=1}^{K}\mathcal{N}(m_{0}, \beta_{0}\Lambda_{k})\\
      \bm{\Lambda} &\sim \displaystyle\prod_{k=1}^{K}\text{Inverse-Gamma}(w_{0}, v_{0})
      \end{cases}
    \]
\end{itemize}

The joint probability factorizes as
\[
  P(\bm{x}, \bm{z}, \bm{\pi}, \bm{\mu}, \bm{\Lambda}) = P(\bm{x}\mid \bm{z}, \bm{\mu}, \bm{\Lambda})P(\bm{z}\mid \bm{\pi})P(\bm{\pi})P(\bm{\mu}\mid \bm{\Lambda})P(\bm{\Lambda}).
\]

\cite{bishop2006pattern} gives the explicit update for the CAVI algorithm.

\begin{figure}[h!]
  \centering
  \begin{tikzpicture}[
    node distance=0.8cm and 1.5cm,
    mynode/.style={draw,circle,text width=0.5cm,align=center},
    param/.style={draw,text width=0.5cm,align=center}
    ]

    \node[mynode] (mu) {\(\bm{\mu}\)};
    \node[mynode, right=of mu] (sigma) {\(\bm{\Lambda}\) };


    \node[mynode, below =of sigma] (z) {\(Z_{n}\)};
    \node[mynode, left =of z] (phi) {\(\bm{\pi}\)};
    \node[mynode, right =of z] (x) {\(X_{n}\)};

    \node[param, above=of mu] (lambda) {\(m_{0}\)};
    \node[param, left=of lambda] (mu0) {\(\beta_{0}\)};
    \node[param, above=of sigma] (v) {\(v_{0}\)};
    \node[param, right=of v] (sigma0) {\(w_{0}\)};
    \node[param, left=of phi] (beta) {\(\alpha_{0}\)};

    \plate{} {(z)(x)} {\(n=1,\dots,N\)}; %

    \path (mu) edge[-latex] (x)
    (sigma) edge[-latex] (x)
    (sigma) edge[-latex] (mu)
    (z) edge[-latex] (x)
    (beta) edge[-latex] (phi)
    (phi) edge[-latex] (z)
    (mu0) edge[-latex] (mu)
    (lambda) edge[-latex] (mu)
    (v) edge[-latex] (sigma)
    (sigma0) edge[-latex] (sigma)
    ;

  \end{tikzpicture}
  \caption{Gaussian mixture model. Squares represent hyper-parameters.}
  \label{fig:gaussian_mixture}
\end{figure}
