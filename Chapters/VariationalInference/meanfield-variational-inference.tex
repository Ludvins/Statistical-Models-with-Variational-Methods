
\section{The mean-field variational family}

The \emph{mean-field variational family} \(\mathcal{Q}\) is defined as the family of distributions where the variables are mutually independent, i.e, any \(Q \in \mathcal{Q}\) verifies
\[
  Q(\bz) = \prod_{n=1}^{N}Q_{n}(z_{n}),
\]

where \(\bm{z} = \{z_{1},\dots,z_{N}\}\) is the considered set of variables. The mean-field family is commonly used to model the family of distributions over the latent variables in our optimization problem. Notice that each \emph{factor} \(Q_{n}\) can be different and this family does not depend on the observed data.

The mean-field family can capture any marginal of the latent variables but not correlation between them, as it assumes they are independent. For example, consider a two dimensional Gaussian distribution where a high percentage of the density is inside the blue ellipse shown in the following figure. Any mean-field approximation would factorize as a product of two Gaussian distributions, condensing its density in a circle as shown in purple.

\begin{figure}[h!]
\centering
\begin{tikzpicture}
  \pgfplotsset{ticks=none}
    \begin{axis}[
        axis lines = middle,
        xmin=-11, xmax=11, ymin=-11, ymax=11,
        axis equal,
        xlabel = $z_{1}$,
        ylabel = {$z_{2}$},
        yticklabels={,,},
        ]
        \filldraw[rotate around={45:(110,110)}, color=blue!50, fill=blue!20, very thick, opacity=0.7] (110,110) ellipse (70 and 20);
        % \addlegendentry{Exact Posterior}
        \filldraw[color=purple!50, fill=purple!20, very thick, opacity=0.7]  (110,110) circle (25);
        % \addlegendentry{Mean-field Approximation}
    \end{axis}
  \end{tikzpicture}
  \caption{Mean-field family distribution (purple) approximating a Gaussian distribution (blue)}
\end{figure}

Notice the parametric form of each factor \(Q_{n}\) is not specified and the appropriate configuration depends on the variable. For example, a continuous variable might have a Gaussian factor and a categorical variable have a categorical factor.

\section{CAVI Algorithm }

In this section, we describe a widely used algorithm to solve the optimization problem we discussed in the previous section using the mean-field family. It is \emph{coordinate ascent variational inference} or \emph{CAVI} and its procedure is to iteratively optimize each factor of the mean-field family distribution, while fixing the others. With this, the ELBO reaches a local optimum.

 \begin{algorithm}[t]
  \SetAlgoLined
  \KwData{A distribution \(P(\V , \bm{z})\) with a dataset \(\V\)}
  \KwResult{A distribution of the mean-field family \(Q(\bm{z}) = \prod_{n=1}^{N}Q_{n}(z_{n})\)}
  Initialize \(Q(\bm{z})\)\;
  \While{Convergence stop criteria}{
    \For{\(n \in 1,\dots,N\)}{
      \(\bm{z}_{\backslash n} = (z_{1},\dots,z_{n-1},z_{n+1},\dots,z_{M})\)\;
      Set \(Q_{n}(z_{n}) \propto \exp{\E{Q_{\backslash n}}{\log{P(z_{n} \mid \bm{z}_{\backslash n}, \V)}}}\)\;
    }
    Compute \(ELBO(Q)\)\tcp*{Used for convergence criteria.}
  }
  \KwRet{\(Q\)}\;
  \caption{Coordinate Ascent Variational Inference}
  \label{alg:cavi}
\end{algorithm}

Consider the set \(\bz = \{z_1,\dots,z_N\}\) comes from a set of i.i.d variables \(\bZ = \{Z_1,\dots,Z_N\}\), and fix the \(n^{th}\) variable \(Z_{n}\), let \(\backslash n\) denote the full set of indexes without the \(n^{th}\), then \(\bm{Z}_{\backslash n}\) is the full set of variables without the focused one. Let the factors \(Q_{m}, m\neq n\) be fixed. Under this assumptions, the ELBO has the form
\[
  \begin{aligned}
    ELBO(Q) &= \E{Q}{\log{P(\bx, \bz)}} - \E{Q}{\log{Q(\bz)}}\\
    &\stackrel{1}{=} \E{Q_{n}}{\E{Q_{\backslash n}}{\log{P(\bx, \bm{z})}}} - \E{Q_{n}}{\log{Q_{n}(z_{n})}} + \text{ const. }\\
    &\stackrel{2}{=} \E{Q_{n}}{\E{Q_{\backslash n}}{\log{P(z_{n} \mid \bm{z}_{\backslash n}, \bx)} + \log{P(\bm{z}_{\backslash n}, \bx)}}} - \E{Q_{n}}{\log{Q_{n}(z_{n})}} + \text{ const. }\\
    &\stackrel{3}{=}  \E{Q_{n}}{\E{Q_{\backslash n}}{\log{P(z_{n} \mid \bm{z}_{\backslash n}, \bx)}}} - \E{Q_{n}}{\log{Q_{n}(z_{n})}} + \text{ const. }\\
    &\stackrel{4}{=} - \KL{Q_{n}(z_{n})}{  \exp{\E{Q_{\backslash n}}{\log{P(z_{n} \mid \bm{z}_{\backslash n}, \bx)}}} } + \text{ const.}.
  \end{aligned}
\]

\begin{enumerate}
  \item The expectations in the ELBO formula are separated. The logarithm factorizes as \(\log Q(z) = \sum_{n=1}^N \log Q_n(z_n)\).  The constant term comes from \( \E{Q_{\backslash n}}{\log Q_{\backslash n}(z_{\backslash n})} \).
  \item \( P \) is separated as \( P(\bz, \bx) = P(z_n \mid \bz_{\backslash n}, \bx)P(\bz_{\backslash n}, \bx) \implies \log P(\bz, \bx) = \log P(z_n \mid \bz_{\backslash n}, \bx) + \log P(\bz_{\backslash n}, \bx)\).
  \item \( \E{Q_{n}}{\E{Q_{\backslash n}}{\log{P(\bm{z}_{\backslash n}, \bx)}}} = \E{Q_{\backslash n}}{\log{P(\bm{z}_{\backslash n}, \bx)}}\) is constant.
  \item Applied Kullback-Leibler definition.
\end{enumerate}

Minimizing the ELBO is equivalent to maximize the given Kullback-Leibler divergence, which corresponds to updating \(Q_{n}^{new}\) as:
\[
  Q_{n}^{new}(z_{n}) \propto \exp{\E{Q_{\backslash n}}{\log{P(z_{n} \mid \bm{z}_{\backslash n}, \V)}}}.
\]

Notice that the proportionality restriction is enough to fully determine the distribution as its integral is normalized. Equivalently, the distribution is proportional to
\[
    Q_{n}^{new}(z_{n}) \propto \exp{\E{Q_{\backslash n}}{\log{P(z_{n}, \bm{z}_{\backslash n}, \V)}}}.
\]

As the ELBO is generally a non-convex function and the CAVI algorithm converges to a local optimum, the initialization values of the algorithm play an important role on its performance.
The convergence criteria is usually a threshold for the ELBO.
