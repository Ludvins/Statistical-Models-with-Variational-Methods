
The exponential family is a parametric set of probability distributions of a certain form. This form is chosen based on some useful algebraic properties and generality (figure~\ref{tab:ef}).

Let \(X\) be a random variable and \(\bm{\theta}\) a set of parameters. A family a distributions is said to belong the exponential family if its probability distribution has the form
\[
  P(x \mid \bm{\theta}) = h(x)\exp{\Big( \sum_{i=1}^{S} \eta_{i}(\bm{\theta})T_{i}(x) - \psi(\bm{\theta}) \Big)},
\]
where \(h(x)\), \(T_{i}(x)\), \(\eta_{i}(\bm{\theta})\) and \(\psi(\bm{\theta})\)  are known functions such that \(h\) is called a \emph{base measure}, \(\eta_{i}(\bm{\theta})\) are called the \emph{distribution parameters},  \(T_{i}(x)\) the \emph{test statistics} and \(\psi\) is the \emph{log normalizer} as it ensures logarithmic normalization due to
\[
  \begin{aligned}
    1 &= \int_{x}  h(x)\exp{\Big( \sum_{i=1}^{S} \eta_{i}(\bm{\theta})T_{i}(x) - \psi(\bm{\theta}) \Big)}\\
    &= \int_{x} e^{-\psi(\btheta)} h(x)\exp{\Big( \sum_{i=1}^{S} \eta_{i}(\bm{\theta})T_{i}(x) \Big)}\\
    &= e^{-\psi(\btheta)} \int_{x} h(x)\exp{\Big( \sum_{i=1}^{S} \eta_{i}(\bm{\theta})T_{i}(x) \Big)},
  \end{aligned}
\]
so \(\psi\) verifies
\[
      \psi(\bm{\theta}) = \log \int_{x} h(x) \exp \Big( \sum_{i=1}^{S} \eta_{i}(\bm{\theta})T_{i}(x) \Big).
\]

Naming \(\bm{\eta}\) and \(\bm{T}\) the corresponding vector functions, the parameters can always be transformed as \(\bm{\theta}^{new} = \bm{\eta}(\bm{\theta})\), in which case we say the distribution into its \emph{canonical form} (notice \(\psi\) has changed but we are not differentiating it from the previous one since it is fully determined by the other functions):
\[
  P(x \mid \bm{\theta}) = h(x) e^{\bm{\theta}^{T}\bm{T}(x) - \psi(\bm{\theta})}.
\]
In this form, it is easier to see that \(\bm{T}\) is the sufficient statistic for \(\btheta\). This is a consequence of \emph{Fisherâ€“Neyman factorization theorem} which says that \textit{\(\bm{T}\) is sufficient for \(\btheta\)  if and only if the probability distribution \(P\)  can be factored into a product such that one factor, \(h\), does not depend on \(\btheta\)  and the other factor, which does depend on \(\btheta\), depends on \(x\)  only through \(\bm{T}\).}

\begin{figure}
  \centering
  \begin{tabular}{ |c|c|c|c|c| }
    \hline
    Distribution & Parameter set \(\btheta\) & Base measure \(h\) & Parameter function \(\bm{\eta}\) & Statistics \(\bm{T}\) \\
    \hline
    Bernoulli & \(p\)  & \(1\)  & \(\log{\frac{p}{1 - p}}\) & \(x\) \\
    Binomial & \(p\)  & \(\binom{n}{x}\) & \(\log{\frac{p}{1 - p}}\) & \(x\)  \\[10pt]
    Categorical & \(p_{1},\dots,p_{K}\) & \(1\) &
                                                  \(\begin{pmatrix}
                                                    \log p_{1}\\
                                                    \vdots\\
                                                    \log p_{K}
                                                  \end{pmatrix}\)
                 &
                                                  \(\begin{pmatrix}
                                                    [x=1]\\
                                                    \vdots\\
                                                    [x=k]
                                                  \end{pmatrix}\)\\[30pt]
    Gaussian & \(\mu, \sigma^{2}\) & \(\frac{1}{\sqrt{2\pi}}\) &
                                     \(\begin{pmatrix}
                                       \frac{\mu}{\sigma^2}\\
                                       -\frac{1}{2\sigma^{2}}
                                     \end{pmatrix}\)
                 &
                   \(\begin{pmatrix}
                     x\\
                     x^{2}
                   \end{pmatrix}\) \\[30pt]
    Beta & \(\alpha, \beta\) & \(\frac{1}{x(1-x)}\)
                                                               &
                                                                 \(\begin{pmatrix}
                                                                   \alpha \\
                                                                   \beta
                                                                 \end{pmatrix}\)
                 &
                   \(\begin{pmatrix}
                     \log x \\
                     \log (1-x)
                   \end{pmatrix}\) \\[30pt]
    Dirichlet & \(\alpha_{1},\dots,\alpha_{K}\) & \(1\) &
                                                  \(\begin{pmatrix}
                                                    \alpha_{1} - 1\\
                                                    \vdots\\
                                                    \alpha_{K} - 1
                                                  \end{pmatrix}\)
                 &
                                                  \(\begin{pmatrix}
                                                    \log x_{1}\\
                                                    \vdots\\
                                                    \log x_{K}
                                                  \end{pmatrix}\)\\
    \hline
  \end{tabular}
  \caption{Some distributions in the exponential family}
  \label{tab:ef}
\end{figure}

An important property of the exponential family is that they have \emph{conjugate priors}, when the posterior distribution is in the same probability distribution family as
the prior distribution, they are called \emph{conjugate distributions}, and
the prior is called a \emph{conjugate prior} of the likelihood distribution.


\section{Latent Variable and Conditionally conjugate models}

One important case of exponential family are \emph{latent variable models} or \emph{LVMs}. In this models, the following assumptions are made:
\begin{enumerate}\itemsep0.5em
  \item There set of i.i.d random variables \(\{X_{1},\dots,X_{N}\}\) and a set of observations \(\bx = (x_{1}, \dots, x_{N})\).
  \item Both global \(\btheta\) and local parameters \(\bm{Z} = \{Z_{1}, \dots, Z_{n}\}\) govern the data (both seen as latent variables). We refer to \emph{global latent variables} when they affect the whole distribution and \emph{local latent variables} when they affect only to a subset, in this case each \(Z_{n}\) affects only \(x_{n}\). Given this, the joint probability is
    \[
    P(\bx, \btheta, \bz) = P(\btheta)\prod_{n=1}^{N}P(z_{n}, x_{n} \mid \btheta).
    \]
  \item The \(n^{th}\) observation \(x_{n}\) and the \(n^{th}\) local hidden variable \(z_{n}\) are conditionally independent, given the global variables \(\btheta\), of all other observations and local hidden variables,
    \[
    P(x_{n},z_{n} \mid \btheta, \bx_{\backslash n}, \bz_{\backslash n}) = P(x_{n}, z_{n} \mid \btheta).
    \]
\end{enumerate}

\begin{remark}
  These models are widely used to discover patterns in data sets (\cite{blei2014build}).
  LVMs include popular models like \emph{Latent Dirichlet Allocation} models used to uncover the hidden topics in text corpora (\cite{blei2003latent}), mixture of Gaussian models to discover hidden clusters in data (\cite{bishop2006pattern}) and probabilistic principal component analysis for dimensionality reduction (\cite{tipping1999probabilistic}).
\end{remark}

One important case of LVMs and exponential family models are \emph{conditionally conjugate models}, where the following assumptions are made.
\begin{enumerate}[itemsep=2ex]
  \item The prior for the global latent variable \(P(\btheta)\) is in the exponential family with an hyper-parameter \(\alpha = [\alpha_{1}, \alpha_{2}]\), where \(\alpha_{1}\) is a vector and \(\alpha_{2}\) is a scalar, and statistics that concatenate the global latent variable and its log normalizer \([\btheta, -\psi(\btheta)]\),
    \[
    P(\btheta) = h(\btheta) \exp \Big( \alpha^{T}[\btheta, -\psi(\btheta)] - \psi(\alpha)\Big).
    \]
  \item Each local term \(P(z_{n},x_{n} \mid \btheta)\) is in the exponential family of the form
    \[
    P(z_{n},x_{n} \mid \btheta) = h(z_{n}, x_{n}) \exp \Big( \btheta^{T} T(z_{n}, x_{n}) - \psi(\btheta) \Big).
    \]
  \item The complete conditional of a local latent variable verifies
    \[
    P(z_{n} \mid \btheta, \bx, \bz_{\backslash n}) = P(z_{n} \mid x_{n}, \btheta)
    \]
    and is also in the exponential family
    \[
    P(z_{n}\mid x_{n} , \btheta) =  h(z_{n})\exp \Big( {\eta(\btheta, x_{n})}^{T}T(z_{n}) - \psi(\btheta, x_{n}) \Big).
    \]

\end{enumerate}

As conjugacy holds, the posterior \(P(\btheta \mid \V, \bz)\) is in the same family with parameter
\[
  \bar{\alpha} = [\alpha_{1} + \sum_{n=1}^{N} T(z_{i}, x_{i}), \alpha_{2}+ N]^{T}.
\]
This can be demostrated as follows,
\[
  \begin{aligned}
    P(\btheta \mid \bx, \bz) &= \frac{P(\bx, \bz \mid \btheta) P(\btheta)}{P(\bx, \bz)} \propto  P(\bx, \bz \mid \btheta) P(\btheta) = P(\btheta) \prod_{n=1}^{N}h(x_{n}, z_{n})\exp \Big( \btheta^{T}T(z_{n}, x_{n})  - \psi(\btheta) \Big)\\
    &\propto  h(\btheta) \exp \Big( \alpha^{T}[\btheta, -\psi(\btheta)]\Big)  \prod_{n=1}^{N}h(x_{n}, z_{n})\exp \Big( \btheta^{T}T(z_{n}, x_{n})  - \psi(\btheta) \Big)\\
    &\propto h(\btheta) \exp \Big(   \alpha^{T}[\btheta, -\psi(\btheta)] + \sum_{n=1}^{N}  \btheta^{T}T(z_{n}, x_{n})  - \psi(\btheta)  \Big)\\
    &\propto h(\btheta) \exp \Big(   \alpha_{1}^{T}\btheta  - \alpha_{2}^{T}\psi(\btheta) - N\psi(\btheta) + \sum_{n=1}^{N}  T(z_{n}, x_{n})^{T}\btheta  \Big)\\
    &\propto h(\btheta) \exp \Big(   \big(\alpha_{1} + \sum_{n=1}^{N}  T(z_{n}, x_{n})\big)^{T} \btheta  - (\alpha_{2} + N)^{T}\psi(\btheta) \Big).
  \end{aligned}
\]

As the inverse reasoning can be done, if the complete conditional of the global hidden variable is in the exponential family with hyper-parameter \(\bar{\alpha}\), the prior is in the same family with the corresponding hyper-parameter \(\alpha\).

\section{CAVI in Conditionally conjugate models} \label{sec:cavi_ccm}

Set aside the conditionally conjugate models and consider the following situation where we fit a distribution \(Q(\bm{z}) = \prod_{n=1}^{N} Q_{n}(z_{n})\) in the mean-field family, using a exponential family distribution for the marginal \(P(z_{n} \mid \bm{z}_{\backslash n}, \bx)\):
\[
  P(z_{n} \mid \bm{z}_{\backslash n}, \bx) = h(z_{n})\exp \Big( {\eta_{n}(\bm{z}_{\backslash n}, \bx)}^{T}T(z_{n}) - \psi(\bm{z}_{\backslash n}, \bx) \Big).
\]

The update of the CAVI algorithm  is then given by
  \begin{align}
    Q(z_{n}) &\propto \exp{\E{Q(\bm{z}_{\backslash n})}{\log P(z_{n} \mid \bm{z}_{\backslash n}, \bx)}} \nonumber\\
    &= h(z_{n})\exp \Big(\E{Q_{\bm{z}_{\backslash n}}}{ {\eta_{n}(\bm{z}_{\backslash n}, \bx)}}^{T}T(z_{n}) - \E{Q_{\bm{z}_{\backslash n}}}{ \psi(\bm{z}_{\backslash n}, \bx)}\Big) \nonumber \\
    &\propto  h(z_{n})\exp \Big(\E{Q_{\bm{z}_{\backslash n}}}{ {\eta_{n}(\bm{z}_{\backslash n}, \bx)}}^{T}T(z_{n})\Big) = h(z_{n})e^{v_{n}^{T}T(z_{n}) }, \label{eq:exponential_update}
  \end{align}

where
\[
  v_{n} = \E{Q_{\bm{z}_{\backslash n}}}{\eta_{n}(\bm{z}_{\backslash n}, \bx)}.
\]

Summarizing, the factor is in the exponential family with the same base measure \(h\) and updating it is equivalent to setting the distribution parameter \(\eta\) to the expected one of the complete conditional \(\E{}{\eta_{n}(\bx, \bm{z}_{\backslash n})}\). This expression facilitates deriving CAVI algorithm for many complex models.

Going back to the conditionally conjugate models, our main goal is to set both the local and global hidden variable update. Our variational distribution is in the mean-field family with \(Q(\btheta \mid \lambda)\) where \(\lambda\) is called the \emph{global variational parameter}, and for each local variable, the distribution is \(Q(z_{n} \mid \gamma_{n})\), where \(\gamma_{n}\) is called a \emph{local variational parameter}:
\[
  Q(z_{n} \mid \gamma_{n}) \propto h(z_{n})e^{{\gamma_{n}}^{T}T(z_{n})},
\]
\[
  Q(\btheta \mid \lambda) = h(\btheta) \exp \Big( {\lambda}^{T}[\btheta, -\psi(\theta)] - \psi(\lambda) \Big).
\]

CAVI iteratively updates each local variational parameter and the global variational parameter.

Following the steps done in~\ref{eq:exponential_update}, the local variational parameter update is
\[
  \gamma_{n}^{new} = \E{Q(\btheta \mid \lambda)}{\eta(\btheta, x_{n})},
\]
In this case, the local hidden variable conditional does not depend on the other local hidden variables, neither other data-points.

The global variational parameter update is calculated as
\[
  \begin{aligned}
    Q^{new}(\btheta \mid \lambda) &\propto \exp \E{Q(\bz)}{\log P(\btheta \mid \bz , \bx)} \propto h(\btheta) \exp \E{Q(\bz)}{\big[  \lambda_{1} + \sum_{n=1}^{N}T(x_{n}, z_{n}), \lambda_{2} + N \big]^{T}\big[ \btheta, \psi(\btheta) \big]}\\
    &\propto h(\btheta) \exp \Bigg( {\Big[  \lambda_{1} + \sum_{n=1}^{N}\E{Q(\bz)}{T(x_{n}, z_{n})}, \lambda_{2} + N \Big]}^{T}\Big[ \btheta, - \psi(\btheta) \Big]\Bigg)\\
    &= Q(\btheta, \lambda^{new}).
  \end{aligned}
\]
Then, the variational parameter updated is
\[
  \lambda^{new} = \Bigg[ \lambda_{1} + \sum_{n=1}^{N} \E{Q(z_{n}\mid \gamma_{n})}{T(x_{n},z_{n})}, \lambda_{2} + N \Bigg].
\]

We can compute the ELBO at each iteration up to a constant that does not depend on the variational parameters,

\[
  \begin{aligned}
    ELBO &= {\Big( \lambda_{1} + \sum_{n=1}^{N} \E{Q(z_{n}\mid \gamma_{n})}{T(x_{n},z_{n})}\Big)}^{T}\E{Q(\btheta \mid \lambda)}{\btheta} - (\lambda_{2} + N) \E{Q(\btheta \mid \lambda)}{\psi(\btheta)}\\
    &+ \lambda ^{T} \E{Q(\btheta, \lambda)}{T(\btheta)} - \psi(\lambda) + \sum_{n=1}^{N}\gamma_{n}^{T}\E{Q(z_{n}, \gamma_{n})}{z_{n}} - \psi(\gamma_{n}) + \text{ const. }
  \end{aligned}
\]

The calculations are the following:

\[
  \begin{aligned}
   ELBO(Q(\btheta, \bz)) &= \E{Q(\btheta, \bz)}{\log{P(\btheta, \bz, \bx)}} - \E{Q(\btheta, \bz)}{\log{Q(\btheta, \bz)}}\\
  &= \E{Q(\bz \mid \bm{\gamma})}{\E{Q(\btheta \mid \lambda)}{\log P(\btheta, \bz, \bx)}}- \E{Q(\btheta, \bz)}{\log{Q(\btheta, \bz)}}\\
  &=  \E{Q(\btheta \mid \lambda)}{\log P(\btheta)} + \E{Q(\bz \mid \bm{\gamma})}{\E{Q(\btheta \mid \lambda)}{\sum_{n=1}^{N} \log P(z_{n}, x_{n} \mid \btheta)}}\\
  &\hspace{0.5cm} - \E{Q(\btheta, \bz)}{\log{Q(\btheta, \bz)}}
  \end{aligned}
\]
The first term is
\[
  \begin{aligned}
  \E{Q(\btheta \mid \lambda)}{\log P(\btheta)} &=  \E{Q(\btheta \mid \lambda)}{ \lambda_{1}\btheta - \lambda_{2}\psi(\btheta) - \psi(\lambda) }\\
  &= \lambda_{1}  \E{Q(\btheta \mid \lambda)}{\btheta} - \lambda_{2}\E{Q(\btheta \mid \lambda)}{\psi(\btheta)} - \psi(\lambda).
  \end{aligned}
\]
The middle term is 

\[
 \begin{aligned}
    &\E{Q(\bz \mid \bm{\gamma})}{\E{Q(\btheta \mid \lambda)}{\log P(\btheta, \bz, \bx)}} =  \E{Q(\bz \mid \bm{\gamma})}{\E{Q(\btheta \mid \lambda)}{\log P(\btheta) + \sum_{n=1}^{N} \log P(z_{n}, x_{n} \mid \btheta)}}\\
    &= \E{Q(\bz \mid \bm{\gamma})}{\E{Q(\btheta \mid \lambda)}{\log P(\btheta)}} + \E{Q(\bz \mid \bm{\gamma})}{\E{Q(\btheta \mid \lambda)}{\sum_{n=1}^{N} \log P(z_{n}, x_{n} \mid \btheta)}}\\
    &= {\Big( \lambda_{1} + \sum_{n=1}^{N} \E{Q(z_{n}\mid \gamma_{n})}{T(x_{n},z_{n})}\Big)}^{T}\E{Q(\btheta \mid \lambda)}{\btheta} - (\lambda_{2} + N) \E{Q(\btheta \mid \lambda)}{\eta(\btheta)}
 \end{aligned}
\]

The last term is
\[
  \begin{aligned}
  \E{Q(\btheta, \bz)}{\log{Q(\btheta, \bz)}}  &= \E{Q(\btheta)}{\log Q(\btheta)} +  \E{Q(\bz)}{ \sum_{n=1}^{N} \log Q(z_{n})}\\
  &= \E{Q(\btheta)}{\lambda^{T}T(\btheta) - \psi(\lambda)} + \sum_{n=1}^{N} \E{Q(z_{n})}{\gamma_{n}^{T}z_{n} - \psi(\gamma_{n})}\\
  &= \lambda ^{T} \E{Q(\btheta, \lambda)}{T(\btheta)} - \psi(\lambda) + \sum_{n=1}^{N}\gamma_{n}^{T}\E{Q(z_{n}, \gamma_{n})}{z_{n}} - \psi(\gamma_{n}).
  \end{aligned}
\]

