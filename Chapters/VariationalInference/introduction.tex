
\emph{Statistical inference} is the process of using data analysis to deduce properties of an underlying distribution.

The following elements are being considered: a random variable \(X\) with its corresponding set of observations \(\bx = (x_{1},\dots, x_{N})\) and a \emph{hidden or latent variable} \(Z\), which means that it is unobserved and the values \(\bz = (z_{1}, \dots, z_{N})\) are unknown. The samples are governed by the joint distribution \(P(\V, z)\). Where \(z\) represents a vector with length \(N\) with a possible configuration of the unknown values, then, it is not fixed as \(\bx\).

In \emph{classical inference} the conditional is calculated as
\[
  P(\bz \mid \bx) = \frac{P(\bx, \bz)}{\int_{\bz} P(\bx,\bz)}\ ,
\]
but for many models this integral is computationally hard to solve.

By comparison, \emph{Bayesian inference} is a method of statistical inference in which Bayes' theorem is used, it derives the posterior probability \(P(\bz \mid \bx)\) as a consequence of two antecedents: a prior probability \(P(\bz)\) and a \emph{likelihood function} \(P(\bx \mid bz)\)  derived from a statistical model for the observed data.

Other methods as \emph{Markov chain Monte Carlo (MCMC)} and \emph{variational Bayesian inference} try a different approach when solving the given inference problem.

On one hand, \emph{Variational Bayesian inference} or \emph{variational inference}, is a machine learning method whose main goal is to approximate probability distributions (\cite{jordan1999introduction}, \cite{wainwright}). On the other hand, \emph{MCMC} approximates the posterior distribution using a Markov chain. Let us introduce briefly the main idea behind this method, we need to introduce two concepts: \emph{Markov chain} and \emph{MCMC}.

A \emph{Markov Chain} is formally defined as a stochastic process, i.e, a family of random variables, that satisfies the \emph{Markov property} also known as the memoryless property: \textit{the conditional probability distribution of future states of the process (conditional on both present and past values) depends only on the present state}. To fully understand it, imagine a system with a number of possible states \(S_{1},\dots,S_{5}\) and the probabilities of going from one state to another as in the following diagram.

\begin{center}
\begin{tikzpicture}[
mynode/.style={
  draw,
  circle,
  minimum size=1em
  },
every loop/.append style={-latex},
start chain=going right
]
\foreach \Value in {1,...,5}
  \node[mynode, on chain] (s\Value) {$S_{\Value}$};
\path[-latex]
  (s2) edge[bend right] node[auto,swap,font=\small] {$0.7$} (s1)
  (s2) edge[bend right] node[auto,swap,font=\small] {$0.3$} (s3)
  (s3) edge[bend right] node[auto,swap,font=\small] {$0.5$} (s2)
  (s3) edge[bend right] node[auto,swap,font=\small] {$0.5$} (s4)
  (s4) edge[bend right] node[auto,swap,font=\small] {$0.65$} (s3)
  (s4) edge[bend right] node[auto,swap,font=\small] {$0.35$} (s5)
  (s1) edge[loop left] node[left,font=\small] {$1$} (s1)
  (s5) edge[loop right] node[right,font=\small] {$1$} (s5);
\end{tikzpicture}
\end{center}

Consider a sequence of random variables \(X_{t}\) that symbolize the current state at the step \(t\). The Markov property means that the probability of moving to the next state depends only on the present one, i.e,
\[
  P(X_{n+1} = x \mid X_{1} = x_{1} \dots, X_{n} = x_{n}) = P(X_{n+1} = x \mid X_{n} = x_{n}).
\]

We need two concepts to define the process of MCMC:
\begin{itemize}
  \item \textbf{Ergodic Markov chain}. A Markov chain where it exists a number \(N \in \mathbb{N}\) such that any state can be reached from any other state in any number of steps less or equal than \(N\).
  \item \textbf{Stationary Distribution}. The probability distribution to which the process converges over time.
\end{itemize}

In MCMC, an ergodic Markov chain over the latent variable whose stationary distribution is the posterior \(P(\bz \mid \bx)\), samples are taken from the chain to approximate the posterior with them.

Instead, \emph{variational inference} changes the inference problem with an optimization one. We fix a family of distributions \(\mathcal{Q}\) over the latent variable \(\bZ\) and find the element that minimizes its Kullback-Leibler divergence with the posterior \(P(\bz \mid \bx)\).
\[
  Q^{opt} = \argmin_{Q \in \mathcal{Q}} \KL{Q(\bz)}{P(\bz \mid \bx)}.
\]
Compared to \emph{Markov Chain Monte Carlo (MCMC)}, variational inference tends to be faster and scale easier to large data.

We can further analyze the Kullback-Leibler divergence we want to minimize, it may be decomposed the following way:
\[
  \begin{aligned}
    \KL{Q(\bz)}{P(\bz \mid \bx)} &= \E{Q(\bz)}{\log{Q(\bz)}} - \E{Q(\bz)}{\log{P(\bz \mid \bx)}}\\
    &= \E{Q(\bz)}{\log{Q(\bz)}} - \E{Q(\bz)}{\log{P(\bx, \bz)}} + \log{P(\bx)}.
  \end{aligned}
\]

Given that the Kullback-Leibler divergence cannot be computed (as \(P(z \mid \V)\) is unknown), we can optimize an equivalent objective: we already saw that the divergence is always positive so we have the following lower bound to the evidence, defined as \emph{evidence lower bound} or \emph{ELBO}.
\[
  \log{P(\bx)} \geq  - \E{Q(\bz)}{\log{Q(\bz)}} + \E{Q(\bz)}{\log{P(\bx, \bz)}}  = \text{ELBO}(Q).
\]

So that minimizing the Kullback-Leibler divergence is equivalent to maximize the ELBO. We can rewrite this bound as
\[
  \begin{aligned}
    \text{ELBO}(Q) &= \E{Q(\bz)}{\log{P(\bz)}} + \E{Q(\bz)}{\log{P(\bx \mid \bz)}} - \E{Q(\bz)}{\log{Q(\bz)}}\\
    &= \E{Q(\bz)}{\log{P(\bx \mid \bz)}} - \KL{Q(\bz)}{P(\bz)},
  \end{aligned}
\]
where it is expressed as the sum of the log likelihood of the observations and the Kullback-Leibler divergence between the prior \(P(\bz)\) and \(Q(\bz)\).

The \emph{Expectation Maximization algorithm} is designed to optimize this lower bound in order to solve the optimization problem we are focusing. We will go deep into this algorithm in Section~\ref{sec:em}.
