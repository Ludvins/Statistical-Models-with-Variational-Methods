

% Plantilla para un Trabajo Fin de Grado de la Universidad de Granada,
% adaptada para el Doble Grado en Ingeniería Informática y Matemáticas.
%
%  Autor de la plantilla original: Mario Román.
%  Enlace: https://github.com/mroman42/templates
%  Cambios en la plantilla por: Luis Ortega
%  Licencia: GNU GPLv2.
%
%
% Esta plantilla es una adaptación al castellano de la plantilla
% classicthesis de André Miede, que puede obtenerse en:
%  https://ctan.org/tex-archive/macros/latex/contrib/classicthesis?lang=en
% La plantilla original se licencia en GNU GPLv2.
%
% Esta plantilla usa símbolos de la Universidad de Granada sujetos a la normativa
% de identidad visual corporativa, que puede encontrarse en:
% http://secretariageneral.ugr.es/pages/ivc/normativa
%
% La compilación se realiza con las siguientes instrucciones:
%   pdflatex --shell-escape main.tex
%   bibtex main
%   pdflatex --shell-escape main.tex
%   pdflatex --shell-escape main.tex

% Opciones del tipo de documento
\documentclass[twoside,openright,titlepage,numbers=noenddot,openany,headinclude,footinclude=true, cleardoublepage=empty,abstractoff,BCOR=5mm,paper=a4,fontsize=11pt, dvipsnames]{scrreprt}

% Paquetes de latex que se cargan al inicio. Cubren la entrada de
% texto, gráficos, código fuente y símbolos.
\usepackage{relsize}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx} % Inclusión de imágenes.
\usepackage{grffile}  % Distintos formatos para imágenes.
\usepackage{longtable} % Tablas multipágina.
\usepackage{wrapfig} % Coloca texto alrededor de una figura.
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage[colorlinks=true]{hyperref}
\usepackage{pgfplots}
\usepackage{dcolumn}
\usepackage{booktabs}
\usepackage{natbib}
\usepackage{bm}
\usepackage{centernot}

% Tikz
\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{bayesnet}
\usetikzlibrary{shapes.geometric}
\usetikzlibrary{decorations.text}

% License
\usepackage[
    type={CC},
    modifier={by-nc-sa},
    version={4.0},
]{doclicense}

\usepackage{caption}
\usepackage[toc,page]{appendix}

% Plantilla classicthesis
\usepackage[beramono,eulerchapternumbers,linedheaders,parts,a5paper,dottedtoc,
manychapters,pdfspacing]{classicthesis}

% Geometría y espaciado de párrafos.
\setcounter{secnumdepth}{0}
\usepackage{enumitem}
\setitemize{noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt}
\setlist[enumerate]{topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex}
\usepackage[top=1in, bottom=1.5in, left=0.9in, right=1.2in]{geometry}
\setlength\itemsep{0em}
\setlength{\parindent}{0pt}
\usepackage{parskip}

% Algoritmos
\usepackage[ruled,vlined]{algorithm2e}
\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{mycommfont}

% Profundidad de la tabla de contenidos.
\setcounter{secnumdepth}{1}
\setcounter{tocdepth}{1}

% Usa el paquete minted para mostrar trozos de código.
% Pueden seleccionarse el lenguaje apropiado y el estilo del código.
\usepackage{minted}
\usemintedstyle{colorful}
\setminted{fontsize=\small}
\renewcommand{\theFancyVerbLine}{\sffamily\textcolor[rgb]{0.5,0.5,1.0}{\oldstylenums{\arabic{FancyVerbLine}}}}

% Archivos de configuración.
\input{tex/macros}  % En macros.tex se almacenan las opciones y comandos para escribir matemáticas.
\input{tex/classicthesis-config} % En classicthesis-config.tex se almacenan las opciones propias de la plantilla.

% Color institucional UGR
% \definecolor{ugrColor}{HTML}{ed1c3e} % Versión clara.
\definecolor{ugrColor}{HTML}{c6474b}  % Usado en el título.
\definecolor{ugrColor2}{HTML}{c6474b} % Usado en las secciones.

% Datos de portada
\usepackage{titling} % Facilita los datos de la portada
\author{Luis Antonio Ortega Andrés}
\date{\today}
\title{Statistical Models with Variational Methods}

% Glossaries
\usepackage[toc, nopostdot,  style=super, nonumberlist, section=chapter ]{glossaries}
\newglossary{symbols}{sym}{sbl}{List of Abbreviations and Symbols}
\makeglossaries
\loadglsentries{tex/glossary}

% Portada
\include{tex/titlepage}
\usepackage{wallpaper}


\begin{document}
\ThisULCornerWallPaper{1}{tex/ugrA4.pdf}
\maketitle
\newpage
\vspace*{\fill}
\doclicenseThis
The source code of this text and developed programs are available in the Github repository \href{https://github.com/Ludvins/Statistical-Models-with-Variational-Methods}{Ludvins/Statistical-Models-with-Variational-Methods}

\chapter*{Abstract}
\input{tex/abstract}

\chapter*{Resumen extendido en español}
\input{tex/resumen_extendido}

\chapter*{Introduction}
\input{tex/introduction}

\tableofcontents

\ctparttext{
  \color{black}
  \begin{center}
    In this part the underlying concepts of probability and
    graph theory that will be needed are reviewed. \emph{Probability distributions} and \emph{random variables} are defined, and \emph{Bayes' theorem} is stated. The \emph{Kullback-Leibler divergence} is defined as it plays a central role in all the following theory.

    Any \emph{discrete and continuous distributions} used later on in the document is also reviewed.
  \end{center}
}
\part{Basic Concepts}

\chapter{Probability}
\input{tex/BasicConcepts/probability}

\chapter{Distributions}
\input{tex/BasicConcepts/distributions}

\chapter{Graph Theory}
\input{tex/BasicConcepts/graph-theory}

\ctparttext{
  \color{black}
  \begin{center}
    \emph{Statistical inference} is defined as the process of deducing properties of an underlying distribution using data analysis.

    \emph{Bayesian inference} is an statistical inference method in which Bayes' theorem is applied. The \emph{posterior probability} is derived as a consequence of a \emph{prior probability} and a \emph{likelihood function}. Both of these are derived from a statistical model for the observed data.

    \emph{Maximum likelihood estimation} is a parameter estimation method which maximizes the \emph{likelihood function} of the underlying parametric distribution. This method maximizes the parameters under which the observed data is most probable.
  \end{center}
}
\part{Statistical Inference}

\chapter{Introduction}
\input{tex/Inference/introduction}

\chapter{Maximum Likelihood}
\input{tex/Inference/maximum-likelihood}

\chapter{Bayesian Inference}
\input{tex/Inference/bayesian}

\chapter{Missing variables}
\input{tex/Inference/missing-variables}

\ctparttext{
  \color{black}
  \begin{center}

    \emph{Variational Bayesian methods} consist of approximation techniques applied to intractable integrals that arise in inference and machine learning problems. Theese methods are commonly used in complex models containing \emph{observed and latent variables} and \emph{unknown parameters}. \emph{Graphical models} might be used to describe the underlying relations of the given variables.

    \emph{Variational Bayesian inference} solves the inference problem by creating an equivalent \emph{optimization problem} and approaching its solution through machine learning techniques.
  \end{center}
}
\part{Variational Inference}

\chapter{Introduction}\label{ch:vi_intro}
\input{tex/VariationalInference/introduction}

\chapter{Expectation Maximization}\label{ch:vi_em}
\input{tex/VariationalInference/expectation-maximization}

\chapter{Mean-Field Variational Inference}
\input{tex/VariationalInference/meanfield-variational-inference}

\chapter{Exponential Family}
\input{tex/VariationalInference/exponential-family}

\chapter{Example: Gaussian Mixture}\label{ch:gm}
\input{tex/VariationalInference/gaussian-mixture}

\ctparttext{
  \color{black}
  \begin{center}

    A \emph{graphical model} is a statistical model for which the dependence structure between random variables is expressed by a graph.

    Commonly, they provide a graph-based representation for encoding a multi-dimensional
    distribution representing a set of independences that hold in the specific
    distribution. The most commonly used are \emph{Bayesian networks} and \emph{Markov random
      fields}, these two differ in the set of independences they are able to encode and the
    factorization of the distribution they include.
  \end{center}
}
\part{Graphical Models}

\chapter{Introduction}
\input{tex/GraphicalModels/introduction}

\chapter{Bayesian networks}
\input{tex/GraphicalModels/bayesian-networks}

\chapter{Markov Random Fields}
\input{tex/GraphicalModels/markov-networks}

\ctparttext{
  \color{black}
  \begin{center}
    Some inference methods get optimized when applied among with a graphical model. In this part we focus in how methods we have already reviewed use the Bayesian network structure to get simplified and optimized.

    The \emph{PC algorithm} is also reviewed as a procedure to generate a Bayesian network from a given dataset.

  \end{center}
}
\part{Bayesian Networks Learning}

\chapter{Structure Learning}
\input{tex/BayesianNetworksLearning/structure-learning}

\chapter{Maximum Likelihood Training}
\input{tex/BayesianNetworksLearning/maximum-likelihood}

\chapter{Bayesian Training}
\input{tex/BayesianNetworksLearning/bayesian}

\chapter{Expectation-maximization algorithm}
\input{tex/BayesianNetworksLearning/expectation-maximization}

\chapter{Variational message passing algorithm}\label{sec:vmp}
\input{tex/BayesianNetworksLearning/variational-message-passing}



\ctparttext{
  \color{black}
  \begin{center}
    In this part, three common latent variable models are reviewed. These are \emph{Gaussian mixture}, \emph{latent Dirichlet allocation} and \emph{principal components analysis}.

    In \emph{principal components analysis}, the extension of regular \emph{latent variable models} with non-linear functions in the form of \emph{neural networks} is reviewed.
  \end{center}
}
\part{Commonly studied Latent Variable Models}

\chapter{Gaussian Mixture}\label{ch:lvm_gm}
\input{tex/LVMs/gm}

\chapter{Latent Dirichlet Allocation}
\input{tex/LVMs/lda}

\chapter{Probabilistic Principal Components Analysis}
\input{tex/LVMs/pca-va}

\ctparttext{
  \color{black}
  \begin{center}
    Three different \texttt{python} frameworks are reviewed: \texttt{InferPy}, \texttt{BayesPy} and \texttt{Scikit-Learn}. The main objective is to test each framework's features and limitations, in order to do so, both Gaussian mixtures and dimensionality reduction models are used.
  \end{center}
}
\part{Case study}

\chapter{Used frameworks}
\input{tex/CaseStudy/introduction}
\section{InferPy}
\input{tex/CaseStudy/inferpy}
\section{BayesPy}
\input{tex/CaseStudy/bayespy}
\section{Scikit-Learn}
\input{tex/CaseStudy/sklearn}

\chapter{Dimensionality Reduction}
\input{tex/CaseStudy/reduction}

\chapter{Gaussian Mixture}
\input{tex/CaseStudy/mixture}

\addtocontents{toc}{\vspace{1\baselineskip}}
\clearpage

\addcontentsline{toc}{chapter}{\textsc{Conclusions}}
\chapter*{Conclusions and further work}
\input{tex/conclusions.tex}
\addcontentsline{toc}{chapter}{\textsc{Acknowledgments}}
\chapter*{Acknowledgments}
\input{tex/Acknowledgments.tex}


\ctparttext{
  \color{black}
  \begin{center}

  \end{center}
}
\part*{\textsc{Appendices}}

\appendix
\chapter{Distributions in the exponential family}\label{ap:exp_fam}
\input{tex/Appendixes/A}

\chapter{Conjugate distributions}\label{ap:conj_distr}
\input{tex/Appendixes/B}


\addtocontents{toc}{\vspace{1\baselineskip}}
\clearpage

\printglossary[title={\textsc{notation}}]
\glsaddallunused

\clearpage
\nocite{*}
\bibliographystyle{authordate1}
\bibliography{tex/refs}
\end{document}
