
\section{The mean-field variational family}

The \emph{mean-field variational family} \(\mathcal{Q}\) as the family of distributions where the latent variables are mutually independent and any \(Q \in \mathcal{Q}\) verifies
\[
  Q(z) = \prod_{i=1}^{N}Q_{j}(z_{j})
\]

Where \(\bm{z} = \{z_{1},\dots,z_{M}\}\) is the set of latent variables considered in the problem. Notice that each \(Q_{j}\) can be different and this family does not depend on the observed data. The mean-field family can capture any marginal of the latent variables but not correlation between them. For example, consider a two dimensional Gaussian distribution shown in blue in the next figure. Any mean-field approximation would factorize as a product of two Gaussian distributions (shown in purple).



\begin{figure}[h!]
\centering
\begin{tikzpicture}
  \pgfplotsset{ticks=none}
    \begin{axis}[
        axis lines = middle,
        xmin=-11, xmax=11, ymin=-11, ymax=11,
        axis equal,
        xlabel = $z_{1}$,
        ylabel = {$z_{2}$},
        yticklabels={,,},
        ]
        \filldraw[rotate around={45:(110,110)}, color=blue!50, fill=blue!20, very thick, opacity=0.7] (110,110) ellipse (70 and 20);
        % \addlegendentry{Exact Posterior}
        \filldraw[color=purple!50, fill=purple!20, very thick, opacity=0.7]  (110,110) circle (25);
        % \addlegendentry{Mean-field Approximation}
    \end{axis}
\end{tikzpicture}
\end{figure}

Using this family to approximate the conditional is then equivalent to find the ``circle'' that best explains the ``ellipse'' given that the Kullback-Leibler divergence penalizes more placing mass in \(Q\) where \(P\) has little mass than the reverse.

Notice the parametric form of each factor \(Q_{j}\) is not specified and the appropriate configuration depends on the variable. For example, a continuous variable might have a Gaussian factor and a categorical variable will have a categorical factor.

\section{CAVI Algorithm }

In this section, we describe a widely used algorithm to solve the optimization problem we discussed in the previous section using the mean-field family. It is \emph{coordinate ascent variational inference (CAVI)} and its procedure is to iteratively optimize each factor of the mean-field family distribution, while fixing the others. With this, the ELBO reaches a local optimum.

 \begin{algorithm}[t]
  \SetAlgoLined
  \KwData{A distribution \(P(\V , \bm{z})\) with a dataset \(\V\)}
  \KwResult{A distribution of the mean-field family \(Q(\bm{z}) = \prod_{j=1}^{M}Q_{j}(z_{j})\)}
  Initialize \(Q(\bm{z})\)\;
  \While{Convergence stop criteria}{
    \For{\(j \in 1,\dots,M\)}{
      \(\bm{z}_{\backslash j} = (z_{1},\dots,z_{j-1},z_{j+1},\dots,z_{M})\)\;
      Set \(Q_{j}(z_{j}) \propto \exp{\E{Q(\bm{z}_{\backslash j})}{\log{P(z_{j} \mid \bm{z}_{\backslash j}, \V)}}}\)\;
    }
    Compute \(ELBO(Q)\)\;
  }
  \KwRet{\(Q\)}\;
  \caption{Coordinate Ascent Variational Inference}
  \label{alg:cavi}
\end{algorithm}

Let us see the main idea behind this iterative process, consider the \(j^{th}\) variable \(z_{j}\) and the other latent variables \(\bm{z}_{\backslash j} = (z_{1},\dots,z_{j-1},z_{j+1},\dots,z_{M})\). Let the other factors \(Q_{r}, r\neq j\) be fixed The ELBO over \(z_{j}\) has the form
\[
  \begin{aligned}
    ELBO(Q_{j}) &= \E{Q_{j}}{\E{Q_{\bm{z}_{\backslash j}}}{\log{P(\V, \bm{z})}}} - \E{Q_{j}}{\log{Q_{j}(z_{j})}}\\
    &= \E{Q_{j}}{\E{Q_{\bm{z}_{\backslash j}}}{\log{P(z_{j} \mid \bm{z}_{\backslash j}, \V)} + \log{P(\bm{z}_{\backslash j}, \V)}}} - \E{Q_{j}}{\log{Q_{j}(z_{j})}}\\
    &=  \E{Q_{j}}{\E{Q_{\bm{z}_{\backslash j}}}{\log{P(z_{j} \mid \bm{z}_{\backslash j}, \V)}}} - \E{Q_{j}}{\log{Q_{j}(z_{j})}} + \text{ const. }\\
    &= - \KL{Q_{j}(z_{z})}{Q_{j}^{*}(z_{j})} + \text{ const.}
  \end{aligned}
\]
Where \(Q_{j}^{*}(z_{j}) \) is proportional to
\[
  Q_{j}^{*}(z_{j}) \propto \exp{\E{Q(\bm{z}_{\backslash j})}{\log{P(z_{j} \mid \bm{z}_{\backslash j}, \V)}}}
\]

Notice that the proportionality restriction is enough to fully determine the distribution as its integral is normalized. Equivalently, the distribution is proportional to
\[
    Q_{j}^{*}(z_{j}) \propto \exp{\E{Q(\bm{z}_{\backslash j})}{\log{P(z_{j}, \bm{z}_{\backslash j}, \V)}}}
\]

Maximizing the ELBO is equivalent to minimizing that Kullback-Leibler divergence, which corresponds to setting \(Q_{j}(z_{j}) = Q_{j}^{*}(z_{j})\).
As the ELBO is generally a non-convex function and the CAVI algorithm converges to a local optimum, the initialization values of the algorithm play an important role on its performance.
The convergence criteria is usually a threshold for the ELBO.
