
\section{The mean-field variational family}

The \emph{mean-field variational family} \(\mathcal{Q}\) is defined as the family of distributions where the variables are mutually independent, i.e, any \(Q \in \mathcal{Q}\) verifies
\[
  Q(\bz) = \prod_{n=1}^{N}Q_{n}(z_{n}),
\]

where \(\bm{z} = \{z_{1},\dots,z_{N}\}\) is the considered set of variables. The mean-field family is commonly used to model the family of distributions over the latent variables in our optimization problem. Notice that each \emph{factor} \(Q_{n}\) can be different and this family does not depend on the observed data.

The mean-field family can capture any marginal of the latent variables but not correlation between them, as it assumes they are independent. For example, consider a two dimensional Gaussian distribution where a high percentage of the density is inside the blue ellipse shown in the next figure. Any mean-field approximation would factorize as a product of two Gaussian distributions, condensing its density in a circle as shown in purple.

\begin{figure}[h!]
\centering
\begin{tikzpicture}
  \pgfplotsset{ticks=none}
    \begin{axis}[
        axis lines = middle,
        xmin=-11, xmax=11, ymin=-11, ymax=11,
        axis equal,
        xlabel = $z_{1}$,
        ylabel = {$z_{2}$},
        yticklabels={,,},
        ]
        \filldraw[rotate around={45:(110,110)}, color=blue!50, fill=blue!20, very thick, opacity=0.7] (110,110) ellipse (70 and 20);
        % \addlegendentry{Exact Posterior}
        \filldraw[color=purple!50, fill=purple!20, very thick, opacity=0.7]  (110,110) circle (25);
        % \addlegendentry{Mean-field Approximation}
    \end{axis}
  \end{tikzpicture}
  \caption{Mean-field family distribution (purple) approximating a Gaussian distribution (blue)}
\end{figure}

Notice the parametric form of each factor \(Q_{n}\) is not specified and the appropriate configuration depends on the variable. For example, a continuous variable might have a Gaussian factor and a categorical variable have a categorical factor.

\section{CAVI Algorithm }

In this section, we describe a widely used algorithm to solve the optimization problem we discussed in the previous section using the mean-field family. It is \emph{coordinate ascent variational inference} or \emph{CAVI} and its procedure is to iteratively optimize each factor of the mean-field family distribution, while fixing the others. With this, the ELBO reaches a local optimum.

 \begin{algorithm}[t]
  \SetAlgoLined
  \KwData{A distribution \(P(\V , \bm{z})\) with a dataset \(\V\)}
  \KwResult{A distribution of the mean-field family \(Q(\bm{z}) = \prod_{n=1}^{N}Q_{n}(z_{n})\)}
  Initialize \(Q(\bm{z})\)\;
  \While{Convergence stop criteria}{
    \For{\(n \in 1,\dots,N\)}{
      \(\bm{z}_{\backslash n} = (z_{1},\dots,z_{n-1},z_{n+1},\dots,z_{M})\)\;
      Set \(Q_{n}(z_{n}) \propto \exp{\E{Q(\bm{z}_{\backslash n})}{\log{P(z_{n} \mid \bm{z}_{\backslash n}, \V)}}}\)\;
    }
    Compute \(ELBO(Q)\)\;
  }
  \KwRet{\(Q\)}\;
  \caption{Coordinate Ascent Variational Inference}
  \label{alg:cavi}
\end{algorithm}

Let us see the main idea behind this iterative process, consider the \(n^{th}\) variable \(z_{n}\) and the let \(\bm{z}_{\backslash n} = (z_{1},\dots,z_{n-1},z_{n+1},\dots,z_{N})\), that is, the full set of variables without the focused one. Let the other factors \(Q_{m}, m\neq n\) be fixed. Under this assumptions, the ELBO has the form
\[
  \begin{aligned}
    ELBO(Q) &= \E{Q_{n}}{\E{Q_{\bm{z}_{\backslash n}}}{\log{P(\V, \bm{z})}}} - \E{Q_{n}}{\log{Q_{n}(z_{n})}} + \text{ const. }\\
    &= \E{Q_{n}}{\E{Q_{\bm{z}_{\backslash n}}}{\log{P(z_{n} \mid \bm{z}_{\backslash n}, \V)} + \log{P(\bm{z}_{\backslash n}, \V)}}} - \E{Q_{n}}{\log{Q_{n}(z_{n})}} + \text{ const. }\\
    &=  \E{Q_{n}}{\E{Q_{\bm{z}_{\backslash n}}}{\log{P(z_{n} \mid \bm{z}_{\backslash n}, \V)}}} - \E{Q_{n}}{\log{Q_{n}(z_{n})}} + \text{ const. }\\
    &= - \KL{Q_{n}(z_{z})}{  \exp{\E{Q(\bm{z}_{\backslash n})}{\log{P(z_{n} \mid \bm{z}_{\backslash n}, \V)}}} } + \text{ const.}.
  \end{aligned}
\]
Minimizing the ELBO corresponds to setting the new distribution \(Q_{n}^{new}\) as:
\[
  Q_{n}^{new}(z_{n}) \propto \exp{\E{Q(\bm{z}_{\backslash n})}{\log{P(z_{n} \mid \bm{z}_{\backslash n}, \V)}}}.
\]

Notice that the proportionality restriction is enough to fully determine the distribution as its integral is normalized. Equivalently, the distribution is proportional to
\[
    Q_{n}^{new}(z_{n}) \propto \exp{\E{Q(\bm{z}_{\backslash n})}{\log{P(z_{n}, \bm{z}_{\backslash n}, \V)}}}.
\]


As the ELBO is generally a non-convex function and the CAVI algorithm converges to a local optimum, the initialization values of the algorithm play an important role on its performance.
The convergence criteria is usually a threshold for the ELBO.
