\section{Model statement}

A Gaussian mixture model is a latent variable model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters.

\pgfmathdeclarefunction{gauss}{2}{%
  \pgfmathparse{1/(#2*sqrt(2*pi))*exp(-((x-#1)^2)/(2*#2^2))}%
}
\begin{figure}[h!]
  \centering
  \begin{tikzpicture}
    \begin{axis}[every axis plot post/.append style={
        mark=none,domain=-2:5,samples=50,smooth}, % All plots: from -2:2, 50 samples, smooth, no marks
      axis x line*=bottom, % no box around the plot, only x and y axis
      axis y line*=left, % the * suppresses the arrow tips
      enlargelimits=upper] % extend the axes a bit to the right and top
      \addplot {gauss(0,0.5)};
      \addplot {gauss(2,0.75)};
      \addplot {gauss(3, 0.4)};
    \end{axis}
  \end{tikzpicture}
  \caption{One-dimensional Gaussian mixture with 3 clusters.}
\end{figure}


The following elements are being considered (using~\cite{bishop2006pattern} notation):
\begin{itemize}\setlength\itemsep{1em}
  \item \(K\) mixture components and \(N\) observations.
  \item A set of i.i.d real valued random variables \(\bX = (X_{1},\dots, X_{N})\) and a corresponding set of observations \(\bx = (x_{1},\dots, x_{n})\).
  \item The cluster assignment latent variables \(\bZ = (Z_{1}, \dots, Z_{N})\), where each \(z_{n}\) is a vector full of zeros but a position with a one, indicating the cluster to which \( x_n \) belongs.
  \item We choose a Dirichlet distribution over the mixing coefficients \(\bm{\pi}\)
    \[
    \bm{\pi} \sim \text{Symmetric-Dirichlet}(\alpha_{0}) \implies P(\bm{\pi}) \propto \prod_{k=1}^{K}\pi_{k}^{\alpha_{0}-1}.
    \]
    The hyper-parameter \(\alpha_{0}\) is the effective prior of each mixture component. Then \(\bm{\pi} = (\pi_{1},\dots,\pi_{K})\) are the mixture weights, i.e, prior probability of a particular component \(k\).
    \[
    P(\bz\mid \bm{\pi}) = \prod_{n=1}^{N}\prod_{k=1}^{K}\pi_{k}^{z_{n,k}} \implies (Z_{n} \mid \bm{\pi}) \sim \text{Categorical}(\bm{\pi}).
    \]
  \item \(\bm{\mu} = (\mu_{1},\dots,\mu_{K})\) and \(\bm{\Lambda} = (\Lambda_{1},\dots,\Lambda_{K})\) are the distribution parameters of each observation full conditional
    \[
    (\bm{X} \mid \bm{Z}, \bm{\mu}, \bm{\Lambda}) \sim \prod_{n=1}^{N}\prod_{k=1}^{K}\mathcal{N}(\mu_{k} \mid \Lambda_{k})^{z_{n,k}}.
    \]
  \item The prior governing \(\bm{\mu}\) and \(\bm{\Lambda}\) is an independent Gaussian, Inverse-Gamma distribution with hyper parameters \(m_{0}, \beta_{0}, w_{0}\) and \(v_{0}\):
    \[
    P(\bm{\mu}, \bm{\Lambda}) = P(\bm{\mu} \mid \bm{\Lambda})P(\bm{\Lambda}) \text{ where }
    \begin{cases}
      (\bm{\mu}\mid\bm{\Lambda}) &\sim \displaystyle\prod_{k=1}^{K}\mathcal{N}(m_{0}, \beta_{0}\Lambda_{k})\\
      \bm{\Lambda} &\sim \displaystyle\prod_{k=1}^{K}\text{Inverse-Gamma}(w_{0}, v_{0})
      \end{cases}.
    \]
\end{itemize}

The joint probability factorizes as
\[
  P(\bm{x}, \bm{z}, \bm{\pi}, \bm{\mu}, \bm{\Lambda}) = P(\bm{x}\mid \bm{z}, \bm{\mu}, \bm{\Lambda})P(\bm{z}\mid \bm{\pi})P(\bm{\pi})P(\bm{\mu}\mid \bm{\Lambda})P(\bm{\Lambda}).
\]

~\cite{bishop2006pattern} gives the explicit update for the CAVI algorithm, in the following section we summarize the needed steps to reach the update for one factor of the variational distribution.

\section{Variational Distribution and CAVI update}

We consider a variational distribution in the mean-field family,
\[
  Q(\bz, \bpi, \bmu, \bLambda) = Q(\bz)Q(\bpi)\prod_{k=1}^{K}Q(\mu_{k})Q(\Lambda_{k}).
\]
Let us consider the update for \(Q(\bz)\), using the update given in~\ref{eq:cavi_update}:
\[
  Q^{new}(\bz) \propto \exp \E{Q(\bpi, \bmu, \bLambda)}{\log P(\bx, \bz, \bpi, \bmu, \bLambda)}.
\]
Which implies
\[
  \begin{aligned}
    \log Q^{new}(\bz) &= \E{Q(\bpi, \bmu, \bLambda)}{\log P(\bx, \bz, \bpi, \bmu, \bLambda)} + \text{const.}\\
    &= \E{Q(\bpi, \bmu, \bLambda)}{\log \big( P(\bx \mid \bz, \bmu, \bLambda)P(\bz \mid \bpi)P(\bpi)P(\bmu \mid \bLambda)P(\bLambda) \big)} + \text{const.}\\
    &= \E{Q(\bpi)}{\log P(\bz \mid \bpi)} + \E{Q(\bmu, \bLambda)}{\log P(\bx \mid \bz, \bmu, \bLambda)} + \text{const.}
  \end{aligned}
\]
Where
\[
  \begin{aligned}
    \E{Q(\bpi)}{\log P(\bz \mid \bpi)} &=  \E{Q(\bpi)}{\log \prod_{n=1}^{N}\prod_{k=1}^{K}\pi_{k}^{z_{n,k}}} = \E{Q(\bpi)}{\sum_{n=1}^{N}\sum_{k=1}^{K}z_{n,k}\log \pi_{k}}\\
    &= \sum_{n=1}^{N}\sum_{k=1}^{K}z_{n,k} \E{Q(\bpi)}{\log \pi_{k}}.
  \end{aligned}
\]
Following a similar reasoning with the other expectation, we get that defining
\[
  \log \rho_{n,k} = \E{Q(\bpi)}{\pi_{k}} + \frac{1}{2}\E{\bLambda}{\log \| \Lambda_{k}\|} - \frac{D}{2} \log (2\pi) - \frac{1}{2}\E{\mu_{k}, \Lambda_{k}}{(x_{n} - \mu_{k})^{T}\Lambda_{k}(x_{n} - \mu_{k})},
\]
where \(D\) is the dimensionality of the data \(x\). We obtain
\[
  Q^{new}(\bz) \propto \prod_{n=1}^{N}\prod_{k=1}^{K} \rho_{n,k}^{z_{n,k}}.
\]
