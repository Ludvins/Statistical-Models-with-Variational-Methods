
The exponential family (\cite{koopman1936distributions}) is a parametric set of probability distributions of a certain form. This form is chosen based on some useful algebraic properties and generality, Appendix~\ref{ap:exp_fam} shows some examples of common distributions in the exponential family.

Let \(X\) be a random variable and \(\bm{\theta}\) a set of parameters. A family of distributions is said to belong the exponential family if its probability distribution has the form
\[
  P(x \mid \bm{\theta}) = h(x)\exp{\Big( \sum_{i=1}^{S} \eta_{i}(\bm{\theta})T_{i}(x) - \psi(\bm{\theta}) \Big)},
\]
where \(h(x)\), \(T_{i}(x)\), \(\eta_{i}(\bm{\theta})\) and \(\psi(\bm{\theta})\)  are known functions such that \(h\) is called a \emph{base measure}, \(\eta_{i}(\bm{\theta})\) are called the \emph{distribution parameters},  \(T_{i}(x)\) the \emph{test statistics} and \(\psi\) is the \emph{log normalizer} as it ensures logarithmic normalization due to
\[
  \begin{aligned}
    1 &= \int_{x}  h(x)\exp{\Big( \sum_{i=1}^{S} \eta_{i}(\bm{\theta})T_{i}(x) - \psi(\bm{\theta}) \Big)}\\
    &= \int_{x} e^{-\psi(\btheta)} h(x)\exp{\Big( \sum_{i=1}^{S} \eta_{i}(\bm{\theta})T_{i}(x) \Big)}\\
    &= e^{-\psi(\btheta)} \int_{x} h(x)\exp{\Big( \sum_{i=1}^{S} \eta_{i}(\bm{\theta})T_{i}(x) \Big)},
  \end{aligned}
\]
so \(\psi\) verifies
\[
      \psi(\bm{\theta}) = \log \int_{x} h(x) \exp \Big( \sum_{i=1}^{S} \eta_{i}(\bm{\theta})T_{i}(x) \Big).
\]

Naming \(\bm{\eta}\) and \(\bm{T}\) the corresponding vector functions, the parameters can always be transformed as \(\bm{\theta}^{new} = \bm{\eta}(\bm{\theta})\), in which case we say the distribution into its \emph{canonical form} (notice \(\psi\) has changed but we do not distinguish it from the previous one since it is fully determined by the other functions):
\[
  P(x \mid \bm{\theta}) = h(x) e^{\bm{\theta}^{T}\bm{T}(x) - \psi(\bm{\theta})}.
\]
In this form, it is easier to see that \(\bm{T}\) is the sufficient statistic for \(\btheta\). This is a consequence of \emph{Fisherâ€“Neyman factorization theorem} which says that \textit{\(\bm{T}\) is sufficient for \(\btheta\)  if and only if the probability distribution \(P\)  can be factored into a product such that one factor, \(h\), does not depend on \(\btheta\)  and the other factor, which does depend on \(\btheta\), depends on \(x\)  only through \(\bm{T}\).}

An important property of the exponential family is that they have \emph{conjugate priors}, this is said when the posterior distribution is in the same probability distribution family as
the prior distribution, they are then called \emph{conjugate distributions}, and
the prior is called a \emph{conjugate prior} of the likelihood distribution. Appendix~\ref{ap:conj_distr} shows examples of conjugate distributions and their conjugate priors.

\begin{proposition}\label{prop:conj_prior}
  Let \(X\) be a random variable and \(\btheta\) a set of parameters. Suppose an exponential family likelihood:
  \[
    P(x \mid \btheta) = h(x)e^{\btheta^{T}\bm{T}(x) - \psi(\btheta)}.
  \]
  and prior with hyper-parameters \(\bm{\alpha}, \gamma\):
  \[
    P(\btheta \mid \bm{\alpha}, \gamma) \propto e^{\btheta^{T} \bm{\alpha} - \gamma \psi(\btheta)}.
  \]
  Then, the posterior is in the same parametric family as the prior with
  \[
    P(\btheta \mid \bx, \bm{\alpha}, \gamma) = P(\btheta \mid \bm{\alpha} + \bm{T}(x), \gamma + 1).
  \]
\end{proposition}
\begin{proof}
  \[
    P(\btheta \mid \bx, \bm{\alpha}, \gamma)  \propto P(x \mid \btheta)P(\btheta \mid \bm{\alpha}, \gamma) \propto \exp \Big( \btheta^{T}[\bm{\alpha} + \bm{T}(x)] - [\gamma + 1]\psi(\btheta) \Big).
  \]
\end{proof}


\section{Latent variable and conditionally conjugate models}

One important case of exponential family are \emph{latent variable models} or \emph{LVMs}. In this models, the following assumptions are made:
\begin{enumerate}\itemsep0.5em
  \item There set of i.i.d random variables \(\bX = (X_{1},\dots,X_{N})\) and a set of observations \(\bx = (x_{1}, \dots, x_{N})\).
  \item There are both global and local latent variables. The global ones are denoted by \(\btheta\) whereas the locals are denoted by \(\bm{Z} = (Z_{1}, \dots, Z_{N})\). We refer to \emph{global hidden variables} when they affect the whole distribution and \emph{local hidden variables} when they affect only to a subset, in this case each \(Z_{n}\) affects only \(X_{n}\). Given this, the joint probability is
    \[
    P(\bx, \btheta, \bz) = P(\btheta)\prod_{n=1}^{N}P(z_{n}, x_{n} \mid \btheta).
    \]
  \item The \(n^{th}\) observation \(x_{n}\) and the \(n^{th}\) local hidden variable \(z_{n}\) are conditionally independent, given the global variables \(\btheta\), of all other observations and local hidden variables,
    \[
    P(x_{n},z_{n} \mid \btheta, \bx_{\backslash n}, \bz_{\backslash n}) = P(x_{n}, z_{n} \mid \btheta).
    \]
\end{enumerate}

\begin{remark}
  These models are widely used to discover patterns in data sets (\cite{blei2014build}).
  LVMs include popular models like \emph{Latent Dirichlet Allocation} models used to uncover the hidden topics in text corpora (\cite{blei2003latent}), mixture of Gaussian models to discover hidden clusters in data (\cite{bishop2006pattern}) and probabilistic principal component analysis for dimensionality reduction (\cite{tipping1999probabilistic}). These three models are reviewed throughout this document.
\end{remark}

One important case of LVMs and exponential family models are \emph{conditionally conjugate models}, where the following assumptions are made.
\begin{enumerate}[itemsep=2ex]
  \item The prior for the global latent variable \(P(\btheta)\) is in the exponential family with an hyper-parameter \(\alpha = [\alpha_{1}, \alpha_{2}]\), where \(\alpha_{1}\) is a vector and \(\alpha_{2}\) is a scalar, and statistics that concatenate the global latent variable \(\btheta\)  and its log normalizer from \(P(z_{n}, x_{n} \mid \btheta)\), \(\psi(\btheta)\)\footnote{The same symbol \(\psi\) is used for every normalizer function. The distinction must be made using their parameters. },
    \[
    P(\btheta) = h(\btheta) \exp \Big( \alpha^{T}[\btheta, -\psi(\btheta)] - \psi(\alpha)\Big).
    \]
  \item Each local term \(P(z_{n},x_{n} \mid \btheta)\) is in the exponential family of the form
    \[
    P(z_{n},x_{n} \mid \btheta) = h(z_{n}, x_{n}) \exp \Big( \btheta^{T} T(z_{n}, x_{n}) - \psi(\btheta) \Big).
    \]
  \item The complete conditional of a local latent variable verifies
    \[
    P(z_{n} \mid \btheta, \bx, \bz_{\backslash n}) = P(z_{n} \mid x_{n}, \btheta)
    \]
    and is also in the exponential family
    \[
    P(z_{n}\mid x_{n} , \btheta) =  h(z_{n})\exp \Big( {\eta(\btheta, x_{n})}^{T}T(z_{n}) - \psi(\btheta, x_{n}) \Big).
    \]
\end{enumerate}

Using Proposition~\ref{prop:conj_prior}, the posterior \(P(\btheta \mid \bx, \bz)\) is in the same family with parameter
\[
  \bar{\alpha} = {[\alpha_{1} + \sum_{n=1}^{N} T(z_{i}, x_{i}), \alpha_{2}+ N]}^{T}.
\]
A step by step reasoning would be:
\[
  \begin{aligned}
    P(\btheta \mid \bx, \bz) &= \frac{P(\bx, \bz \mid \btheta) P(\btheta)}{P(\bx, \bz)} \propto  P(\bx, \bz \mid \btheta) P(\btheta) = P(\btheta) \prod_{n=1}^{N}h(x_{n}, z_{n})\exp \Big( \btheta^{T}T(z_{n}, x_{n})  - \psi(\btheta) \Big)\\
    &\propto  h(\btheta) \exp \Big( \alpha^{T}[\btheta, -\psi(\btheta)]\Big)  \prod_{n=1}^{N}h(x_{n}, z_{n})\exp \Big( \btheta^{T}T(z_{n}, x_{n})  - \psi(\btheta) \Big)\\
    &\propto h(\btheta) \exp \Big(   \alpha^{T}[\btheta, -\psi(\btheta)] + \sum_{n=1}^{N}  \btheta^{T}T(z_{n}, x_{n})  - \psi(\btheta)  \Big)\\
    &\propto h(\btheta) \exp \Big(   \alpha_{1}^{T}\btheta  - \alpha_{2}^{T}\psi(\btheta) - N\psi(\btheta) + \sum_{n=1}^{N}  T(z_{n}, x_{n})^{T}\btheta  \Big)\\
    &\propto h(\btheta) \exp \Big(   \big(\alpha_{1} + \sum_{n=1}^{N}  T(z_{n}, x_{n})\big)^{T} \btheta  - (\alpha_{2} + N)^{T}\psi(\btheta) \Big).
  \end{aligned}
\]

\section{CAVI in conditionally conjugate models}~\label{sec:cavi_ccm}

Consider the following situation where we fit a distribution \(Q(\bm{z}) = \prod_{n=1}^{N} Q_{n}(z_{n})\) in the mean-field family, using an exponential family distribution for the marginal \(P(z_{n} \mid \bm{z}_{\backslash n}, \bx)\):
\[
  P(z_{n} \mid \bm{z}_{\backslash n}, \bx) = h(z_{n})\exp \Big( {\eta_{n}(\bm{z}_{\backslash n}, \bx)}^{T}T(z_{n}) - \psi(\bm{z}_{\backslash n}, \bx) \Big).
\]

The update of the CAVI algorithm  is then given by
  \begin{align}
    Q(z_{n}) &\propto \exp{\E{Q(\bm{z}_{\backslash n})}{\log P(z_{n} \mid \bm{z}_{\backslash n}, \bx)}} \nonumber\\
    &= h(z_{n})\exp \Big(\E{Q_{\bm{z}_{\backslash n}}}{ {\eta_{n}(\bm{z}_{\backslash n}, \bx)}}^{T}T(z_{n}) - \E{Q_{\bm{z}_{\backslash n}}}{ \psi(\bm{z}_{\backslash n}, \bx)}\Big) \nonumber \\
    &\propto  h(z_{n})\exp \Big(\E{Q_{\bm{z}_{\backslash n}}}{ {\eta_{n}(\bm{z}_{\backslash n}, \bx)}}^{T}T(z_{n})\Big) = h(z_{n})e^{v_{n}^{T}T(z_{n}) }, \label{eq:exponential_update}
  \end{align}

where
\[
  v_{n} = \E{Q_{\bm{z}_{\backslash n}}}{\eta_{n}(\bm{z}_{\backslash n}, \bx)}.
\]

Summarizing, the factor is in the exponential family with the same base measure \(h\) and updating it is equivalent to setting the distribution parameter \(\eta\) to the expected one of the complete conditional \(\E{}{\eta_{n}(\bx, \bm{z}_{\backslash n})}\). This expression facilitates deriving CAVI algorithm for many complex models.

Going back to the conditionally conjugate models, our variational distribution is in the mean-field family with \(Q(\btheta \mid \lambda)\) where \(\lambda\) is called the \emph{global variational parameter}, and for each local variable, the distribution is \(Q(z_{n} \mid \gamma_{n})\), where \(\gamma_{n}\) is called a \emph{local variational parameter}:
\[
  Q(z_{n} \mid \gamma_{n}) \propto h(z_{n})e^{{\gamma_{n}}^{T}T(z_{n})},
\]
\[
  Q(\btheta \mid \lambda) = h(\btheta) \exp \Big( {\lambda}^{T}[\btheta, -\psi(\btheta)] - \psi(\lambda) \Big).
\]

CAVI iteratively updates each local variational parameter and the global variational parameter.

Following the steps done in~\ref{eq:exponential_update}, the local variational parameter update is
\[
  \gamma_{n}^{new} = \E{Q(\btheta \mid \lambda)}{\eta(\btheta, x_{n})},
\]
In this case, the local hidden variable conditional does not depend on the other local hidden variables, neither other data-points.

The global variational parameter update is calculated as
\[
  \begin{aligned}
    Q^{new}(\btheta \mid \lambda) &\propto \exp \E{Q(\bz)}{\log P(\btheta \mid \bz , \bx)} \propto h(\btheta) \exp \E{Q(\bz)}{\big[  \lambda_{1} + \sum_{n=1}^{N}T(x_{n}, z_{n}), \lambda_{2} + N \big]^{T}\big[ \btheta, \psi(\btheta) \big]}\\
    &\propto h(\btheta) \exp \Bigg( {\Big[  \lambda_{1} + \sum_{n=1}^{N}\E{Q(\bz)}{T(x_{n}, z_{n})}, \lambda_{2} + N \Big]}^{T}\Big[ \btheta, - \psi(\btheta) \Big]\Bigg)\\
    &= Q(\btheta, \lambda^{new}).
  \end{aligned}
\]
Then, the variational parameter updated is
\[
  \lambda^{new} = \Bigg[ \lambda_{1} + \sum_{n=1}^{N} \E{Q(z_{n}\mid \gamma_{n})}{T(x_{n},z_{n})}, \lambda_{2} + N \Bigg].
\]

The ELBO can be computed at each iteration up to a constant which does not depend on the variational parameters,
\[
  \begin{aligned}
    ELBO &= {\Big( \lambda_{1} + \sum_{n=1}^{N} \E{Q(z_{n}\mid \gamma_{n})}{T(x_{n},z_{n})}\Big)}^{T}\E{Q(\btheta \mid \lambda)}{\btheta} - (\lambda_{2} + N) \E{Q(\btheta \mid \lambda)}{\psi(\btheta)}\\
    &+ \lambda ^{T} \E{Q(\btheta, \lambda)}{T(\btheta)} - \psi(\lambda) + \sum_{n=1}^{N}\gamma_{n}^{T}\E{Q(z_{n}, \gamma_{n})}{z_{n}} - \psi(\gamma_{n}) + \text{ const. }
  \end{aligned}
\]
The calculations are the following:
\[
  \begin{aligned}
   ELBO(Q(\btheta, \bz)) &= \E{Q(\btheta, \bz)}{\log{P(\btheta, \bz, \bx)}} - \E{Q(\btheta, \bz)}{\log{Q(\btheta, \bz)}}\\
  &= \E{Q(\bz \mid \bm{\gamma})}{\E{Q(\btheta \mid \lambda)}{\log P(\btheta, \bz, \bx)}}- \E{Q(\btheta, \bz)}{\log{Q(\btheta, \bz)}}\\
  &= \E{Q(\btheta \mid \lambda)}{\log P(\btheta)} + \E{Q(\bz \mid \bm{\gamma})}{\E{Q(\btheta \mid \lambda)}{\sum_{n=1}^{N} \log P(z_{n}, x_{n} \mid \btheta)}}\\
  &- \E{Q(\btheta, \bz)}{\log{Q(\btheta, \bz)}}
  \end{aligned}
\]
The first term is
\[
  \begin{aligned}
  \E{Q(\btheta \mid \lambda)}{\log P(\btheta)} &=  \E{Q(\btheta \mid \lambda)}{ \lambda_{1}\btheta - \lambda_{2}\psi(\btheta) - \psi(\lambda) }\\
  &= \lambda_{1}  \E{Q(\btheta \mid \lambda)}{\btheta} - \lambda_{2}\E{Q(\btheta \mid \lambda)}{\psi(\btheta)} - \psi(\lambda).
  \end{aligned}
\]
The middle term is 
\[
 \begin{aligned}
    &\E{Q(\bz \mid \bm{\gamma})}{\E{Q(\btheta \mid \lambda)}{\log P(\btheta, \bz, \bx)}} =  \E{Q(\bz \mid \bm{\gamma})}{\E{Q(\btheta \mid \lambda)}{\log P(\btheta) + \sum_{n=1}^{N} \log P(z_{n}, x_{n} \mid \btheta)}}\\
    &= \E{Q(\bz \mid \bm{\gamma})}{\E{Q(\btheta \mid \lambda)}{\log P(\btheta)}} + \E{Q(\bz \mid \bm{\gamma})}{\E{Q(\btheta \mid \lambda)}{\sum_{n=1}^{N} \log P(z_{n}, x_{n} \mid \btheta)}}\\
    &= {\Big( \lambda_{1} + \sum_{n=1}^{N} \E{Q(z_{n}\mid \gamma_{n})}{T(x_{n},z_{n})}\Big)}^{T}\E{Q(\btheta \mid \lambda)}{\btheta} - (\lambda_{2} + N) \E{Q(\btheta \mid \lambda)}{\eta(\btheta)}.
 \end{aligned}
\]
The last term is
\[
  \begin{aligned}
  \E{Q(\btheta, \bz)}{\log{Q(\btheta, \bz)}}  &= \E{Q(\btheta)}{\log Q(\btheta)} +  \E{Q(\bz)}{ \sum_{n=1}^{N} \log Q(z_{n})}\\
  &= \E{Q(\btheta)}{\lambda^{T}T(\btheta) - \psi(\lambda)} + \sum_{n=1}^{N} \E{Q(z_{n})}{\gamma_{n}^{T}z_{n} - \psi(\gamma_{n})}\\
  &= \lambda ^{T} \E{Q(\btheta, \lambda)}{T(\btheta)} - \psi(\lambda) + \sum_{n=1}^{N}\gamma_{n}^{T}\E{Q(z_{n}, \gamma_{n})}{z_{n}} - \psi(\gamma_{n}).
  \end{aligned}
\]

