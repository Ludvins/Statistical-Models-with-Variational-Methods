
All our theory will be made under the assumption that there is a
\emph{referential set} \(\Omega\), set of all possible outcomes of an experiment. Any subset of
\(\Omega\) will be called an \emph{event}.

\begin{definition}
Let \(\mathcal{P}(\Omega)\) be the power set of \(\Omega\). Then, \(\mathcal{F} \subset \mathcal{P}(\Omega)\) is called a
\emph{\(\sigma\)-algebra} if it satisfies three conditions:
\begin{itemize}
\item It contains the full referential set, \(\Omega \in \mathcal{F}\).
\item \(\mathcal{F}\) is closed under complementation.
\item \(\mathcal{F}\) is closed under countable unions.
\end{itemize}
From these properties it follows that \(\emptyset \in \mathcal{F}\) and that \(\mathcal{F}\)
is closed under countable intersections.

The tuple \((\Omega, \mathcal{F})\) is called a \emph{measurable space}.
\end{definition}

\begin{definition}
A \emph{probability} \(P\) over a measurable space \((\Omega, \mathcal{F})\) is a mapping
\(P: \mathcal{F} \to [0,1]\) which satisfies:
\begin{itemize}
\item \(P(\alpha) \geq 0 \ \ \forall \alpha \in \mathcal{F}\).
\item \(P(\Omega) = 1\).
\item \(P\) is countably additive, that is, if \(\{\alpha_n\}_{n \in \mathbb{N}}
  \subset \mathcal{F}\) is a countable collection of pairwise disjoint sets,
  then
  \[
  P\Big(\bigcup_{n\in \mathbb{N}}\alpha_n \Big) = \sum_{n \in \mathbb{N}}P(\alpha_n).
  \]
\end{itemize}
\end{definition}

The first condition guarantees non negativity. The second one states that the
\emph{trivial event} has the maximal possible probability of 1.
The third condition implies that given a set of pairwise disjoint events,
the probability of either one of them occurring is equal to the sum of the
probabilities of each one.

These conditions imply the following two:
\begin{itemize}
\item \(P(\emptyset) = 0\).
\item \(P(\alpha \cup \beta) = P(\alpha) + P(\beta) - P(\alpha \cap \beta)\).
\end{itemize}

The triple \((\Omega, \mathcal{F}, P)\) is called a \emph{probability space}.

\begin{definition}
  Given two events \(\alpha, \beta \in \mathcal{F}\), with \(P(\beta) \neq 0\),
  the conditional probability of \(\alpha\) given \(\beta\) is defined as the
  quotient of the probability of the joint events and the probability of
  \(\beta\):
  \[
    P(\alpha \mid \beta) = \frac{P(\alpha \cap \beta)}{P(\beta)}.
  \]
\end{definition}

\begin{theorem}
  \textbf{(Bayes' theorem)}. Let \(\alpha, \beta\) be two events of an
  experiment, given that \(P(\beta) \neq 0\). Then
  \[
  P(\alpha \mid \beta)= \frac{P(\beta \mid \alpha)P(\alpha)}{P(\beta)}.
\]
\end{theorem}

\begin{exampleth}
Consider a study where the relation of a disease \(d\) and an habit \(h\)
is being investigated. Suppose that \(P(d)=10^{-5}\), \(P(h)=0.5\) and \(P(h\mid d) = 0.9\). What is the
probability that a person with habit \(h\) will have the disease \(d\)?

\[
P(d \mid h) = \frac{P(d \cap h)}{P(h)} = \frac{P(h \mid d)P(d)}{P(h)} =
\frac{ 0.9 \times 10^{-5}}{ 0.5 } = 1.8 \times 10^{-5}.
\]

If the probability of having habit \(h\) is set to a much lower value as \(P(h) =
0.001\), then the above calculation gives approximately \(1/100\). Intuitively, a smaller number of people have the habit and most of them have the
desease. This means that the relation between having the desease and the habit
is stronger now compared with the case where more people had the habit.
\end{exampleth}

\begin{definition}
  Two events \(\alpha, \beta \in \mathcal{F}\) are said
  \emph{independent} if knowing one of them does not give any extra information
  about the other. Mathematically,
  \[
    P(\alpha \cap \beta) = P(\alpha)P(\beta) \quad \text{and} \quad P(\alpha \mid \beta) = P(\alpha).
  \]
  Let \(\alpha, \beta, \gamma \in \mathcal{F}\), \(\alpha\) and \(\beta\) are said to be
  \emph{conditionally independent} on \(\gamma\), expressed as \(\alpha \bigCI \beta \mid \gamma\),  if and only if
  \[
    P(\alpha \cup \beta \mid \gamma) = P(\alpha \mid \gamma)P(\beta \mid \gamma).
  \]
  Otherwise, they are said to be \emph{conditionally dependent} on \(\gamma\), expressed as \(\alpha \bigCD \beta \mid \gamma\).
\end{definition}

\section*{Random variables}

\begin{definition}
A function \(f:\Omega_1 \to \Omega_2\) between two
measurable spaces \((\Omega_1, \mathcal{F}_1)\) and \((\Omega_2, \mathcal{F}_2)\) is said to be \emph{measurable} if \(f^{-1}(\alpha) \in \mathcal{F}_1\) for every \(\alpha \in \mathcal{F}_2\).
\end{definition}

\begin{definition}
  A \emph{random variable} is a measurable function \(X:\Omega \to E\) from a probability
  space \((\Omega, \mathcal{F}, P)\) to a measurable space \((E,
  \mathcal{F}')\) verifying \(X(\omega)\in \mathcal{F}' \ \forall \omega \in \Omega\).

  The probability of \(X\) taking a value on a measurable set \(S \in E\) is
  written as
  \[
    P_X(S) = P(X \in S) = P(\{a \in \Omega \ \mid  \ X(a) \in S \}).
  \]
  Where the sub-index is usually omitted. A \emph{probability distribution} \(P_{X}\) of a random
  variable \(X\) over the probability space \((\Omega, \mathcal{F}, P)\) is defined
  as the push-forward measure of it, that is, \(P_X = P \circ X^{-1}\).
\end{definition}

Questions like ``How likely is that the value of \(X\) equals
\(a\)?'' are equivalent to ask for the probability (measure) of the set \(\{\omega
\in \Omega \ \mid  \ X(w) = a\}\).

The following notation is going to be used: random variables will be
denoted with an upper case letter like \(X\) and a set of variables with a
bold symbol like \(\bm{X}\). The meaning of \(P(state)\) will be clear without a reference to the variable.
Otherwise \(P(X = state)\) will be used.
Using a lower case letter like \(P(x)\) will denote the probability of the
corresponding upper case variable \(X\) taking a specific value \(x\).

\begin{definition}
The \emph{cumulative distribution function} of a real-valued random variable \(X\) is defined as
\[
F_X (x) = P(X \leq x),
\]
where the right-hand side represents the probability of the random variable
taking value below or equal to \(x\).
\end{definition}

\begin{definition}
When the image of a random variable \(X\) is countable, the random variable
is called a
\emph{discrete random variable} and its \emph{probability mass function} \(p\) gives the
probability of it being equal to some value:
\[
p(x) = P(X = x).
\]
In case the image is uncountable and real, then \(X\) is called a \emph{continuous random
  variable} and if there exists is a non-negative
Lebesgue-integrable \(f\) such that
\[
F_X(x) = P(X \leq x) = \int_{-\infty}^x f(u) du,
\]
then it is called its \emph{probability density function}.

A \emph{mixed random variable} is a random variable who is neither discrete nor
continuous, it can be realized as the sum of a discrete and continuous random
variables. An example of a random variable of mixed type would be based on an
experiment where a coin is flipped and a random positive number is chose only if
the result of the coin toss is heads, $-1$ otherwise.
\end{definition}

From now on, \(P(x)\) will denote \(f(x)\) when \(X\) is a continuous random
variable.

Integrate notation will be used in both continuous and discrete cases, where the last one can be interpreted as integration with respect to the \emph{counting measure} defined as
\[
 \#(dx) = \sum_{n \in \I}\delta(x - n)dx,
\]
where \(\I\) is the set of values \(X\) can take, and \(\delta\) is the Dirac measure. Given this measure, integration corresponds to summation as
\[
  \int_{x} P(x) \#(dx) = \sum_{n \in \I}\int_{x} P(x) \delta(x-n) dx = \sum_{n \in \I}P(n).
\]

Where \(\int_{x} f(x)\delta(x - x_{0}) = f(x_{0})\) is used. Given this, from now on, integration notation will be used for both discrete and continuous variables given that the integrals will be respect to the counting measure when needed.

\begin{definition}
  The \emph{conditional probability} might be defined over
  random variables, let \(X, Y\) be two random variables, then
  \[
    P(x \mid y) = \frac{P(x,y)}{P(y)}.
  \]
  It is  required that \(P(y) \neq 0\) for the conditional probability to be defined.
\end{definition}

The \emph{Bayes' theorem} may be enunciated as
\[
  P(x,y) = \frac{P(y\mid x)P(x)}{P(y)}
\]
Clearly, an arbitrary number of variables can be considered in both cases.

\begin{definition}
  The \emph{marginal distribution} of a subset of random variables is the
  probability distribution of the variables contained in that subset.
\end{definition}

Let \(X, Y\) be two random variables, the marginal distribution of \(X\) is:
\[
  P(x) = \int_y P(x,y).
\]


\begin{definition}
  Let \(\bm{X} = (X_1, X_2,\dots,X_n)\) be a set of random variables, the
  \emph{joint probability distribution} for \(\bm{X}\) is function that gives the probability of each random variable \(X_n\)  falling in a particular range or discrete set of values for that variable. It is called a \emph{multi-variate distribution}.

  When using only two random variables, then is called a \emph{bi-variate
    distribution}.

  This distribution can be expressed either in terms of a joint cumulative distribution
  function
  \[
    F_{\bm{X}}(\bm{x}) = F_{X_1,\dots,X_n}(x_1,\dots,x_n) = P(X_1 \leq x_1, \dots,
    X_n \leq x_n) \footnote{Where \(\bm{x} = (x_1,\dots,x_n)\)},
  \]
  or using a probability density or mass function.
\end{definition}

\begin{definition}
Two random variables \(X\) and \(Y\) are said to be \emph{independent} if knowing one of them doesn't give any extra information about the other. Mathematically,
\[
P(x,y) = P(x)P(y).
\]
From this it follows that if \(X\) and \(Y\) are independent, then \(P(x\mid y) = P(x)\).
\end{definition}


\begin{definition}
Let \(X,Y\) and \(Z\) be three random variables, then \(X\) and \(Y\) are
\emph{conditionally independent} given \(Z\) if and only if
\[
P(x,y \mid  z) = P(x\mid z)P(y\mid z),
\]
in that case we will denote \(X \bigCI Y \mid Z\). If \(X\) and \(Y\) are not
conditionally independent, they are \emph{conditionally dependent} \(X \bigCD Y \mid Z\)

\end{definition}

Both independence definitions can be made over sets of variables \(\bm{X},
\bm{Y}\) and \(\bm{Z}\) in a straightforward way.


\begin{definition}
  A set of \(N\) random variables \(\{X_1,\dots,X_N\}\) defined to
  assume values in \(I \subset \R\) are said
  \emph{independent and identically distributed (i.i.d)}
  if and only if they are independent, i.e,
  \[
    F_{X_1,\dots,X_N}(x_1,\dots,x_n) = F_{X_1}(x_1)\dots F_{X_N}(x_N) \ \forall
    x_1,\dots,x_N \in I,
  \]
  and are identically distributed
  \[
    F_{X_1}(x) = F_{X_n}(x) \ \forall n \in \{2,\dots,N\} \text{ and } \forall x
    \in I.
  \]
\end{definition}


\begin{definition}
  A \emph{multi-variate random variable} or \emph{random vector} is a column vector \(\bm{X} =
  {(X_1,\dots,X_N)}^T\) whose components are random variables that can be defined
  over different probability spaces.

  Note that the same symbol \(\bm{X}\) is used for random vectors and sets of
  variables, but the meaning will be clear within the context.
\end{definition}
