
LVMs have usually been restricted to the exponential family because, in this case, inference is feasible. But recent advances in variational inference have enabled LVMs to be extended with neural networks. For example, \emph{Variational Auto-encoders} or \emph{VAE} (\cite{kingma2013auto}) are the most influential models combining both concepts.

VAEs extent the classical technique of \emph{principal components analysis} for data representation in lower-dimensional spaces. Suppose we have a \(D\)-dimensional representation of a data point \(x\) and \(z\) is its latent \(K\)-dimensional representation (\(K < D\)). PCA computes an affine transformation \(\bm{W}\), represented by a \(K \times D\) matrix.


A probabilistic view of PCA can be modeled with an LVM (\cite{tipping1999probabilistic}), with the following elements:

\begin{itemize}
  \item \(\bX = \{X_{1},\dots,X_{N}\}\) i.i.d \(\mathbb{R}^{D}\)-valued random variables and the corresponding observations \(\bx = \{x_{1},\dots, x_{N}\}\).
  \item \(\bZ = \{Z_{1}, \dots, Z_{N}\}\) i.i.d latent \(\mathbb{R}^{K}\)-valued random variables, where \(Z_{n}\) models the \(K\)-dimensional representation of \(x_{n}\).
  \item A global latent \(K\times D\)-dimensional random variable \(\bm{W}\).
  \item A noise hyper-parameter \(\sigma^{2}\).
\end{itemize}

\begin{figure}[h!]
  \centering
  \begin{tikzpicture}[
    node distance=1cm and 0.5cm,
    mynode/.style={draw,circle,text width=0.5cm,align=center},
    param/.style={draw,text width=0.5cm,align=center}
    ]

    \node[mynode] (theta) {\(\bm{W}\)};
    \node[mynode, below left=of theta] (zn) {\(Z_{n}\)};
    \node[mynode, below right=of theta] (xn) {\(X_{n}\)};
    \node[param, right=of xn] (sigma) {\(\sigma\)};

    \plate{} {(zn)(xn)} {\(n = 1\dots N\)}; %
    \path (theta) edge[-latex] (xn)
    (sigma) edge[-latex] (xn)
    (zn) edge[-latex] (xn)
    ;

  \end{tikzpicture}
  \caption{Probabilistic PCA model}\label{fig:ppca}
\end{figure}



We assume the priors are normally distributed:
\[
  Z_{n} \sim N_{K}(0, I) \quad \forall n =1,\dots,N \quad \text{ and } \quad \bm{W} \sim N_{K\times D}(0, I).
\]
The data points are considered generated via a projection,
\[
  (X_{n} \mid Z_{n}, \bm{W}) \sim N(\bm{W}^{T}Z_{n}, \sigma^{2}I)\quad \forall n = 1,\dots, N.
\]
The probabilistic model extends the classical one in the way that the latter assumes the noise is infinitesimally small, i.e, \(\sigma^{2} \to 0\). The \emph{expectation-maximization algorithm} (Section~\ref{sec:em}) is commonly used to solve this variational inference problem.



\section{Artificial Neural networks}

An \emph{artificial neural network} or \emph{ANN} with \(L\) hidden layers can be seen as a deterministic non-linear function \(f\) parameterized by a set of matrix \(\bm{W} = \{\bm{W}_{0},\dots, \bm{W}_{L}\}\) and non-linear activation functions \(\{r_{0},\dots, r_{L}\}\). Given an input \(x\) the output \(y\) is calculated has
\[
  h_{0} = r_{0}(\bm{W}^{T}_{0}x), \quad \dots, \quad h_{l} = r_{l}(\bm{W}_{l}^{T}h_{l-1}) \quad \dots \quad y = r_{L}(\bm{W}_{L}^{T}h_{L-1}).
\]

\emph{Deep neural networks} or \emph{DNNs} are ANNs where the number of hidden layers is higher. Commonly, any neural network with more that 2 hidden layers is considered deep. Given a dataset \(\{(x_{1}, y_{1}), \dots, (x_{N}, y_{N})\}\) and a loss function \(l(y,y^{*})\) that defines how well the output \(y^{*} = f_{\bm{W}}(x)\)  returned by the model matches the real output \(y\), learning reduces to the optimization problem
\[
  \bm{W}^{opt} = \argmin_{\bm{W}} \sum_{n=1}^{N}l(y_{n}, f_{\bm{W}}(x_{n})).
\]

This problem is usually solved by applying a variant of the stochastic gradient descent method, which involves the computation of the gradient of the loss function with respect to the parameters of the network. The algorithm for computing this gradient is known as \emph{back-propagation}, which is based on a recursive application of the chain-rule of derivates. This can be implemented using the computational graph on the network.

The main idea of a computational graph is to express a deterministic function, as is the case of a neural network, using an acyclic directed graph. It is composed of input, output and operation nodes, where model data and parameters are shown as input nodes.

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[
    node distance=0.5cm and 1cm,
    mynode/.style={draw,rectangle,minimum size=1cm,,align=center}
    ]

    \node[mynode] (1) {\(*\)};
    \node[left=of 1] (2) {\(4\)};
    \node[above=of 1] (3) {\(x\)};
    \node[mynode, right=of 1] (4) {\(+\)};
    \node[above=of 4] (5) {\(y\)};
    \node[right=of 4] (6) {\(f\)};

    \path (2) edge[-latex] (1)
    (3) edge[-latex] (1)
    (1) edge[-latex] (4)
    (5) edge[-latex] (4)
    (4) edge[-latex] (6)
    ;

  \end{tikzpicture}
  \caption{Computational graph example of function \(f(x,y) = 4x + y\) }\label{fig:cnn_cg}
\end{figure}

\section{Non-linear PCA}

\textit{Non-linear PCA} or NLPCA, extends the classical PCA where the relation between the low dimensional space and the observed data is governed by a DNN instead of a linear transformation.
It can be seen as a non-linear probabilistic PCA model.

\begin{figure}[h!]
  \centering
  \begin{tikzpicture}[
    node distance=1cm and 0.5cm,
    mynode/.style={draw,circle,text width=0.5cm,align=center},
    param/.style={draw,text width=0.7cm,align=center}
    ]

    \node[param] (w0) {\(\bm{W}_0\)};
    \node[param, right = of w0] (w1) {\(\bm{W}_1\)};
    \node[mynode, below left=of w0] (zn) {\(Z_{n}\)};
    \node[mynode, below right=of w0] (xn) {\(X_{n}\)};

    \plate{} {(zn)(xn)} {\(n = 1\dots N\)}; %
    \path (w0) edge[-latex] (xn)
    (w1) edge[-latex] (xn)
    (zn) edge[-latex] (xn)
    ;

  \end{tikzpicture}
  \caption{Non-linear PCA model. \( \bm{W}_0 \) and \( \bm{W}_1 \)  together with two activation functions \( r_0, r_1 \)  represent an ANN. }\label{fig:ppca}
\end{figure}


The model is quite similar to the one presented for the PCA, the difference comes from the conditional distribution of \(X\), that depends on \(Z\) through a fully-connected ANN with a single hidden layer.

The prior of the latent variable is a centered Gaussian
\[
  Z_{n} \mid N(0,I) \quad \forall n \in 1,\dots,N
\]

Let \(D\) be the dimension of the data \(X\) and \(K\) the dimension of the hidden variable \(Z\).Let \(f\) a single hidden layer ANN with input dimension \(Z\) and output dimension \(D\). Where the output is the mean value of the normal distribution under \(X\).

As we are considering a single hidden layer, let \(\bm{W}_{0}\) and \(\bm{W}_{1}\) be the matrixes governing that ANN and \(r_{0}, r_{1}\) the activation functions, the ANN \(f\) is
\[
  f(z_{n}) = r_{1}(\bm{W_{1}}(r_{0}(\bm{W}_{0}(z_{n})))),
\]

and the data-points are then generated as
\[
  (X_{n}\mid Z_{n}) \sim N(f(Z_{n}), I) \quad \forall n \in 1,\dots,N.
\]
Where no noise is being considered this time.

\section{Variational Auto-encoder}

Similarly to the models PCA and NLPCA, a \emph{variational autoencoder} or VAE, allows to perform dimensionality reduction. However a VAE will contain a neural network in the \(P\)  model (decoder) and another one in the variational model \(Q\) (encoder).

The \(P\) model has the same structure as the nonlinear PCA. On the other hand, the  distribution \(Q\) is defined with a reverse ANN, with input dimension \(D\) and output dimension \(K\).

The neural networks can be defined to give an output of double the dimension, that is, the encoder would give a point in with \( 2K \) components, so the first \( K \) are used for the mean and the last \( K \) sor the variance of the normal distribution.  
