
Another method to deal with hidden variables is using \emph{Variational Bayes (VB)}, in contrast with the EM algorithm, this one uses a distribution that better represents the posterior.

Consider a simple datapoint \(x = (v,h)\), in this situation we focus our interest on the posterior distribution:
\[
  P(\theta \mid v) = \frac{P(v \mid \theta)P(\theta)}{P(v)} = \frac{1}{P(v)}\int_{h}P(v,h \mid \theta)P(\theta).
\]

Variational Bayes assumes the joint hidden and parameter posterior can be approximated as the product of two distributions, one over the hidden variable and one over the parameter such that
\[
  P(h ,\theta \mid v) \approx Q(h)Q(\theta).\footnote{We use the same letter for both distributions, as they can be differenced from the context.}
\]

\begin{remark}
  By doing this assumption, we are also assuming that the posterior can be approximated with \(Q(\theta)\)
  \[
    P(\theta \mid v) = \int_{h} P(\theta, h \mid v) \approx \int_{h} Q(h)Q(\theta) = Q(\theta).
  \]
  For this reason, the main goal of the algorithm is to make this approximation as tightest as possible with an iterative method that calculates both \(Q(\theta)\) and \(Q(h)\) distributions.
\end{remark}

To achieve that, we minimize the Kullback-Leibler divergence between them.
\[
  \KL{Q(h)Q(\theta)}{P(h, \theta \mid v)} = \E{Q(h)}{\log{Q(h)}} + \E{Q(\theta)}{\log{Q(\theta)}} - \E{Q(h)Q(\theta)}{\log{P(h,\theta \mid v)}}.
\]

We use that \(\log{P(v)}\) is independent from \(Q(h)Q(\theta)\) to get the desired inequality.
\[
  \begin{aligned}
    0 &\leq \E{Q(h)}{\log{Q(h)}} + \E{Q(\theta)}{\log{Q(\theta)}} - \E{Q(h)Q(\theta)}{\log{P(h,\theta \mid v)}}\\
    &= \E{Q(h)}{\log{Q(h)}} + \E{Q(\theta)}{\log{Q(\theta)}} - \E{Q(h)Q(\theta)}{\log{\frac{P(h,\theta,v)}{P(v)}}} \\
    &=\E{Q(h)}{\log{Q(h)}} + \E{Q(\theta)}{\log{Q(\theta)}} - \E{Q(h)Q(\theta)}{\log{P(h,\theta, v)}} + \E{Q(h)Q(\theta)}{\log{P(v)}} \\
    &= \E{Q(h)}{\log{Q(h)}} + \E{Q(\theta)}{\log{Q(\theta)}} - \E{Q(h)Q(\theta)}{\log{P(h,\theta, v)}} + \log{P(v)}. \\
  \end{aligned}
\]
We got the following lower bound
\[
  \log{P(v)} \geq -\E{Q(h)}{\log{Q(h)}}- \E{Q(\theta)}{\log{Q(\theta)}} + \E{Q(h)Q(\theta)}{\log{P(h,\theta, v)}}.
\]
Therefore, minimizing the Kullback-Leibler divergence is equivalent to find the tightest lower bound.

The procedure is then split in two steps to keep the structure of the EM algorithm.

\begin{itemize}
  \item \textbf{E-step}. Given a fixed \(Q(\theta)\), minimize the Kullback-Leibler divergence.
    \[
    Q^{new}(h) = \argmin_{Q(h)}\KL{Q(h)Q(\theta)}{P(h,\theta \mid v)}.
    \]

  \item \textbf{M-step}. Given a fixed \(Q(h)\), minimize the Kullback-Leibler divergence.
    \[
    Q^{new}(\theta) = \argmin_{Q(\theta)}\KL{Q(h)Q(\theta)}{P(h,\theta \mid v)}.
    \]
\end{itemize}

For fixed \( Q(\theta) \), the contribution from \( Q(h) \) to the Kullback-Leibler divergence is, as we derived in the CAVI algorithm, 
\[
  \begin{aligned}
  \KL{Q(h)Q(\theta)}{P(h,\theta \mid v)} &= \E{Q(h)}{\log Q(h)} - \E{Q(h)Q(\theta)}{\log P(v,h,\theta)} + \text{const.} \\
  &= - \KL{Q(h)}{\exp \E{Q(\theta)}{P(h, v, \theta)}} + \text{const.}
  \end{aligned}
\]
Therefore, the update is proportional to 
\[
   Q^{new}(h) \propto \exp \E{Q(\theta)}{\log P(v,h,\theta)} \propto \exp \E{Q(\theta)}{\log P(v,h \mid \theta)}.
\]
Following a similar argument, 
\[
  Q^{new}(\theta) \propto \exp \E{Q(h)}{\log P(v,h,\theta)} \propto \exp \E{Q(h)}{\log P(v,h \mid \theta)}.
\]

As in the case of the EM algorithm, each iterations guarantees an increase in the lower bound of the marginal likelihood, but increasing the marginal likelihood itself is not guaranteed.

When using an i.i.d dataset \(\V, \mathcal{H}\), we may assume that \(Q(\mathcal{H})\) is in the mean-field family:
\[
  Q(h_{1}, \dots, h_{N}) = \prod_{n=1}^{N}Q(h_{n}).
\]

The lower bound to the marginal likelihood is then written as a summation of the bounds on each datapoint.
\[
  \log{P(\V)} \geq \mathlarger{\sum_{n}} -\E{Q(h_{n})}{\log{Q(h_{n})}}- \E{Q(\theta)}{\log{Q(\theta)}} + \E{Q(h_{n})Q(\theta)}{\log{P(v_{n}, h_{n},\theta)}}
\]
