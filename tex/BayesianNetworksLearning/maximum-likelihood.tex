
 Remember that, in case \(P(x \mid \theta)\) is unconstrained, the maximum likelihood distribution corresponds to the empirical distribution.

 For a Belief Network we know there is the following constraint
 \[
   P(x_{1}, \dots, x_{N} \mid \theta) = \prod_{n = 1}^N P(x_n  \mid  pa(x_n), \theta).
 \]
 In this case \( N \) variables are being considered so the empirical distribution counts the number of occurrences of a configuration of these variables.

 We now aim to minimize the Kullback-Leibler divergence between the empirical
 distribution \(Q(x_1,\dots,x_N)\) and \(P(x_1, \dots, x_N \mid \theta)\) in order to get the Maximum Likelihood value:
 \[
   \begin{aligned}
   \KL{Q}{P} &= - \E{Q}{\sum_{n = 1}^N\log{P(x_n \mid pa(x_n), \theta)}} +
   \E{Q}{\sum_{n = 1}^N\log{Q(x_n \mid pa(x_n))}}
   \\ &= - \sum_{n = 1}^N \E{Q}{\log{P(x_n \mid pa(x_n), \theta)}} + \sum_{n =
     1}^N \E{Q}{\log{Q(x_n \mid pa(x_n))}}.
   \end{aligned}
 \]

We might use Proposition~\ref{prop:expectation_over_marginal} on \(\log{P(x_n \mid pa(x_n), \theta)}\) and \(Q(x_{n}, pa(x_{n}))\), resulting:
 \[
   \begin{aligned}
     \KL{Q}{P} &= \sum_{n = 1}^N \E{Q(x_n,pa(x_n))}{\log{Q(x_n \mid pa(x_n))}} - \E{Q(x_n,pa(x_n))}{\log{P(x_n \mid pa(x_n), \theta)}}\\
     &= \sum_{n = 1}^N \E{Q(x_i,pa(x_n))}{\KL{Q(x_n \mid pa(x_n))}{P(x_n \mid pa(x_n), \theta)}}.
   \end{aligned}
 \]

 The optimal setting is then
 \[
   P(x_n \mid pa(x_n), \theta) = Q(x_n \mid pa(x_n)),
 \]
 in terms of the initial data it is to set \(P(x_n \mid pa(x_n))\) to the number of
 times the state appears in it.
