
In this section we come back to variational inference, in this case we are  reviewing two main algorithms, \emph{expectation maximization} and \emph{variational message passing}.


\section{Expectation Maximization Algorithm}

As both hidden variables and Bayesian networks are used, the following notation from~\cite{barber} is utilized: \(\bX = (\bm{V}, \bm{H}) = \{X_{1}, \dots , X_{M}\}\) is the set of variables partitioned in visible and hidden. Let \(\{\bv^{1}, \dots, \bv^{N}\}\) be the set of observations of the visible variables. For each data-point \(\bx^{n} = (\bv^{n}, \bh^{n})\) decomposes into visible and hidden parts.

Remember the ELBO is:
\[
  \text{ELBO}(Q) = \underbrace{\sum_{n=1}^{N}\E{Q(\bh^{n})}{\log{Q(\bh^{n})}}}_{Entropy} + \underbrace{\sum_{n=1}^{N}\E{Q(\bh^{n})}{\log{P(\bv^{n}, \bh^{n} \mid \btheta)}}}_{Energy}.
\]
Where the steps of the EM algorithm are:
\begin{itemize}
  \item \textbf{E-step}:  \(Q^{new}(\bh^{n}) = P(\bh^{n} \mid \bv^{n} , \btheta) \quad \forall n=1,\dots,N\).
  \item \textbf{M-step}: \(\btheta^{new} = \argmax_{\btheta} \text{ELBO}(Q)\).
\end{itemize}

\begin{algorithm}[ht]
  \SetAlgoLined\KwData{A dataset \(\bx\), a distribution \(P(\bx, \bz \mid \btheta)\) that factorizes in a Bayesian network.}
  \KwResult{The maximum likelihood estimates for \(P(x_{m}\mid pa(x_{m})), m=m,\dots,M\), }
  Initialize \( P(x_{m}\mid pa(x_{i})), m=1,\dots,M \)\;
  \While{Convergence stop criteria}{
    \(t \leftarrow t+1 \)\;
    \For{\(n=1\) to \(N\)  }{
      \(Q^{n}_{t}(\bx) = P_{t}(\bh^{n} \mid \bv^{n})\delta(\bv, \bv^{n})\)\tcp*{E-step}
    }
    \For{\(i=1\) to \(M\)  }{
      \(P_{t}(x_{m} \mid pa(x_{m})) = \frac{\sum_{n=1}^{N}Q_{t}^{n}(x_{m}, pa(x_{m}))}{ \sum_{n=1}^{N}Q_{t}^{n}(pa(x_{m}))}\)\tcp*{M-step}
    }
  }
  \KwRet{\( P_{t}(x_{m}\mid pa(x_{m})), m=1,\dots,M  \)}\;
  \caption{Expectation Maximization Algorithm for Bayesian networks}\label{alg:em_bn}
\end{algorithm}


Whereas the structure of the Bayesian network gives no advantage over the E-step, it does on the M-step. Let the variational distribution be fixed as \(Q_{old}\), the \emph{energy term} in a Bayesian network is
\[
  \sum_{n=1}^{N} \E{Q_{old}(\bh^{n})}{ \log{P(\bx^{n} \mid \btheta)} } = \sum_{n = 1}^{N}\sum_{m = 0}^{M} \E{Q_{old}(\bh^{n})}{\log{P( x_{m}^{n} \mid pa(x_{m}^{n}), \btheta)}}.
\]

It is useful to use the following notation that defines a conditional distribution of the hidden variable when the visible one equals \(\bv^{n}\).
\[
  Q^{n}(\bx) = Q^{n}(\bv,\bh) = Q_{old}(\bh^{n}) \mathbb{I}(\bv = \bv^{n}).
\]
We can define the mixture distribution
\[
  Q(\bx) = \frac{1}{N}\sum_{n = 1}^{N}Q^{n}(\bx).
\]
One may show that the energy term equals
\[
  \sum_{n = 1}^{N}\E{Q_{old}(\bh^n)}{\log{P(\bx^{n} \mid \btheta)}} = N \ \E{Q(\bx)}{\log{P(\bx \mid \btheta)}},
\]
As
\[
  \begin{aligned}
    \E{Q(\bx)}{\log{P(\bx \mid \theta)}} &= \int_{\bx}Q(\bx)\log{P(\bx \mid \btheta)} =  \int_{\bx} \frac{1}{N}\sum_{n=1}^{N}Q^{n}(\bx)\log{P(\bx \mid \btheta)} \\
    &= \frac{1}{N} \int_{\bx}\sum_{n=1}^{N}Q_{old}(\bh^n)\mathbb{I}[\bv = \bv^{n}]\log{P(\bx \mid \btheta)}\\
    &= \frac{1}{N}\sum_{n = 1}^{N}\E{Q_{old}(\bh^n)} {\log{P(\bh^n, \bv^{n} \mid \btheta)}}\\
    &= \frac{1}{N}\sum_{n = 1}^{N}\E{Q_{old}(\bh^n)} {\log{P(\bx^{n} \mid \btheta)}},
  \end{aligned}
\]
In the other hand, using the Bayesian network structure and Proposition~\ref{prop:expectation_over_marginal}:
\[
  \begin{aligned}
    \E{Q(x)}{\log{P(\bx \mid \btheta)}} &= \sum_{m = 1}^{M}\E{Q(\bx)}{\log{P(x_{m} \mid pa(x_{m}), \btheta)}}\\
    &= \sum_{m = 1}^{M}\E{Q(x_{m}, pa(x_{m}))}{\log{P(x_{m} \mid pa(x_{m}, \btheta))}}\\
    &= \sum_{m = 1}^{M}\E{Q(pa(x_{m}))}{ \E{Q(x_{m}\mid pa(x_{m}))}{ \log{P(x_{m} \mid pa(x_{m}), \btheta)}}}.
\end{aligned}
\]
Adding a constant as
\[
  \E{Q(pa(x_{m}))}{\E{Q(x_{m}\mid pa(x_{m}))}{\log{Q(x_{m} \mid pa(x_{m}))}}}
\]
to the last term results in a Kullback-Leibler Divergence:
\[
  \sum_{m = 1}^{M} E_{Q(pa(x_{m}))} \Big[ \KL{Q(x_{m}\mid pa(x_{m}))}{P(x_{m} \mid pa(x_{m}), \btheta)}\Big]
\]
\[
  =\sum_{m = 1}^{M}\E{Q(pa(x_{m}))}{\E{Q(x_{m}\mid pa(x_{m}))}{\log{Q(x_{m} \mid pa(x_{m}))}} - \E{Q(x_{m}\mid pa(x_{m}))}{\log{P(x_{m} \mid pa(x_{m}), \btheta)}}}
\]
So maximizing the energy term is equivalent to minimize the above formula, that is, setting
\[
  P^{new}(x_{m} \mid pa(x_{m}), \btheta) = Q(x_{m} \mid pa(x_{m})).
\]
So the first observation is that \(\btheta\) is not needed in order to maximize the energy term due to the Belief network structure. Algorithm~\ref{alg:em_bn} shows the full procedure of the algorithm in Bayesian networks.


\section{Variational Message Passing (Work in Progress)}\label{sec:vmp}

In this section, the \emph{Variational Message Passing} or \emph{VMP} algorithm is reviewed as a variational Bayes application to Bayesian networks where the exponential family is considered using a message passing procedure between the nodes of a graphical model.

VMP allows variational inference to be applied automatically to a large class of Bayesian networks, without the need to derive application-specific update equations (\cite{winn2005variational}).

The full set of variables is \( \bX = (X_1\dots,X_N) \), where we are considering both hidden variables \( \bm{H} = (H_1,\dots,H_J) \) and visible ones \( \bm{V} = (V_{1}, \dots, V_{I})\). A variational distribution \( Q \) in the mean-field family
\[
   Q(\bm{H}) = \prod_{j=1}^J Q_{j}(h_j).
\]
The optimized factor for a fixed term \(Q(h_{j})\) is (as shown in the CAVI section~\ref{eq:cavi_update}) given by
\[
   \log Q_{j}^{new}(h_j) = \E{Q_{\backslash j}}{\log P(\bm{V}, \bm{H})} + \text{const.}
\]
Using the Bayesian network structure, the update is
\[
  \log Q_{j}^{new}(h_j) = \E{Q_{\backslash j}}{ \sum_{n=1}^N \log P(x_n \mid pa(x_n))} + \text{const.}
\]
The contribution of \(H_{j}\) to the given formula lies on the terms \( P(h_j \mid pa(h_j)) \) and the conditionals of all its children. Let \(cp(X,Y)\) denote the co-parents of \(Y\) with \(X\),
\[
  cp(X,Y) = pa(X)\backslash \{Y\},
\]
then, adding all other terms to the constant,
\[
   \log Q_{j}^{new}(h_j) = \E{Q_{\backslash j}}{\log P(h_j \mid pa(h_j))} + \sum_{X_k \in ch(H_j)} \E{Q_{\backslash j}}{\log P(x_k \mid h_{j},cp(x_k, h_{j}))} + \text{const.}
\]

This shows how the update of a hidden variable only depends on its Markov blanket. The optimization of \( Q_j \) is therefore computed as the sum of a term involving \( H_j \) and its parent nodes, along with a term for each children. This terms can be interpreted as ``messages'' from the corresponding nodes.

The exact form of the messages will depend on the functional form of the conditional distributions in the model. Important simplifications to the variational update equations occur when the conditional distribution of a node given its parents is in the exponential family. Sub-indexes will be used to denote parents, children and co-parents.

Consider a variable \( X \) and \( Y \in pa_X \), such as \( Y \) is a hidden variable. The s that both distributions belong to the exponential family:
\[
     P(y \mid pa_y) = h_Y(pa_Y)\exp \Big( {\bm{\eta}_Y(pa_y)}^T\bm{T}_Y(y) - \psi_Y(pa_y) \Big).
\]
\[
P(x \mid y,cp_{x,y}) = h_X(y, cp_{x,y})\exp \Big( {\bm{\eta}_X(y, cop_{x,y})}^T\bm{T}_X(x) - \psi_X(y, cp_{x,y}) \Big).
\]

\begin{remark}
  Given a variable \(X\) in the exponential family, if we know the natural parameter vector \(\bm{\eta}\), then we can find the expectation of the statistic function with respect to the distribution. Defining \(\bar{\phi}\) as a reparameterisation of \(\phi\) in terms of \(\bm{\eta}\):
  \[
    \begin{aligned}
      P(x\mid \theta) &= h(x)\exp \Big( \bm{\eta}(\theta)^{T}\bm{T}(x) + \psi(\theta) \Big)\\
       &= h(x)\exp \Big( \bm{\eta}(\theta)^{T}\bm{T}(x) + \bar{\psi}(\bm{\eta}(\theta)) \Big).
    \end{aligned}
  \]
  Integrating with respect to \(X\):
  \[
    1 = \int_{x} h(x)\exp \Big( \bm{\eta}(\theta)^{T}\bm{T}(x) + \bar{\psi}(\bm{\eta}(\theta)) \Big).
  \]
  Differentiating with respect to \(\bm{\eta}\):
  \[
    \frac{d}{d\theta}1 = 0 = \int_{x} \frac{d}{d\bm{\eta}}h(x)\exp \Big( \bm{\eta}(\theta)^{T}\bm{T}(x) + \bar{\psi}(\bm{\eta}(\theta)) \Big) = \int_{x}P(x \mid \theta)\Big[ \bm{T}(x) + \frac{ d\bar{\psi}(\bm{\eta}(\theta)) }{d \bm{\eta}}\Big].
  \]

  What implies that
  \[
    \E{P}{\bm{T}(x)} =   -  \frac{ d\bar{\psi}(\bm{\eta}(\theta)) }{d \bm{\eta}}
  \]

\end{remark}

The distribution \( P(Y \mid pa_Y) \) can be though as a prior over \( Y \), and \( P(X \mid pa_X) = P(X \mid Y, cop_{X,Y})\) as a contribution to the likelihood of \( Y \).

Conjugacy requires that these two conditionals have the same functional form with respect to \( Y \), so the latter has to be rewritten in terms of \( \bm{T}_Y (y)\) by defining functions \( \bm{\eta}_{X,Y} \) and \( \lambda \) as
\[
     \log P(x \mid y , cp_{x,y}) = \bm{\eta}_{XY}(x, cp_{x,y})^T \bm{T}_Y(y) + \lambda(x, cp_{x,y}).
\]

\begin{exampleth}
    If \( X \) is Gaussian distributed with mean \( \mu \) (Gaussian distributed) and standard deviation \( \sigma \), let \(\tau = 1/\sigma^{2}\) be its precision,  the log conditional is
    \[
         \log P(x \mid \mu, \tau) =
         \begin{pmatrix}
             \mu \tau & -\tau/2
         \end{pmatrix}^T
         \begin{pmatrix}
             x\\
             x^2
         \end{pmatrix}
         - \frac{\mu^{2}\tau}{2} + \frac{1}{2}\big( \log \tau - \log 2\pi \big).
    \]
    We may rewrite it the conditional as
    \[
         \log  P(x \mid \mu, \tau) =
         \begin{pmatrix}
             \tau x & -\tau/2
         \end{pmatrix}
         \begin{pmatrix}
             \mu\\
             \mu^2
         \end{pmatrix}
         + \frac{1}{2}\big( \log \tau - \tau x^2 - \log 2\pi \big),
    \]
    where
    \[
         \bm{\eta}_{X,\mu}(x,\tau) =  \begin{pmatrix}
            \tau x\\
            -\tau/2
        \end{pmatrix}^T,\quad \bm{T}_\mu(\mu)=  \begin{pmatrix}
            \mu\\
            \mu^2
        \end{pmatrix}.
    \]
\end{exampleth}

From this results, it can be seen that \( \log P(x \mid y , cp_y) \) is linear in \( \bm{T}_X(x) \) and \( \bm{T}_Y(y) \), and, by the same reasoning, linear in any sufficient statistic of any parent of \( X \). This is a general result for any variable \( X \) in this kind of models: \emph{For any variable \( X \), the log conditional under its parents must be multi-linear of the statistics of \( X \) and its parents}.\footnote{A function is multi-linear if it depends linearly with respect each variable.}

Returning to the variational update for a node \( Y \):
\[
     \log Q^{new}(y) = \E{Q_{\backslash Y}}{log P(y \mid pa_Y)} + \sum_{X_k \in ch(Y)}\E{Q_{\backslash Y}}{\log P(x_k \mid pa_{X_k}))} + \text{const.} \ ,
\]
where the expectations are over the variational distribution of all other hidden variables can be calculated in terms of \( \bm{T}_Y(y) \):
\[
    \begin{aligned}
     \log Q^{new}(y) &= \E{Q_{\backslash Y}}{\log(h_Y (pa_y)) + \bm{\eta}_Y(pa_y)^T \bm{T}_Y(y) + \psi_Y(pa_y)}\\
     &+ \sum_{X_k \in ch(Y)} \E{Q_{\backslash Y}}{ \bm{\eta}_{X_k, Y}(x, cp(x_k))^T \bm{T}_Y(y) + \lambda_{k}(x_k, cp_{x_{k},y}) } + \text{const.}\\
     &= \Bigg[ \E{Q_{\backslash Y}}{\bm{\eta}_Y(pa_Y)^T} + \sum_{X_k \in ch(Y)} \E{Q_{\backslash Y}}{ \bm{\eta}_{X_k, Y}(x, cp_{x_k,y})^T}  \Bigg]^T \bm{T}_Y(y) \\
     & + \log h_{Y}(pa_y)+ \text{const.}
    \end{aligned}
\]

It follows that \( Q^{new}(y) \) is in the exponential family of the same form as \( P(y \mid pa_y) \) but with parameter function
\[
     \bm{\eta}^{new}_Y = \E{Q_{\backslash Y}}{\bm{\eta}_Y(pa_y)} + \sum_{X_k \in Ch(Y)}\E{Q_{\backslash Y}}{\bm{\eta}_{x_k, Y}(x_k, cp_{x_{k},y})}.
\]

As the expectations of \( \bm{\eta}_Y \) and \( \bm{\eta}_{X_k, Y} \) are multi-linear functions of the expectations of the statistic functions of their corresponding variables, it is possible to reparameterize these functions in terms of these expectations
\[
     \begin{aligned}
     \bar{\bm{\eta}}_Y (\{ \mathbb{E}[{\bm{T}_{X_k}}(x_k)] \}_{X_k \in pa_Y}) &= \E{}{\bm{\eta}_Y(pa_Y)}\\
     \bar{\bm{\eta}}_{X_k,Y} (  \mathbb{E}[\bm{T}_{X_k}(x_k)], \{ \mathbb{E}[{\bm{T}_{X_j}}(x_j)] \}_{X_j \in cp_{X_k}}) &= \E{}{\bm{\eta}_{X_k, cp_{X_k}}(x_k, cp_{x_k})}\\
     \end{aligned}
\]
As a result, to compute the variational update, only the expectations of the sufficient statistics are needed.

The message from a parent node \( Y \) to a child node \( X \) is the expectation under \( Q \) of its statistic vector
\[
      \bm{m}_{Y \to X} = \E{}{\bm{T}_Y(y)}.
\]
The message from a child node \( X \) to a parent node \( Y \) is:
\[
      \bm{m}_{X \to Y} = \bar{\bm{\eta}_{X,Y}}(\E{}{\bm{T}_X(x)}, \{\bm{m}_{X_k \to X}\}_{X_k\in cp_Y})
\]
which relied on having received all messages from all the co-parents. If a node \( X \)  is observed, the messages defined above are defined as \( \bm{T}_A(a) \) instead of \( \E{}{\bm{T}_A(a)} \).

\begin{exampleth}
     If \( X \) is Gaussian distributed and \( Y, \beta \) are its parents, the messages are:
     \[
           \bm{m}_{X \to Y} = \begin{pmatrix}
                \E{}{\beta}\E{}{X}\\
                - \E{}{\beta}/2
           \end{pmatrix}, \quad
           \bm{m}_{X \to \beta} = \begin{pmatrix}
               -\frac{1}{2}\Bigg( \E{}{x^2} - 2\E{}{x}\E{}{y} + \E{}{y^2} \Bigg)\\
               \frac{1}{2}
          \end{pmatrix}.
     \]
     And the messages from \( X \) to any of its child nodes is
     \[
          \begin{pmatrix}
               \E{}{\beta}\E{}{x}\\
               - \E{}{x^2}
          \end{pmatrix}.
     \]
\end{exampleth}

When a node \( Y \) has received all messages from its parents and children, we can compute the updated parameter \( \bar{\eta}^{new}_Y \) as
\[
     \eta^{new}_Y = \bar{\bm{\eta}}_Y(\{ \bm{m}_{X_k \to Y} \}_{X_k \in pa_Y}) + \sum_{X_k \in ch_Y}\bm{m}_{X_k \to Y}.
\]

