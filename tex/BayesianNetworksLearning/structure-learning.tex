
The Bayesian network structure is not always given and has to be learned from the data, to achieve this, there are some issues that need to be considered. These are:
\begin{itemize}\setlength{\itemsep}{0.15cm}
  \item The number of Bayesian networks is exponential over the number of variables so a brute force algorithm would not be viable.
  \item Testing dependencies requires a large amount of data, therefore, a threshold needs to be established to measure when a dependence is significant.
  \item The presence of hidden variables might not be learned from the data.
\end{itemize}


\begin{algorithm}[t]
  \SetAlgoLined\KwData{Complete undirected graph \(G\), with vertices \(V\)}
  \KwResult{\(G\) with removed links}
  \(i = 0\)\;
  \While{all nodes have \(\leq i\) neighbors}{
    \For{\(X \in V\)}{
      \For{\(Y \in ne(x)\)}{
        \If{\(\exists S \subset ne(X)\backslash Y\) such that \(\#S = i\) and
          \(X \bigCI Y \mid S\)}{
          Remove \(X-Y\) from \(G\)\;
          \(S_{XY} = S\)\;
        }
      }
    }
    \(i = i+1\)\;
  }
  \caption{PC Algorithm for skeleton learning}\label{alg:pc}
\end{algorithm}

\section{PC algorithm}

The \emph{PC algorithm} (\cite{spirtes2000causation} Chapter 5.4.2) learns the skeleton structure given a complete graph \(G=(V,E)\) (constructed from the considered set of variables) and orients its edges using variable independence in the empirical distribution. The following theorem provides the motivation for the algorithm's procedure.

\begin{theorem}[\cite{spirtes2000causation} Theorem 3.4]
  A distribution \(P\) is faithful to a directed acyclic graph \(G = (V,E)\)  if and only if
  \begin{itemize}
    \item two vertices \(X\) and \(Y\) are adjacent if and only if are dependent conditionally on every set of vertices that does not contain either of them, i.e,
      \[
      X \in ne(Y) \iff X \bigCD Y \mid S \quad \forall S \subset V \setminus \{X,Y\},
      \]
      and
    \item for any three vertices such that \(X \in ne(Y)\), \(Y \in ne(Z)\) and \(X \notin ne(Z)\),  \(X \to Y \leftarrow Z\) is in \(G\) if and only if \(X \bigCD Z \mid S\cup \{Y\} \ \forall S \subset V \setminus \{X,Z\}\).
  \end{itemize}

\end{theorem}

The \emph{PC algorithm} is split in two parts, firstly, the skeleton structure is learned, that is, given the complete graph, as many edges as possible are removed. This part (Algorithm~\ref{alg:pc}) iterates over subsets of neighborhoods, from smaller to bigger ones. It chooses a linked pair of variables \((X,Y) \in E\) and
a subset \(S_{XY} \subset ne(X)\), verifying \(Y \notin S_{XY}\). If \(X \bigCI Y \mid S_{XY}\), then the link is removed and \(S_{XY}\) is stored as their separation set.

The followed reasoning is: as \(X \bigCI Y \mid S_{XY}\) and \(S_{XY} \subset V \setminus \{X,Y\}\), the first part of the theorem ensures that \(X \notin ne(Y)\).

This procedure results in the skeleton of the Bayesian network, no more edges will be removed or added. The directed graph may be constructed following two rules:
\begin{enumerate}
  \item For any undirected link \(X - Y - Z\), if \(Y \notin S_{XZ}\) then set
    \(X \to Y \leftarrow Z\) (we are creating a collider for that path).
  \item The rest of links may oriented arbitrarily not
creating cycles or colliders.
\end{enumerate}
The reasoning behind is the d-separation Theorem~\ref{th:d-separation}, if \(Y \notin S_{XZ}\) then \(X \bigCD Z \mid Y\). Otherwise (\(X \bigCI Z \mid Y\)) \(\{Y\}\) would be the d-separation set \(S_{XZ} = \{Y\}\) as they are checked from smaller to bigger ones, which we know is not the case. Therefore, using Theorem~\ref{th:d-separation}, \(Y\) must d-connect \(X\) and \(Y\), to achieve this, \(Y\)  must be set as a collider of the path. This is because, using the d-connection definition, the only known undirected path is \(U = X - Y - Z\) and no non-collider on \(U\) must belong to \(\{Y\}\).

Conversely, if \(Y \in S_{XZ}\) and \(X \bigCI Z \mid S_{XZ}\) then \(S_{XZ}\) should d-separate them, that is, using any configuration that does not create a collider in \(S_{XZ}\).

\section{Independence learning}

The PC algorithms assumes there exists a procedure of testing conditional independence of variables, that is, given three variables \(X, Y\) and \( Z \),  measure \(X \bigCI Y \mid Z\). One approach is to measure the empirical \emph{conditional mutual information} of the variables.

\begin{definition}
  Given two random variables \(X, Y\), we define their \emph{mutual information} as the Kullback-Leibler divergence of their joint distribution and the product of their marginals,
  \[
    MI(X;Y) = \KL{P_{X,Y}}{P_{X}P_{Y}}.
  \]
\end{definition}

\begin{definition}
  Given three random variables \(X, Y, Z\) we define the \emph{conditional mutual information} of \(X\) and \(Y\) over \(Z\) as
  \[
    MI(X;Y\mid Z) = \E{Z}{\KL{P_{X,Y \mid Z}}{P_{X\mid Z} P_{Y \mid Z}}}.
  \]
\end{definition}
Where \(MI(X;Y \mid Z) \geq 0\) and
\[
MI(X;Y \mid Z) = 0 \iff P_{X,Y \mid Z} = P_{X\mid Z} P_{Y \mid Z} \iff X\bigCI Y \mid Z.
\]
These values might be estimated using their empirical distributions, however, this \emph{empirical} mutual information will be typically greater than \(0\) even when \(X\bigCI Y \mid Z\). Thus, a threshold must be established, This is commonly done by using a statistical test, taking into consideration that the empirical mutual information follows a chi-squared distribution under independence (\cite{kullback1997information}).

A Bayesian approach would consist on comparing the model likelihood under independence and dependence hypothesis, let \((\bx, \by, \bz) = \{(x_{n}, y_{n}, z_{n})\}_{n=1,\dots, N}\) be the known data of each variable:
\[
  P_{indep}(\bx,\by,\bz) = \int_{\btheta} P(\btheta) \prod_{n=1}^{N} P(x_{n}\mid z_{n}, \btheta)P(y_{n} \mid z_{n}, \btheta)P(z_{n} \mid \btheta),
\]
\[
P_{dep}(\bx, \by, \bz) = \int_{\btheta} P(\btheta) \prod_{n=1}^{N}P(x_{n}, y_{n}, z_{n} \mid \btheta).
\]
Which means checking which assumptions is most probable to have generated the data.

