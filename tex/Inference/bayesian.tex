

\emph{Bayesian inference} considers a probability distribution over the set of parameters. This distribution is then governed by so called \emph{hyper-parameters} \(\bm{\alpha}\), these are typically omitted when using \(P(\btheta \mid \bm{\alpha})\), where \(P(\btheta)\) is written instead.

Bayesian inference attempts to determine the \emph{posterior distribution} \(P(\btheta \mid \bx)\) using a \emph{prior belief} \(P(\btheta)\) and the \emph{likelihood function} \(P(\bx \mid \btheta)\) on the basis of Bayes' theorem:
\[
  P(\btheta \mid  \bx) = \frac{P(\bx  \mid  \btheta)P(\btheta)}{P(\bx)}.
\]


\section{Example: Discrete prior}

For this example, consider a set of i.i.d variables \(\bX = (X_{1},\dots, X_{N})\) with their corresponding set of observations be \(\bx = (x_{1},\dots,x_{N})\), where each \(X\) models the results of coin-tossing experiment, let \(1\) symbolize \emph{heads} and \(0\) \emph{tails}.

Bayesian inference attempts to estimate the probability distribution of \(\theta\) given \(\bx\). This parameter models the probability of the tossing resulting in heads as
\[
P(x_n = 1  \mid  \theta) = \theta \quad \forall n \in \{1,\dots,N\}.
\]

The joint probability takes the form:
\[
  P(\bx,\theta) = P(\theta)\prod_{n=1}^N P(x_n \mid \theta).
\]
We want to calculate the posterior distribution:
\[
  P(\theta \mid \bx) = \frac{P(\bx \mid \theta)P(\theta)}{P(\bx)},
\]
to do so, we need to specify the prior distribution \(P(\theta)\). For now, we are using a discrete variable that verifies:
\[
  P(\theta = 0.2) = 0.1, \quad P(\theta = 0.5) = 0.7 \quad \text{and} \quad P(\theta = 0.8) = 0.2.
\]
This means that we have a \(70\%\) belief that the coin is fair, a \(10\%\)
belied that is biased to tails and \(20\%\) that is biased to heads. Let \(n_h\) be the number of heads in our observed data and \(n_t\)
the number of tails, mathematically:
\[
  n_{h} = \#\{x \in \bx \ : \ x = 1\} \quad \text{and} \quad n_{t} = \#\{x \in \bx \ : \ x = 0\}.
\]
Given that \(x_{n} \mid \theta\) is a Bernoulli trail \(\forall n \in \{1,\dots,N\}\) , the posterior is:
\[
  P(\theta  \mid \bx) = \frac{P(\theta)}{P(\bx)} \theta^{n_h}{(1-\theta)}^{n_t}.
\]

Suppose that \(n_h = 2\) and \(n_t = 8\), the posterior might be calculated up to a normalization factor:
\[
\begin{aligned}
  P(\theta = 0.2  \mid  \bx) &= \frac{1}{P(\bx)}\times 0.1 \times 0.2^{2}
  \times 0.8^{8} = \frac{1}{P(\bx)} \times 6.71\times10^{-4}, \\
   P(\theta = 0.5  \mid  \bx) &= \frac{1}{P(\bx)}\times 0.7 \times 0.5^{2}
   \times 0.5^{8} = \frac{1}{P(\bx)} \times 6.83\times10^{-4},\\
    P(\theta = 0.8  \mid  \bx) &= \frac{1}{P(\bx)}\times 0.2 \times 0.2^{2}
  \times 0.8^{8} = \frac{1}{P(\bx)} \times 3.27\times10^{-7}.
\end{aligned}
\]
We can compute the normalizing factor as
\[
   P(\bx) = \sum_{\theta \in \{0.2, 0.5, 0.8\}} P(\bx, \theta) =   6.71\times10^{-4} +   6.83\times10^{-4} +
   3.27\times10^{-7} = 0.00135.
 \]
Therefore, the posterior is
\[
\begin{aligned}
  P(\theta = 0.2  \mid  \bx) &= 0.4979,\\
  P(\theta = 0.5  \mid  \bx) &= 0.5059,\\
  P(\theta = 0.8  \mid  \bx) &= 0.00024.
\end{aligned}
\]

\section{Example: Continuous Prior}

In the previous example, we have used a discrete prior for the parameter distribution, a continuous prior might be chosen instead. Suppose an uniform prior distribution:
\[
P(\theta) = k \implies \int_0^1 P(\theta) d\theta = k = 1
\]
due to normalization.

Using the previous calculations we have
\[
  P(\theta \mid  \bx) = \frac{1}{P(\bx)} \theta^{n_h}{(1-\theta)}^{n_t},
\]
where
\[
  P(\bx) = \int_0^1 \theta^{n_h}{(1-\theta)}^{n_t} d\theta.
\]
This implies that
\[
  P(\theta \mid \bx) = \frac{\theta^{n_h}{(1-\theta)}^{n_t} }{ \int_0^1 u^{n_h}{(1-u)}^{n_t} du} \implies \theta \mid \bx \sim Beta(n_h + 1, n_t + 1).
\]

A Beta distribution could be also considered as the prior distribution:
\[
  \theta \sim \text{Beta}(\alpha, \beta) \implies P(\theta) = \frac{1}{B(\alpha, \beta)}\theta^{\alpha - 1}{(1 - \theta)}^{\beta - 1}.
\]
in this case, the posterior is:
\[
  P(\theta, \bx) = \frac{1}{B(\alpha + n_h, \beta + n_t)}\theta^{\alpha
    + n_h - 1}{(1 - \theta)}^{\beta + n_t - 1} \implies \theta \mid \bx \sim Beta(n_h + \alpha, n_t + \beta)
\]

\section{Utility}
The Bayesian posterior says nothing about how to benefit from the beliefs it
represents, in order to do this we need to specify the utility of each decision.

With this idea we define an utility function over the parameters
\[
  U(\theta, \theta_{true}) = \alpha \mathbb{I}[\theta = \theta_{true}] - \beta
  \mathbb{I}[\theta \neq \theta_{true}],
\]
where \(\alpha, \beta \in \R\). This symbolizes the gains or looses of choosing
the parameter \(\theta\), when the true value of the parameter is supposed to be
\(\theta_{true}\). Therefore, the expected utility of a parameter \(\theta_0\) is
calculated as
\[
  U(\theta = \theta_0) = \int_{\theta_{true}}U(\theta = \theta_0,
  \theta_{true})P(\theta = \theta_{true}  \mid  \bx).
\]
We might as well define an utility function over the previous example:
\[
  U(\theta, \theta_{true}) = 10\mathbb{I}[\theta = \theta_{true}] - 20
  \mathbb{I}[\theta \neq \theta_{true}],
\]
where we interpret that the loss of choosing the wrong parameter is twice as
important as the gains from doing it right.

The expected utility of the decision that the parameter is \(\theta = 0.2\)
in our discrete example would be
\[
  \begin{aligned}
  U(\theta = 0.2) &= U(\theta = 0.2, \theta_{true} = 0.2)P(\theta_{true} = 0.2  \mid
  \bx)\\
  &+ U(\theta = 0.2, \theta_{true} = 0.5)P(\theta_{true} = 0.5  \mid
  \bx) \\
  & +  U(\theta = 0.2, \theta_{true} = 0.8)P(\theta_{true} = 0.8  \mid  \bx)\\
  &= 10 \times 0.4979 - 20\times 0.5059 -20 \times 0.00024 \\
  &= -5.1438,\\
  U(\theta = 0.5) &= -4.9038, \\
  U(\theta = 0.8) &= -20.0736.
\end{aligned}
\]

Given this, if we had to make a decision for the parameter, we could choose the value with the highest utility. Other approaches like the mode or mean (continuous posterior) of the distribution are possible.

\section{Maximum a Posteriori Estimation}

\emph{Maximum a posteriori probability estimation} is a Bayesian inference method of estimating the mode of the posterior distribution. In contrast to maximum likelihood estimation, it employs an augmented optimization objective which incorporates a prior distribution.

\begin{definition}
  \emph{Maximum A Posteriori (MAP)} refers to the value of the parameter \( \btheta \) that better fits the data:
  \[
    \btheta^{MAP} = \argmax_{\btheta} P(\bx \mid \btheta)P(\btheta) = \argmax_{\btheta} P(\btheta \mid \bx).
  \]
\end{definition}

\begin{remark}
  Maximum likelihood estimation is a particular case of maximum a posterior estimation with a flat (constant) prior.
\end{remark}

\begin{remark}
  MAP estimation can be seen as a limiting case of Bayesian estimation under the 0â€“1 utility function:
  \[
    U(\btheta, \btheta_{true}) = \mathbb{I}[\btheta = \btheta_{true}],
  \]
  using this, the expected utility of a parameter \(\btheta = \btheta_0\) is
  \[
    U(\btheta = \btheta_0) = \int_{\btheta_{true}}\mathbb{I}[\btheta_{true} = \btheta_0]P(\btheta = \btheta_{true}  \mid  \bx) = P(\btheta_0  \mid  \bx).
  \]
  This means that the maximum utility decision is to take the value \(\btheta_0\) with the highest posterior value.
\end{remark}
