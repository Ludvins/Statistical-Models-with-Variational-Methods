
\texttt{InferPy} (\cite{cozar2019inferpy}) is a high-level API written in \texttt{Python} inspired by \texttt{Keras} and run on top of \texttt{Tensorflow} and \texttt{Edward} for probabilistic modeling. It is focused on: enabling probabilistic modeling, flexible data processing and scalable inference.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.7\textwidth]{tex/images/arch.png}
    \caption{InferPy architecture.}
\end{figure}

\texttt{InferPy}'s main features are:
\begin{itemize}
  \item Allows to define probabilistic models whether they contain or not neural networks in a simple way.
  \item All models that can be defined using \texttt{Edward2} can also be defined in  \texttt{InferPy}, whose probability distributions are mainly inherited from \texttt{tensorflow-probability}.
  \item All the models parameters can be defined using the standard \texttt{Python} types (\texttt{Numpy} compatibility).
  \item \texttt{InferPy} relies on top of \texttt{Edward}'s inference engine, therefore, it includes all the inference algorithms available in that package.
  \item As it uses \texttt{Edward} and \texttt{TensorFlow} as its inference engine, all the parallelization details are hidden to the user.
  \item The same code will run either in CPUs or GPUs.
\end{itemize}

\section{Installation}

\texttt{InferPy} has the following requirements:
\begin{itemize}
    \item \texttt{Python} \( \geq 3.5 \) and \( < 3.8 \).  
    \item \texttt{Tensorflow} \(\geq  1.12.1 \) and \( < 2.0 \).
    \item \texttt{Tensorflow-probability} \( 0.7.0 \).
    \item \texttt{NetworkX} \( \geq 2.2.0 \) and \( < 3.0 \).   
\end{itemize}

\texttt{InferPy} is available at \texttt{Pip} and can be installed with the following command:

\begin{minted}{sh}
    pip install inferpy
\end{minted}

\section{Usage guide with PCA}

In this section we are constructing a PCA inference model with \texttt{InferPy}. Programming and \texttt{Python} knowledge are assumed, the guidance will be over the API usage.

Let us remember the variables we had in the model, consider \( X_1, \dots, X_N \) i.i.d \( \mathbb{R}^D \) variables, these are the observed ones. Then there are \( Z_1,\dots,Z_N \) hidden variables that correspond to the hidden representation of the data in \( \mathbb{R}^K \). There is also a global hidden variable \( \bm{W} \) modeling the linear transformation from one space to the other. We are also considering another global variable \( \bm{W}_0 \) that will allow the model to generate non centered points. 

The hidden variables prior is a centered gaussian and the data is supposed to be generated as
\[
     X_n \mid z_n, \bm{w}, \bm{w_0} \sim \mathcal{N}(\bm{w}^T z_n + \bm{w_0}, I)
\]
No noise is being considered in this example.

\begin{figure}[h!]
    \centering
    \begin{tikzpicture}[
      node distance=1cm and 0.5cm,
      mynode/.style={draw,circle,text width=0.6cm,align=center},
      param/.style={draw,text width=0.5cm,align=center}
      ]
  
      \node[mynode] (theta) {\(\bm{W}\)};
      \node[mynode, below left=of theta] (zn) {\(Z_{n}\)};
      \node[mynode, below right=of theta] (xn) {\(X_{n}\)};
      \node[mynode, above right=of xn] (theta0) {\(\bm{W}_0\)};

  
      \plate{} {(zn)(xn)} {\(n = 1\dots N\)}; %
      \path (theta) edge[-latex] (xn)
      (theta0) edge[-latex] (xn)
      (zn) edge[-latex] (xn)
      ;
  
    \end{tikzpicture}
    \caption{Probabilistic PCA model. No noise considered.}
  \end{figure}
  
  The database we are using is \texttt{Mnist}, which is a largely used dataset on a set of handwritten digits.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{tex/images/mnist.png}
    \caption{Mnist dataset example.}
\end{figure}

The full model definition would be the following:
\begin{minted}[]{python}
    @inf.probmodel
    def pca(k,d):
        w =  inf.Normal(loc=tf.zeros([k,d]), scale=1, name="w")
        w0 = inf.Normal(loc=tf.zeros([d]), scale=1, name="w0")
        with inf.datamodel():
            z = inf.Normal(tf.zeros([k]),1, name="z")
            x = inf.Normal(np.dot(z,w) + w0, 1, name="x")
\end{minted}

Models are defined by decorating a \texttt{Python} functions with \texttt{@inf.probmodel}. In this case we are defining a model with 2 parameters:

\begin{minted}{python}
    @inf.probmodel
    def pca(k, d):
\end{minted}

Now we declare the global hidden variables, we use the distributions inside \texttt{InferPy} which is assumed to be imported as \texttt{inf}, in this case \texttt{inf.Normal}. This distribution has three arguments, the mean (\texttt{loc}), the standard deviation (\texttt{scale}), and the name of the variable, this last parameter is needed as its how this model and the variational will communicate.

If either the mean or the standard desviation are given as an array, a constant value on the other parameter would be interpreted as a constant array of the corresponding size.
\begin{minted}[]{python}
    w =  inf.Normal(loc=tf.zeros([k,d]), scale=1, name="w")                   
\end{minted}
Here we are defining a \( K\times D \)  Gaussian distributed variable with mean \( 0 \) and standard deviation \( 1 \), named "w".

We have reached the point where we have to define a set of i.i.d random variables, one for each observation, \texttt{InferPy} gives a explicit syntaxis for this, variables defined inside \texttt{with inf.datamodel(size)} are replicated and i.i.d of each other. The size can be omitted as it is calculated from the dataset.
This is how we declare the hidden local variables in our model
\begin{minted}[]{python}
    with inf.datamodel():
        z = inf.Normal(tf.zeros([k]),1, name="z")  
\end{minted}

Our generative model is now defined, now we might define the variational one:

\begin{minted}{python}
    @inf.probmodel
    def Q(k,d):
        qw_loc = inf.Parameter(tf.zeros([k,d]), name="qw_loc")
        qw_scale = tf.math.softplus(inf.Parameter(tf.ones([k,d]), name="qw_scale"))
        qw = inf.Normal(qw_loc, qw_scale, name="w")

        qw0_loc = inf.Parameter(tf.ones([d]), name="qw0_loc")
        qw0_scale = tf.math.softplus(inf.Parameter(tf.ones([d]), name="qw0_scale"))
        qw0 = inf.Normal(qw0_loc, qw0_scale, name="w0")
    
        with inf.datamodel():
            qz_loc = inf.Parameter(np.zeros([k]), name="qz_loc")
            qz_scale = tf.math.softplus(inf.Parameter(tf.ones([k]), name="qz_scale"))
            qz = inf.Normal(qz_loc, qz_scale, name="z")
\end{minted}

In this case, among the variables, we must define each parameter. Let us focus on "w" whose variational distribution is a Gaussian distribution, we must define two parameters, one for the mean and other for the standard deviation.

\begin{minted}{python}
    qw_loc = inf.Parameter(tf.zeros([k,d]), name="qw_loc")
    qw_scale = tf.math.softplus(inf.Parameter(tf.ones([k,d]), name="qw_scale"))
    qw = inf.Normal(qw_loc, qw_scale, name="w")
\end{minted}

As the standard deviation value must not reach a negative value, we use a \texttt{softplus} function to smoothly approximate a rectifier function. 

The same argument is applied to the rest of variables.

When both models are defined and instantiated in a variable, we need to create an inference object:
\begin{minted}{python}
    VI = inf.inference.VI(qmodel)
\end{minted}

By default, the inference object uses el ELBO and loss function and uses \texttt{AdamOptimizer} from \texttt{TensorFlow}. An amount of \texttt{epochs} for the fitting might be set.

Given a training dataset \texttt{X\_train}, the model is trained indicating which set of observations corresponds to each observed variable using the following syntax:
\begin{minted}{python}
    pmodel.fit({"x": X_train}, VI)
\end{minted}

Once the model is trained, a variable posterior can be taken as
\begin{minted}{python}
    pmodel.posterior("z").parameters()
\end{minted}

We can also take a sample from the parameter posterior 
\begin{minted}{python}
    post = {"z": pmodel.posterior("z").sample()}
\end{minted}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{tex/images/pca_mnist.png}
    \caption{PCA posterior sample of MNIST.}
\end{figure}
