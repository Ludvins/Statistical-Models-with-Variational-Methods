
\emph{Variational inference} concepts, which are adapted from statistical physics, first appeared in~\cite{anderson1987mean}, in which the authors used them to fit a neural-network. More precisely, they used \emph{mean-field} methods to achieve it.

In the coming years, several studies were done on variational inference, such as~\cite{hinton1993keeping}, which used further mean-field methods in neural networks, and~\cite{jordan1999introduction} that generalized variational inference to many models.

Today, variational inference is more scalable and easy to derive, in some cases it is even automated. It has been applied to many different models and types of learning.

This document attempts to give an overview of some results in \emph{Bayesian variational inference} as well as to test some frameworks for probabilistic modeling. In these tests an effort to apply variational inference techniques to real databases is made.

The theoretical part of this document, which is encompassed by chapters 1 to 23, describes the basic concepts of \emph{statistical inference}, from classical to \emph{variational}. After this, the \emph{exponential family} and \emph{graphical models} are reviewed together with their influence in variational inference, focusing on how the inference task is simplified by their usage.

On the other hand, the last chapters, focus on the utilization of different frameworks to experiment different models, which involve \emph{Gaussian mixture} and dimensionality reduction via \emph{principal components analysis} and \emph{variational auto-encoders}.

The main sources used for writing this documents were~\cite{barber},~\cite{bishop2006pattern},~\cite{koller_friedman},~\cite{masegosa2019probabilistic} and~\cite{blei2017variational}. Citations are made in each section within the text.
