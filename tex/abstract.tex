In this document, the theoretical fundamentals of \emph{statistical inference}, more precisely, \emph{variational inference} are reviewed, making special emphasis on how the application of \emph{graphical models} affects inference.

\emph{Statistical inference} is the process of inferring the underlying properties of a dataset or population. Two main paradigms are discussed, \emph{Bayesian inference} and \emph{likelihoodist inference} which differ in that the former uses \emph{Bayes' theorem} during the inference task, assuming a \emph{prior distribution} over the model parameters.

\emph{Variational Bayesian methods} are a class of techniques that along with \emph{Bayes' theorem}, transform the inference task in a optimization one, which might then be approached through machine learning algorithms, such as \emph{gradient or coordinate descent}. Specific algorithms do also arise, as the case of \emph{expectation maximization}.

The combination of \emph{variational inference}, the \emph{exponential family} and \emph{graphical models} do highly simplify the optimization task, automatizing it in some models. \emph{Variational message passing} is an example of this.

Some common models which are usually approached using variational inference are \emph{Gaussian mixtures}, \emph{latent Dirichlet allocation} and \emph{principal components analysis}.


\textbf{Keywords:} \emph{statistical inference, variational inference, exponential family, graphical models, expectation-maximization algorithm, variational Bayes, Gaussian mixture, variational auto-encoders} and \emph{variational message passing}.
